{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcyL1nxrMj6q"
      },
      "source": [
        "Step 1 Import Liblary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QE5NGBYnMYF_"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import math\n",
        "from warnings import simplefilter\n",
        "import plotly.graph_objects as go\n",
        "import time\n",
        "\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "simplefilter(action='ignore', category=DeprecationWarning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KunqEX9CMqGg"
      },
      "source": [
        "Step 2, Download Data dari Pustaka Yahoo Finance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcmlwQBBMphB",
        "outputId": "44b604c6-ccb9-442b-d44b-65d4fd88482b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "stock_symbol = \"GC=F\"\n",
        "start_date = \"2022-08-01\"\n",
        "end_date = \"2024-08-01\"\n",
        "price_type = \"Close\"\n",
        "\n",
        "data = yf.download(stock_symbol, start=start_date, end=end_date)\n",
        "\n",
        "# Scaling Data\n",
        "close_prices = data[price_type].values.reshape(-1, 1)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(close_prices)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq0Im_qrMy7Q"
      },
      "source": [
        "Step 3, Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKxxjD4AMxtS"
      },
      "outputs": [],
      "source": [
        "def prepare_data(data, n_steps):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - n_steps):\n",
        "        X.append(data[i:(i + n_steps), 0])\n",
        "        y.append(data[i + n_steps, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "n_steps = 30\n",
        "X, y = prepare_data(scaled_data, n_steps)\n",
        "\n",
        "#Spliting Data 80/20\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OqQASFDNEti"
      },
      "source": [
        "Step 4 Membangun dan Melatih Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9EO0uNyNCG5"
      },
      "outputs": [],
      "source": [
        "def build_model(params, X_train):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=params['units'], return_sequences=True, activation='tanh', input_shape=(X_train.shape[1], 1)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units=params['units'], activation='tanh'))\n",
        "    model.add(Dense(units=1))\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9B2KvY9TNjGS",
        "outputId": "033646cc-8cf4-41d5-e789-8493d4c33a3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Epochs: 50, Batch Size: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "22/22 - 5s - 231ms/step - loss: 0.0172 - val_loss: 0.0034\n",
            "Epoch 2/50\n",
            "22/22 - 1s - 64ms/step - loss: 0.0033 - val_loss: 0.0036\n",
            "Epoch 3/50\n",
            "22/22 - 1s - 57ms/step - loss: 0.0024 - val_loss: 0.0046\n",
            "Epoch 4/50\n",
            "22/22 - 1s - 32ms/step - loss: 0.0021 - val_loss: 0.0035\n",
            "Epoch 5/50\n",
            "22/22 - 1s - 57ms/step - loss: 0.0019 - val_loss: 0.0028\n",
            "Epoch 6/50\n",
            "22/22 - 1s - 58ms/step - loss: 0.0020 - val_loss: 0.0039\n",
            "Epoch 7/50\n",
            "22/22 - 1s - 57ms/step - loss: 0.0018 - val_loss: 0.0023\n",
            "Epoch 8/50\n",
            "22/22 - 1s - 33ms/step - loss: 0.0018 - val_loss: 0.0045\n",
            "Epoch 9/50\n",
            "22/22 - 1s - 58ms/step - loss: 0.0016 - val_loss: 0.0034\n",
            "Epoch 10/50\n",
            "22/22 - 1s - 61ms/step - loss: 0.0016 - val_loss: 0.0026\n",
            "Epoch 11/50\n",
            "22/22 - 1s - 55ms/step - loss: 0.0015 - val_loss: 0.0021\n",
            "Epoch 12/50\n",
            "22/22 - 1s - 56ms/step - loss: 0.0014 - val_loss: 0.0023\n",
            "Epoch 13/50\n",
            "22/22 - 1s - 60ms/step - loss: 0.0014 - val_loss: 0.0016\n",
            "Epoch 14/50\n",
            "22/22 - 1s - 49ms/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 15/50\n",
            "22/22 - 1s - 46ms/step - loss: 0.0013 - val_loss: 0.0033\n",
            "Epoch 16/50\n",
            "22/22 - 1s - 45ms/step - loss: 0.0014 - val_loss: 0.0019\n",
            "Epoch 17/50\n",
            "22/22 - 1s - 34ms/step - loss: 0.0012 - val_loss: 0.0011\n",
            "Epoch 18/50\n",
            "22/22 - 1s - 33ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 19/50\n",
            "22/22 - 1s - 57ms/step - loss: 0.0011 - val_loss: 0.0022\n",
            "Epoch 20/50\n",
            "22/22 - 1s - 58ms/step - loss: 0.0011 - val_loss: 0.0022\n",
            "Epoch 21/50\n",
            "22/22 - 1s - 58ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 22/50\n",
            "22/22 - 1s - 33ms/step - loss: 0.0010 - val_loss: 9.6942e-04\n",
            "Epoch 23/50\n",
            "22/22 - 2s - 77ms/step - loss: 9.9582e-04 - val_loss: 0.0012\n",
            "Epoch 24/50\n",
            "22/22 - 1s - 60ms/step - loss: 9.9091e-04 - val_loss: 0.0026\n",
            "Epoch 25/50\n",
            "22/22 - 1s - 43ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 26/50\n",
            "22/22 - 1s - 48ms/step - loss: 9.0472e-04 - val_loss: 7.8258e-04\n",
            "Epoch 27/50\n",
            "22/22 - 1s - 32ms/step - loss: 9.1680e-04 - val_loss: 0.0015\n",
            "Epoch 28/50\n",
            "22/22 - 1s - 58ms/step - loss: 8.7738e-04 - val_loss: 0.0014\n",
            "Epoch 29/50\n",
            "22/22 - 1s - 58ms/step - loss: 8.3386e-04 - val_loss: 0.0011\n",
            "Epoch 30/50\n",
            "22/22 - 1s - 33ms/step - loss: 8.1535e-04 - val_loss: 0.0012\n",
            "Epoch 31/50\n",
            "22/22 - 1s - 32ms/step - loss: 8.4128e-04 - val_loss: 0.0022\n",
            "Epoch 32/50\n",
            "22/22 - 1s - 33ms/step - loss: 9.3309e-04 - val_loss: 0.0019\n",
            "Epoch 33/50\n",
            "22/22 - 1s - 57ms/step - loss: 8.9499e-04 - val_loss: 0.0014\n",
            "Epoch 34/50\n",
            "22/22 - 1s - 33ms/step - loss: 8.6421e-04 - val_loss: 8.0939e-04\n",
            "Epoch 35/50\n",
            "22/22 - 1s - 62ms/step - loss: 7.7389e-04 - val_loss: 8.0753e-04\n",
            "Epoch 36/50\n",
            "22/22 - 1s - 54ms/step - loss: 8.5881e-04 - val_loss: 7.5805e-04\n",
            "Epoch 37/50\n",
            "22/22 - 1s - 59ms/step - loss: 7.7298e-04 - val_loss: 7.0321e-04\n",
            "Epoch 38/50\n",
            "22/22 - 1s - 51ms/step - loss: 7.9288e-04 - val_loss: 9.2110e-04\n",
            "Epoch 39/50\n",
            "22/22 - 1s - 33ms/step - loss: 7.9685e-04 - val_loss: 9.1186e-04\n",
            "Epoch 40/50\n",
            "22/22 - 1s - 57ms/step - loss: 7.7555e-04 - val_loss: 5.8827e-04\n",
            "Epoch 41/50\n",
            "22/22 - 1s - 57ms/step - loss: 7.2612e-04 - val_loss: 7.3720e-04\n",
            "Epoch 42/50\n",
            "22/22 - 1s - 32ms/step - loss: 7.3695e-04 - val_loss: 6.1520e-04\n",
            "Epoch 43/50\n",
            "22/22 - 1s - 58ms/step - loss: 7.2198e-04 - val_loss: 7.9849e-04\n",
            "Epoch 44/50\n",
            "22/22 - 1s - 32ms/step - loss: 6.3635e-04 - val_loss: 0.0011\n",
            "Epoch 45/50\n",
            "22/22 - 1s - 58ms/step - loss: 8.0405e-04 - val_loss: 0.0017\n",
            "Epoch 46/50\n",
            "22/22 - 1s - 58ms/step - loss: 8.0401e-04 - val_loss: 5.3964e-04\n",
            "Epoch 47/50\n",
            "22/22 - 1s - 33ms/step - loss: 7.3190e-04 - val_loss: 5.5036e-04\n",
            "Epoch 48/50\n",
            "22/22 - 1s - 37ms/step - loss: 7.4378e-04 - val_loss: 7.0969e-04\n",
            "Epoch 49/50\n",
            "22/22 - 1s - 53ms/step - loss: 6.2357e-04 - val_loss: 6.0984e-04\n",
            "Epoch 50/50\n",
            "22/22 - 1s - 59ms/step - loss: 7.2761e-04 - val_loss: 5.2319e-04\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 182ms/step\n",
            "Training with Epochs: 50, Batch Size: 32\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11/11 - 4s - 342ms/step - loss: 0.0277 - val_loss: 0.0275\n",
            "Epoch 2/50\n",
            "11/11 - 1s - 46ms/step - loss: 0.0056 - val_loss: 0.0031\n",
            "Epoch 3/50\n",
            "11/11 - 0s - 44ms/step - loss: 0.0035 - val_loss: 0.0051\n",
            "Epoch 4/50\n",
            "11/11 - 1s - 56ms/step - loss: 0.0025 - val_loss: 0.0051\n",
            "Epoch 5/50\n",
            "11/11 - 1s - 46ms/step - loss: 0.0021 - val_loss: 0.0035\n",
            "Epoch 6/50\n",
            "11/11 - 1s - 55ms/step - loss: 0.0020 - val_loss: 0.0028\n",
            "Epoch 7/50\n",
            "11/11 - 1s - 46ms/step - loss: 0.0019 - val_loss: 0.0035\n",
            "Epoch 8/50\n",
            "11/11 - 0s - 45ms/step - loss: 0.0019 - val_loss: 0.0036\n",
            "Epoch 9/50\n",
            "11/11 - 1s - 58ms/step - loss: 0.0018 - val_loss: 0.0040\n",
            "Epoch 10/50\n",
            "11/11 - 0s - 44ms/step - loss: 0.0018 - val_loss: 0.0027\n",
            "Epoch 11/50\n",
            "11/11 - 1s - 47ms/step - loss: 0.0018 - val_loss: 0.0037\n",
            "Epoch 12/50\n",
            "11/11 - 1s - 63ms/step - loss: 0.0018 - val_loss: 0.0023\n",
            "Epoch 13/50\n",
            "11/11 - 1s - 76ms/step - loss: 0.0018 - val_loss: 0.0020\n",
            "Epoch 14/50\n",
            "11/11 - 1s - 72ms/step - loss: 0.0017 - val_loss: 0.0028\n",
            "Epoch 15/50\n",
            "11/11 - 1s - 111ms/step - loss: 0.0015 - val_loss: 0.0028\n",
            "Epoch 16/50\n",
            "11/11 - 0s - 45ms/step - loss: 0.0016 - val_loss: 0.0029\n",
            "Epoch 17/50\n",
            "11/11 - 1s - 56ms/step - loss: 0.0015 - val_loss: 0.0029\n",
            "Epoch 18/50\n",
            "11/11 - 1s - 49ms/step - loss: 0.0014 - val_loss: 0.0019\n",
            "Epoch 19/50\n",
            "11/11 - 1s - 53ms/step - loss: 0.0014 - val_loss: 0.0016\n",
            "Epoch 20/50\n",
            "11/11 - 1s - 46ms/step - loss: 0.0014 - val_loss: 0.0032\n",
            "Epoch 21/50\n",
            "11/11 - 0s - 45ms/step - loss: 0.0014 - val_loss: 0.0027\n",
            "Epoch 22/50\n",
            "11/11 - 1s - 57ms/step - loss: 0.0013 - val_loss: 0.0019\n",
            "Epoch 23/50\n",
            "11/11 - 1s - 55ms/step - loss: 0.0012 - val_loss: 0.0022\n",
            "Epoch 24/50\n",
            "11/11 - 1s - 46ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 25/50\n",
            "11/11 - 0s - 45ms/step - loss: 0.0014 - val_loss: 0.0018\n",
            "Epoch 26/50\n",
            "11/11 - 1s - 47ms/step - loss: 0.0013 - val_loss: 0.0022\n",
            "Epoch 27/50\n",
            "11/11 - 0s - 45ms/step - loss: 0.0012 - val_loss: 0.0022\n",
            "Epoch 28/50\n",
            "11/11 - 1s - 58ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 29/50\n",
            "11/11 - 1s - 46ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 30/50\n",
            "11/11 - 1s - 56ms/step - loss: 0.0011 - val_loss: 0.0027\n",
            "Epoch 31/50\n",
            "11/11 - 1s - 46ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 32/50\n",
            "11/11 - 0s - 45ms/step - loss: 0.0012 - val_loss: 9.6871e-04\n",
            "Epoch 33/50\n",
            "11/11 - 1s - 48ms/step - loss: 0.0012 - val_loss: 9.5392e-04\n",
            "Epoch 34/50\n",
            "11/11 - 1s - 86ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 35/50\n",
            "11/11 - 1s - 114ms/step - loss: 0.0010 - val_loss: 0.0015\n",
            "Epoch 36/50\n",
            "11/11 - 1s - 86ms/step - loss: 9.8124e-04 - val_loss: 0.0012\n",
            "Epoch 37/50\n",
            "11/11 - 1s - 55ms/step - loss: 9.2219e-04 - val_loss: 0.0012\n",
            "Epoch 38/50\n",
            "11/11 - 1s - 57ms/step - loss: 9.7106e-04 - val_loss: 0.0013\n",
            "Epoch 39/50\n",
            "11/11 - 0s - 44ms/step - loss: 9.3326e-04 - val_loss: 0.0013\n",
            "Epoch 40/50\n",
            "11/11 - 1s - 47ms/step - loss: 9.1113e-04 - val_loss: 0.0010\n",
            "Epoch 41/50\n",
            "11/11 - 0s - 45ms/step - loss: 8.6378e-04 - val_loss: 0.0012\n",
            "Epoch 42/50\n",
            "11/11 - 1s - 58ms/step - loss: 8.8488e-04 - val_loss: 9.6628e-04\n",
            "Epoch 43/50\n",
            "11/11 - 1s - 54ms/step - loss: 8.6668e-04 - val_loss: 7.7871e-04\n",
            "Epoch 44/50\n",
            "11/11 - 1s - 46ms/step - loss: 9.4284e-04 - val_loss: 9.7946e-04\n",
            "Epoch 45/50\n",
            "11/11 - 1s - 57ms/step - loss: 8.0546e-04 - val_loss: 8.2453e-04\n",
            "Epoch 46/50\n",
            "11/11 - 1s - 55ms/step - loss: 8.3687e-04 - val_loss: 7.3401e-04\n",
            "Epoch 47/50\n",
            "11/11 - 1s - 46ms/step - loss: 8.3129e-04 - val_loss: 9.8491e-04\n",
            "Epoch 48/50\n",
            "11/11 - 1s - 54ms/step - loss: 8.3036e-04 - val_loss: 9.4642e-04\n",
            "Epoch 49/50\n",
            "11/11 - 1s - 47ms/step - loss: 7.9939e-04 - val_loss: 0.0013\n",
            "Epoch 50/50\n",
            "11/11 - 1s - 55ms/step - loss: 8.0001e-04 - val_loss: 8.1921e-04\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 148ms/step\n",
            "Training with Epochs: 50, Batch Size: 64\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 - 5s - 814ms/step - loss: 0.0464 - val_loss: 0.0191\n",
            "Epoch 2/50\n",
            "6/6 - 0s - 79ms/step - loss: 0.0075 - val_loss: 0.0282\n",
            "Epoch 3/50\n",
            "6/6 - 0s - 82ms/step - loss: 0.0074 - val_loss: 0.0058\n",
            "Epoch 4/50\n",
            "6/6 - 0s - 78ms/step - loss: 0.0037 - val_loss: 0.0029\n",
            "Epoch 5/50\n",
            "6/6 - 0s - 82ms/step - loss: 0.0029 - val_loss: 0.0097\n",
            "Epoch 6/50\n",
            "6/6 - 0s - 79ms/step - loss: 0.0031 - val_loss: 0.0053\n",
            "Epoch 7/50\n",
            "6/6 - 1s - 107ms/step - loss: 0.0025 - val_loss: 0.0033\n",
            "Epoch 8/50\n",
            "6/6 - 1s - 99ms/step - loss: 0.0022 - val_loss: 0.0056\n",
            "Epoch 9/50\n",
            "6/6 - 1s - 84ms/step - loss: 0.0022 - val_loss: 0.0038\n",
            "Epoch 10/50\n",
            "6/6 - 1s - 83ms/step - loss: 0.0021 - val_loss: 0.0032\n",
            "Epoch 11/50\n",
            "6/6 - 1s - 99ms/step - loss: 0.0020 - val_loss: 0.0045\n",
            "Epoch 12/50\n",
            "6/6 - 1s - 108ms/step - loss: 0.0019 - val_loss: 0.0028\n",
            "Epoch 13/50\n",
            "6/6 - 0s - 77ms/step - loss: 0.0019 - val_loss: 0.0039\n",
            "Epoch 14/50\n",
            "6/6 - 0s - 81ms/step - loss: 0.0019 - val_loss: 0.0029\n",
            "Epoch 15/50\n",
            "6/6 - 1s - 103ms/step - loss: 0.0018 - val_loss: 0.0032\n",
            "Epoch 16/50\n",
            "6/6 - 1s - 104ms/step - loss: 0.0018 - val_loss: 0.0032\n",
            "Epoch 17/50\n",
            "6/6 - 0s - 77ms/step - loss: 0.0019 - val_loss: 0.0030\n",
            "Epoch 18/50\n",
            "6/6 - 1s - 146ms/step - loss: 0.0018 - val_loss: 0.0034\n",
            "Epoch 19/50\n",
            "6/6 - 1s - 203ms/step - loss: 0.0018 - val_loss: 0.0026\n",
            "Epoch 20/50\n",
            "6/6 - 1s - 176ms/step - loss: 0.0017 - val_loss: 0.0036\n",
            "Epoch 21/50\n",
            "6/6 - 0s - 80ms/step - loss: 0.0019 - val_loss: 0.0027\n",
            "Epoch 22/50\n",
            "6/6 - 1s - 100ms/step - loss: 0.0016 - val_loss: 0.0032\n",
            "Epoch 23/50\n",
            "6/6 - 1s - 108ms/step - loss: 0.0016 - val_loss: 0.0027\n",
            "Epoch 24/50\n",
            "6/6 - 1s - 100ms/step - loss: 0.0016 - val_loss: 0.0031\n",
            "Epoch 25/50\n",
            "6/6 - 1s - 103ms/step - loss: 0.0016 - val_loss: 0.0026\n",
            "Epoch 26/50\n",
            "6/6 - 0s - 81ms/step - loss: 0.0016 - val_loss: 0.0030\n",
            "Epoch 27/50\n",
            "6/6 - 0s - 77ms/step - loss: 0.0015 - val_loss: 0.0028\n",
            "Epoch 28/50\n",
            "6/6 - 1s - 108ms/step - loss: 0.0016 - val_loss: 0.0021\n",
            "Epoch 29/50\n",
            "6/6 - 0s - 78ms/step - loss: 0.0015 - val_loss: 0.0026\n",
            "Epoch 30/50\n",
            "6/6 - 1s - 109ms/step - loss: 0.0015 - val_loss: 0.0026\n",
            "Epoch 31/50\n",
            "6/6 - 0s - 81ms/step - loss: 0.0015 - val_loss: 0.0022\n",
            "Epoch 32/50\n",
            "6/6 - 1s - 106ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "Epoch 33/50\n",
            "6/6 - 0s - 79ms/step - loss: 0.0015 - val_loss: 0.0032\n",
            "Epoch 34/50\n",
            "6/6 - 0s - 82ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 35/50\n",
            "6/6 - 1s - 101ms/step - loss: 0.0017 - val_loss: 0.0036\n",
            "Epoch 36/50\n",
            "6/6 - 0s - 81ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 37/50\n",
            "6/6 - 1s - 101ms/step - loss: 0.0014 - val_loss: 0.0029\n",
            "Epoch 38/50\n",
            "6/6 - 1s - 122ms/step - loss: 0.0014 - val_loss: 0.0017\n",
            "Epoch 39/50\n",
            "6/6 - 1s - 233ms/step - loss: 0.0014 - val_loss: 0.0023\n",
            "Epoch 40/50\n",
            "6/6 - 1s - 171ms/step - loss: 0.0014 - val_loss: 0.0022\n",
            "Epoch 41/50\n",
            "6/6 - 1s - 123ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "Epoch 42/50\n",
            "6/6 - 0s - 79ms/step - loss: 0.0013 - val_loss: 0.0021\n",
            "Epoch 43/50\n",
            "6/6 - 1s - 102ms/step - loss: 0.0012 - val_loss: 0.0021\n",
            "Epoch 44/50\n",
            "6/6 - 0s - 80ms/step - loss: 0.0012 - val_loss: 0.0017\n",
            "Epoch 45/50\n",
            "6/6 - 0s - 81ms/step - loss: 0.0012 - val_loss: 0.0018\n",
            "Epoch 46/50\n",
            "6/6 - 1s - 102ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 47/50\n",
            "6/6 - 0s - 81ms/step - loss: 0.0012 - val_loss: 0.0024\n",
            "Epoch 48/50\n",
            "6/6 - 0s - 81ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 49/50\n",
            "6/6 - 1s - 101ms/step - loss: 0.0011 - val_loss: 0.0017\n",
            "Epoch 50/50\n",
            "6/6 - 0s - 82ms/step - loss: 0.0012 - val_loss: 0.0016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7eda70b10700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 286ms/step"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7eda70b10700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 147ms/step\n",
            "Training with Epochs: 50, Batch Size: 128\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 - 4s - 1s/step - loss: 0.0472 - val_loss: 0.0162\n",
            "Epoch 2/50\n",
            "3/3 - 0s - 142ms/step - loss: 0.0169 - val_loss: 0.0037\n",
            "Epoch 3/50\n",
            "3/3 - 1s - 290ms/step - loss: 0.0041 - val_loss: 0.0256\n",
            "Epoch 4/50\n",
            "3/3 - 1s - 406ms/step - loss: 0.0091 - val_loss: 0.0216\n",
            "Epoch 5/50\n",
            "3/3 - 1s - 346ms/step - loss: 0.0055 - val_loss: 0.0059\n",
            "Epoch 6/50\n",
            "3/3 - 0s - 130ms/step - loss: 0.0032 - val_loss: 0.0033\n",
            "Epoch 7/50\n",
            "3/3 - 1s - 208ms/step - loss: 0.0047 - val_loss: 0.0032\n",
            "Epoch 8/50\n",
            "3/3 - 1s - 204ms/step - loss: 0.0029 - val_loss: 0.0072\n",
            "Epoch 9/50\n",
            "3/3 - 1s - 211ms/step - loss: 0.0029 - val_loss: 0.0101\n",
            "Epoch 10/50\n",
            "3/3 - 1s - 201ms/step - loss: 0.0032 - val_loss: 0.0073\n",
            "Epoch 11/50\n",
            "3/3 - 0s - 131ms/step - loss: 0.0025 - val_loss: 0.0038\n",
            "Epoch 12/50\n",
            "3/3 - 1s - 208ms/step - loss: 0.0024 - val_loss: 0.0032\n",
            "Epoch 13/50\n",
            "3/3 - 0s - 132ms/step - loss: 0.0024 - val_loss: 0.0038\n",
            "Epoch 14/50\n",
            "3/3 - 0s - 129ms/step - loss: 0.0022 - val_loss: 0.0054\n",
            "Epoch 15/50\n",
            "3/3 - 1s - 213ms/step - loss: 0.0022 - val_loss: 0.0053\n",
            "Epoch 16/50\n",
            "3/3 - 0s - 130ms/step - loss: 0.0021 - val_loss: 0.0039\n",
            "Epoch 17/50\n",
            "3/3 - 0s - 132ms/step - loss: 0.0021 - val_loss: 0.0033\n",
            "Epoch 18/50\n",
            "3/3 - 1s - 202ms/step - loss: 0.0021 - val_loss: 0.0035\n",
            "Epoch 19/50\n",
            "3/3 - 0s - 133ms/step - loss: 0.0020 - val_loss: 0.0038\n",
            "Epoch 20/50\n",
            "3/3 - 0s - 135ms/step - loss: 0.0020 - val_loss: 0.0036\n",
            "Epoch 21/50\n",
            "3/3 - 1s - 203ms/step - loss: 0.0020 - val_loss: 0.0032\n",
            "Epoch 22/50\n",
            "3/3 - 1s - 204ms/step - loss: 0.0020 - val_loss: 0.0033\n",
            "Epoch 23/50\n",
            "3/3 - 1s - 228ms/step - loss: 0.0019 - val_loss: 0.0035\n",
            "Epoch 24/50\n",
            "3/3 - 1s - 210ms/step - loss: 0.0020 - val_loss: 0.0035\n",
            "Epoch 25/50\n",
            "3/3 - 1s - 430ms/step - loss: 0.0019 - val_loss: 0.0032\n",
            "Epoch 26/50\n",
            "3/3 - 1s - 341ms/step - loss: 0.0019 - val_loss: 0.0033\n",
            "Epoch 27/50\n",
            "3/3 - 0s - 128ms/step - loss: 0.0018 - val_loss: 0.0032\n",
            "Epoch 28/50\n",
            "3/3 - 0s - 129ms/step - loss: 0.0019 - val_loss: 0.0031\n",
            "Epoch 29/50\n",
            "3/3 - 0s - 132ms/step - loss: 0.0018 - val_loss: 0.0031\n",
            "Epoch 30/50\n",
            "3/3 - 0s - 125ms/step - loss: 0.0019 - val_loss: 0.0036\n",
            "Epoch 31/50\n",
            "3/3 - 0s - 130ms/step - loss: 0.0017 - val_loss: 0.0030\n",
            "Epoch 32/50\n",
            "3/3 - 0s - 131ms/step - loss: 0.0018 - val_loss: 0.0029\n",
            "Epoch 33/50\n",
            "3/3 - 1s - 209ms/step - loss: 0.0017 - val_loss: 0.0033\n",
            "Epoch 34/50\n",
            "3/3 - 0s - 137ms/step - loss: 0.0018 - val_loss: 0.0035\n",
            "Epoch 35/50\n",
            "3/3 - 1s - 203ms/step - loss: 0.0017 - val_loss: 0.0027\n",
            "Epoch 36/50\n",
            "3/3 - 1s - 204ms/step - loss: 0.0017 - val_loss: 0.0027\n",
            "Epoch 37/50\n",
            "3/3 - 0s - 133ms/step - loss: 0.0017 - val_loss: 0.0037\n",
            "Epoch 38/50\n",
            "3/3 - 1s - 202ms/step - loss: 0.0017 - val_loss: 0.0026\n",
            "Epoch 39/50\n",
            "3/3 - 0s - 129ms/step - loss: 0.0017 - val_loss: 0.0028\n",
            "Epoch 40/50\n",
            "3/3 - 0s - 135ms/step - loss: 0.0016 - val_loss: 0.0031\n",
            "Epoch 41/50\n",
            "3/3 - 0s - 130ms/step - loss: 0.0016 - val_loss: 0.0028\n",
            "Epoch 42/50\n",
            "3/3 - 0s - 134ms/step - loss: 0.0016 - val_loss: 0.0028\n",
            "Epoch 43/50\n",
            "3/3 - 1s - 205ms/step - loss: 0.0016 - val_loss: 0.0029\n",
            "Epoch 44/50\n",
            "3/3 - 0s - 133ms/step - loss: 0.0016 - val_loss: 0.0025\n",
            "Epoch 45/50\n",
            "3/3 - 1s - 199ms/step - loss: 0.0015 - val_loss: 0.0029\n",
            "Epoch 46/50\n",
            "3/3 - 0s - 134ms/step - loss: 0.0014 - val_loss: 0.0027\n",
            "Epoch 47/50\n",
            "3/3 - 0s - 162ms/step - loss: 0.0015 - val_loss: 0.0027\n",
            "Epoch 48/50\n",
            "3/3 - 1s - 229ms/step - loss: 0.0014 - val_loss: 0.0025\n",
            "Epoch 49/50\n",
            "3/3 - 1s - 425ms/step - loss: 0.0015 - val_loss: 0.0023\n",
            "Epoch 50/50\n",
            "3/3 - 1s - 326ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step\n",
            "Training with Epochs: 100, Batch Size: 16\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 - 4s - 181ms/step - loss: 0.0179 - val_loss: 0.0032\n",
            "Epoch 2/100\n",
            "22/22 - 1s - 41ms/step - loss: 0.0032 - val_loss: 0.0031\n",
            "Epoch 3/100\n",
            "22/22 - 1s - 57ms/step - loss: 0.0022 - val_loss: 0.0054\n",
            "Epoch 4/100\n",
            "22/22 - 1s - 32ms/step - loss: 0.0019 - val_loss: 0.0033\n",
            "Epoch 5/100\n",
            "22/22 - 1s - 58ms/step - loss: 0.0017 - val_loss: 0.0043\n",
            "Epoch 6/100\n",
            "22/22 - 2s - 76ms/step - loss: 0.0018 - val_loss: 0.0021\n",
            "Epoch 7/100\n",
            "22/22 - 1s - 54ms/step - loss: 0.0016 - val_loss: 0.0033\n",
            "Epoch 8/100\n",
            "22/22 - 1s - 44ms/step - loss: 0.0016 - val_loss: 0.0031\n",
            "Epoch 9/100\n",
            "22/22 - 1s - 33ms/step - loss: 0.0015 - val_loss: 0.0021\n",
            "Epoch 10/100\n",
            "22/22 - 1s - 34ms/step - loss: 0.0014 - val_loss: 0.0028\n",
            "Epoch 11/100\n",
            "22/22 - 1s - 34ms/step - loss: 0.0014 - val_loss: 0.0028\n",
            "Epoch 12/100\n",
            "22/22 - 1s - 32ms/step - loss: 0.0016 - val_loss: 0.0021\n",
            "Epoch 13/100\n",
            "22/22 - 1s - 34ms/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 14/100\n",
            "22/22 - 1s - 57ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 15/100\n",
            "22/22 - 1s - 56ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 16/100\n",
            "22/22 - 1s - 33ms/step - loss: 0.0013 - val_loss: 0.0025\n",
            "Epoch 17/100\n",
            "22/22 - 1s - 33ms/step - loss: 0.0013 - val_loss: 0.0022\n",
            "Epoch 18/100\n",
            "22/22 - 1s - 33ms/step - loss: 0.0012 - val_loss: 0.0026\n",
            "Epoch 19/100\n",
            "22/22 - 1s - 33ms/step - loss: 0.0011 - val_loss: 0.0028\n",
            "Epoch 20/100\n",
            "22/22 - 2s - 79ms/step - loss: 0.0011 - val_loss: 0.0010\n",
            "Epoch 21/100\n",
            "22/22 - 1s - 57ms/step - loss: 0.0010 - val_loss: 8.4084e-04\n",
            "Epoch 22/100\n",
            "22/22 - 1s - 55ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 23/100\n",
            "22/22 - 1s - 36ms/step - loss: 0.0011 - val_loss: 0.0010\n",
            "Epoch 24/100\n",
            "22/22 - 1s - 33ms/step - loss: 0.0011 - val_loss: 8.7652e-04\n",
            "Epoch 25/100\n",
            "22/22 - 1s - 57ms/step - loss: 9.3900e-04 - val_loss: 0.0012\n",
            "Epoch 26/100\n",
            "22/22 - 1s - 58ms/step - loss: 8.4898e-04 - val_loss: 0.0012\n",
            "Epoch 27/100\n",
            "22/22 - 1s - 33ms/step - loss: 7.9186e-04 - val_loss: 7.3409e-04\n",
            "Epoch 28/100\n",
            "22/22 - 1s - 33ms/step - loss: 9.0774e-04 - val_loss: 7.3846e-04\n",
            "Epoch 29/100\n",
            "22/22 - 1s - 34ms/step - loss: 9.0086e-04 - val_loss: 0.0014\n",
            "Epoch 30/100\n",
            "22/22 - 1s - 55ms/step - loss: 9.5818e-04 - val_loss: 7.6567e-04\n",
            "Epoch 31/100\n",
            "22/22 - 1s - 58ms/step - loss: 9.0430e-04 - val_loss: 6.7705e-04\n",
            "Epoch 32/100\n",
            "22/22 - 1s - 32ms/step - loss: 7.7306e-04 - val_loss: 6.7693e-04\n",
            "Epoch 33/100\n",
            "22/22 - 1s - 43ms/step - loss: 8.2258e-04 - val_loss: 0.0012\n",
            "Epoch 34/100\n",
            "22/22 - 1s - 67ms/step - loss: 7.6379e-04 - val_loss: 0.0011\n",
            "Epoch 35/100\n",
            "22/22 - 1s - 55ms/step - loss: 7.8766e-04 - val_loss: 6.8304e-04\n",
            "Epoch 36/100\n",
            "22/22 - 1s - 43ms/step - loss: 7.2048e-04 - val_loss: 6.2162e-04\n",
            "Epoch 37/100\n",
            "22/22 - 1s - 34ms/step - loss: 9.2186e-04 - val_loss: 6.2973e-04\n",
            "Epoch 38/100\n",
            "22/22 - 1s - 32ms/step - loss: 7.6252e-04 - val_loss: 0.0014\n",
            "Epoch 39/100\n",
            "22/22 - 1s - 57ms/step - loss: 7.4823e-04 - val_loss: 7.9145e-04\n",
            "Epoch 40/100\n",
            "22/22 - 1s - 33ms/step - loss: 6.9249e-04 - val_loss: 7.1854e-04\n",
            "Epoch 41/100\n",
            "22/22 - 1s - 34ms/step - loss: 7.4344e-04 - val_loss: 8.4430e-04\n",
            "Epoch 42/100\n",
            "22/22 - 1s - 57ms/step - loss: 6.8920e-04 - val_loss: 6.4219e-04\n",
            "Epoch 43/100\n",
            "22/22 - 1s - 33ms/step - loss: 7.4547e-04 - val_loss: 0.0014\n",
            "Epoch 44/100\n",
            "22/22 - 1s - 57ms/step - loss: 7.2643e-04 - val_loss: 0.0017\n",
            "Epoch 45/100\n",
            "22/22 - 1s - 56ms/step - loss: 7.7160e-04 - val_loss: 0.0013\n",
            "Epoch 46/100\n",
            "22/22 - 2s - 71ms/step - loss: 6.6337e-04 - val_loss: 5.3931e-04\n",
            "Epoch 47/100\n",
            "22/22 - 1s - 52ms/step - loss: 6.2471e-04 - val_loss: 6.3482e-04\n",
            "Epoch 48/100\n",
            "22/22 - 1s - 54ms/step - loss: 6.5899e-04 - val_loss: 5.6355e-04\n",
            "Epoch 49/100\n",
            "22/22 - 1s - 45ms/step - loss: 7.0625e-04 - val_loss: 5.4650e-04\n",
            "Epoch 50/100\n",
            "22/22 - 1s - 33ms/step - loss: 6.8264e-04 - val_loss: 4.9572e-04\n",
            "Epoch 51/100\n",
            "22/22 - 1s - 33ms/step - loss: 7.2178e-04 - val_loss: 5.2151e-04\n",
            "Epoch 52/100\n",
            "22/22 - 1s - 33ms/step - loss: 6.2410e-04 - val_loss: 5.2353e-04\n",
            "Epoch 53/100\n",
            "22/22 - 1s - 33ms/step - loss: 6.9087e-04 - val_loss: 4.8368e-04\n",
            "Epoch 54/100\n",
            "22/22 - 1s - 57ms/step - loss: 6.9493e-04 - val_loss: 4.7843e-04\n",
            "Epoch 55/100\n",
            "22/22 - 1s - 57ms/step - loss: 6.3066e-04 - val_loss: 5.0855e-04\n",
            "Epoch 56/100\n",
            "22/22 - 1s - 33ms/step - loss: 5.8633e-04 - val_loss: 8.5151e-04\n",
            "Epoch 57/100\n",
            "22/22 - 1s - 58ms/step - loss: 6.1821e-04 - val_loss: 0.0014\n",
            "Epoch 58/100\n",
            "22/22 - 1s - 57ms/step - loss: 6.9497e-04 - val_loss: 8.5816e-04\n",
            "Epoch 59/100\n",
            "22/22 - 2s - 68ms/step - loss: 6.3655e-04 - val_loss: 4.6585e-04\n",
            "Epoch 60/100\n",
            "22/22 - 1s - 55ms/step - loss: 6.0339e-04 - val_loss: 6.6273e-04\n",
            "Epoch 61/100\n",
            "22/22 - 1s - 58ms/step - loss: 6.1964e-04 - val_loss: 4.4984e-04\n",
            "Epoch 62/100\n",
            "22/22 - 1s - 49ms/step - loss: 6.9552e-04 - val_loss: 4.5424e-04\n",
            "Epoch 63/100\n",
            "22/22 - 1s - 33ms/step - loss: 5.9017e-04 - val_loss: 0.0011\n",
            "Epoch 64/100\n",
            "22/22 - 1s - 33ms/step - loss: 6.6188e-04 - val_loss: 6.7026e-04\n",
            "Epoch 65/100\n",
            "22/22 - 1s - 32ms/step - loss: 6.2350e-04 - val_loss: 7.9305e-04\n",
            "Epoch 66/100\n",
            "22/22 - 1s - 58ms/step - loss: 6.3672e-04 - val_loss: 6.5123e-04\n",
            "Epoch 67/100\n",
            "22/22 - 1s - 58ms/step - loss: 6.2995e-04 - val_loss: 9.1771e-04\n",
            "Epoch 68/100\n",
            "22/22 - 1s - 57ms/step - loss: 6.4383e-04 - val_loss: 4.2294e-04\n",
            "Epoch 69/100\n",
            "22/22 - 1s - 57ms/step - loss: 5.8236e-04 - val_loss: 5.2337e-04\n",
            "Epoch 70/100\n",
            "22/22 - 1s - 58ms/step - loss: 5.7481e-04 - val_loss: 8.4506e-04\n",
            "Epoch 71/100\n",
            "22/22 - 1s - 59ms/step - loss: 5.7238e-04 - val_loss: 5.0896e-04\n",
            "Epoch 72/100\n",
            "22/22 - 1s - 55ms/step - loss: 5.4995e-04 - val_loss: 7.0934e-04\n",
            "Epoch 73/100\n",
            "22/22 - 1s - 54ms/step - loss: 5.8467e-04 - val_loss: 5.0571e-04\n",
            "Epoch 74/100\n",
            "22/22 - 1s - 57ms/step - loss: 7.1116e-04 - val_loss: 7.8406e-04\n",
            "Epoch 75/100\n",
            "22/22 - 1s - 38ms/step - loss: 5.5414e-04 - val_loss: 9.6620e-04\n",
            "Epoch 76/100\n",
            "22/22 - 1s - 53ms/step - loss: 6.2914e-04 - val_loss: 4.0657e-04\n",
            "Epoch 77/100\n",
            "22/22 - 1s - 57ms/step - loss: 6.4286e-04 - val_loss: 4.9806e-04\n",
            "Epoch 78/100\n",
            "22/22 - 1s - 33ms/step - loss: 5.7022e-04 - val_loss: 7.1969e-04\n",
            "Epoch 79/100\n",
            "22/22 - 1s - 57ms/step - loss: 6.5663e-04 - val_loss: 0.0016\n",
            "Epoch 80/100\n",
            "22/22 - 1s - 58ms/step - loss: 6.1861e-04 - val_loss: 5.6239e-04\n",
            "Epoch 81/100\n",
            "22/22 - 1s - 57ms/step - loss: 6.0946e-04 - val_loss: 3.9086e-04\n",
            "Epoch 82/100\n",
            "22/22 - 1s - 58ms/step - loss: 6.4420e-04 - val_loss: 3.9489e-04\n",
            "Epoch 83/100\n",
            "22/22 - 1s - 57ms/step - loss: 6.3804e-04 - val_loss: 4.3133e-04\n",
            "Epoch 84/100\n",
            "22/22 - 2s - 78ms/step - loss: 5.1899e-04 - val_loss: 5.6275e-04\n",
            "Epoch 85/100\n",
            "22/22 - 1s - 55ms/step - loss: 5.1665e-04 - val_loss: 5.2402e-04\n",
            "Epoch 86/100\n",
            "22/22 - 1s - 43ms/step - loss: 5.8887e-04 - val_loss: 5.9693e-04\n",
            "Epoch 87/100\n",
            "22/22 - 1s - 33ms/step - loss: 6.2004e-04 - val_loss: 3.8224e-04\n",
            "Epoch 88/100\n",
            "22/22 - 1s - 33ms/step - loss: 5.5794e-04 - val_loss: 3.8231e-04\n",
            "Epoch 89/100\n",
            "22/22 - 1s - 33ms/step - loss: 5.6750e-04 - val_loss: 7.5913e-04\n",
            "Epoch 90/100\n",
            "22/22 - 1s - 56ms/step - loss: 6.0404e-04 - val_loss: 5.3215e-04\n",
            "Epoch 91/100\n",
            "22/22 - 1s - 33ms/step - loss: 5.5200e-04 - val_loss: 3.8209e-04\n",
            "Epoch 92/100\n",
            "22/22 - 1s - 57ms/step - loss: 6.3258e-04 - val_loss: 3.6809e-04\n",
            "Epoch 93/100\n",
            "22/22 - 1s - 56ms/step - loss: 6.0644e-04 - val_loss: 4.4532e-04\n",
            "Epoch 94/100\n",
            "22/22 - 1s - 33ms/step - loss: 5.1380e-04 - val_loss: 3.9523e-04\n",
            "Epoch 95/100\n",
            "22/22 - 1s - 57ms/step - loss: 5.5162e-04 - val_loss: 3.6652e-04\n",
            "Epoch 96/100\n",
            "22/22 - 2s - 69ms/step - loss: 5.7645e-04 - val_loss: 6.8479e-04\n",
            "Epoch 97/100\n",
            "22/22 - 1s - 66ms/step - loss: 5.5258e-04 - val_loss: 3.6996e-04\n",
            "Epoch 98/100\n",
            "22/22 - 1s - 61ms/step - loss: 4.9349e-04 - val_loss: 5.9596e-04\n",
            "Epoch 99/100\n",
            "22/22 - 1s - 37ms/step - loss: 4.9470e-04 - val_loss: 6.5144e-04\n",
            "Epoch 100/100\n",
            "22/22 - 1s - 33ms/step - loss: 5.0469e-04 - val_loss: 3.5403e-04\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step\n",
            "Training with Epochs: 100, Batch Size: 32\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11/11 - 4s - 335ms/step - loss: 0.0312 - val_loss: 0.0335\n",
            "Epoch 2/100\n",
            "11/11 - 0s - 45ms/step - loss: 0.0075 - val_loss: 0.0033\n",
            "Epoch 3/100\n",
            "11/11 - 0s - 44ms/step - loss: 0.0035 - val_loss: 0.0121\n",
            "Epoch 4/100\n",
            "11/11 - 1s - 57ms/step - loss: 0.0029 - val_loss: 0.0035\n",
            "Epoch 5/100\n",
            "11/11 - 1s - 57ms/step - loss: 0.0023 - val_loss: 0.0042\n",
            "Epoch 6/100\n",
            "11/11 - 0s - 44ms/step - loss: 0.0020 - val_loss: 0.0051\n",
            "Epoch 7/100\n",
            "11/11 - 1s - 58ms/step - loss: 0.0021 - val_loss: 0.0033\n",
            "Epoch 8/100\n",
            "11/11 - 1s - 55ms/step - loss: 0.0020 - val_loss: 0.0029\n",
            "Epoch 9/100\n",
            "11/11 - 1s - 76ms/step - loss: 0.0020 - val_loss: 0.0047\n",
            "Epoch 10/100\n",
            "11/11 - 1s - 123ms/step - loss: 0.0021 - val_loss: 0.0063\n",
            "Epoch 11/100\n",
            "11/11 - 1s - 75ms/step - loss: 0.0022 - val_loss: 0.0042\n",
            "Epoch 12/100\n",
            "11/11 - 1s - 67ms/step - loss: 0.0019 - val_loss: 0.0038\n",
            "Epoch 13/100\n",
            "11/11 - 1s - 97ms/step - loss: 0.0017 - val_loss: 0.0022\n",
            "Epoch 14/100\n",
            "11/11 - 1s - 53ms/step - loss: 0.0018 - val_loss: 0.0030\n",
            "Epoch 15/100\n",
            "11/11 - 1s - 57ms/step - loss: 0.0016 - val_loss: 0.0033\n",
            "Epoch 16/100\n",
            "11/11 - 1s - 57ms/step - loss: 0.0017 - val_loss: 0.0024\n",
            "Epoch 17/100\n",
            "11/11 - 1s - 54ms/step - loss: 0.0016 - val_loss: 0.0018\n",
            "Epoch 18/100\n",
            "11/11 - 0s - 45ms/step - loss: 0.0017 - val_loss: 0.0018\n",
            "Epoch 19/100\n",
            "11/11 - 1s - 56ms/step - loss: 0.0016 - val_loss: 0.0022\n",
            "Epoch 20/100\n",
            "11/11 - 1s - 57ms/step - loss: 0.0014 - val_loss: 0.0031\n",
            "Epoch 21/100\n",
            "11/11 - 1s - 56ms/step - loss: 0.0014 - val_loss: 0.0031\n",
            "Epoch 22/100\n",
            "11/11 - 0s - 44ms/step - loss: 0.0014 - val_loss: 0.0031\n",
            "Epoch 23/100\n",
            "11/11 - 1s - 58ms/step - loss: 0.0013 - val_loss: 0.0030\n",
            "Epoch 24/100\n",
            "11/11 - 1s - 55ms/step - loss: 0.0014 - val_loss: 0.0026\n",
            "Epoch 25/100\n",
            "11/11 - 1s - 58ms/step - loss: 0.0013 - val_loss: 0.0027\n",
            "Epoch 26/100\n",
            "11/11 - 0s - 44ms/step - loss: 0.0013 - val_loss: 0.0021\n",
            "Epoch 27/100\n",
            "11/11 - 1s - 46ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "Epoch 28/100\n",
            "11/11 - 1s - 56ms/step - loss: 0.0013 - val_loss: 0.0019\n",
            "Epoch 29/100\n",
            "11/11 - 1s - 82ms/step - loss: 0.0012 - val_loss: 0.0028\n",
            "Epoch 30/100\n",
            "11/11 - 1s - 71ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "Epoch 31/100\n",
            "11/11 - 1s - 74ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 32/100\n",
            "11/11 - 1s - 80ms/step - loss: 0.0011 - val_loss: 0.0017\n",
            "Epoch 33/100\n",
            "11/11 - 1s - 81ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 34/100\n",
            "11/11 - 0s - 45ms/step - loss: 0.0012 - val_loss: 0.0018\n",
            "Epoch 35/100\n",
            "11/11 - 1s - 58ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 36/100\n",
            "11/11 - 1s - 54ms/step - loss: 0.0011 - val_loss: 0.0019\n",
            "Epoch 37/100\n",
            "11/11 - 1s - 58ms/step - loss: 0.0011 - val_loss: 0.0017\n",
            "Epoch 38/100\n",
            "11/11 - 0s - 45ms/step - loss: 0.0010 - val_loss: 0.0026\n",
            "Epoch 39/100\n",
            "11/11 - 1s - 46ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 40/100\n",
            "11/11 - 1s - 55ms/step - loss: 0.0010 - val_loss: 9.6277e-04\n",
            "Epoch 41/100\n",
            "11/11 - 0s - 45ms/step - loss: 9.7292e-04 - val_loss: 0.0011\n",
            "Epoch 42/100\n",
            "11/11 - 0s - 44ms/step - loss: 9.3790e-04 - val_loss: 8.9864e-04\n",
            "Epoch 43/100\n",
            "11/11 - 1s - 58ms/step - loss: 9.2304e-04 - val_loss: 9.0031e-04\n",
            "Epoch 44/100\n",
            "11/11 - 1s - 57ms/step - loss: 9.1980e-04 - val_loss: 0.0012\n",
            "Epoch 45/100\n",
            "11/11 - 1s - 55ms/step - loss: 8.3115e-04 - val_loss: 8.2471e-04\n",
            "Epoch 46/100\n",
            "11/11 - 0s - 45ms/step - loss: 8.8459e-04 - val_loss: 8.4495e-04\n",
            "Epoch 47/100\n",
            "11/11 - 0s - 44ms/step - loss: 8.4569e-04 - val_loss: 8.6304e-04\n",
            "Epoch 48/100\n",
            "11/11 - 1s - 46ms/step - loss: 7.8814e-04 - val_loss: 0.0012\n",
            "Epoch 49/100\n",
            "11/11 - 0s - 44ms/step - loss: 9.1005e-04 - val_loss: 8.3515e-04\n",
            "Epoch 50/100\n",
            "11/11 - 1s - 49ms/step - loss: 7.9344e-04 - val_loss: 9.4875e-04\n",
            "Epoch 51/100\n",
            "11/11 - 1s - 80ms/step - loss: 8.4143e-04 - val_loss: 7.5275e-04\n",
            "Epoch 52/100\n",
            "11/11 - 1s - 116ms/step - loss: 8.1699e-04 - val_loss: 0.0012\n",
            "Epoch 53/100\n",
            "11/11 - 1s - 87ms/step - loss: 8.1789e-04 - val_loss: 0.0015\n",
            "Epoch 54/100\n",
            "11/11 - 1s - 56ms/step - loss: 8.1618e-04 - val_loss: 7.1865e-04\n",
            "Epoch 55/100\n",
            "11/11 - 0s - 45ms/step - loss: 7.6662e-04 - val_loss: 0.0015\n",
            "Epoch 56/100\n",
            "11/11 - 1s - 56ms/step - loss: 8.3029e-04 - val_loss: 8.3733e-04\n",
            "Epoch 57/100\n",
            "11/11 - 1s - 57ms/step - loss: 8.0759e-04 - val_loss: 7.2184e-04\n",
            "Epoch 58/100\n",
            "11/11 - 0s - 45ms/step - loss: 9.2126e-04 - val_loss: 7.4362e-04\n",
            "Epoch 59/100\n",
            "11/11 - 0s - 45ms/step - loss: 8.4690e-04 - val_loss: 8.4211e-04\n",
            "Epoch 60/100\n",
            "11/11 - 0s - 45ms/step - loss: 7.2938e-04 - val_loss: 0.0012\n",
            "Epoch 61/100\n",
            "11/11 - 1s - 49ms/step - loss: 7.1790e-04 - val_loss: 0.0016\n",
            "Epoch 62/100\n",
            "11/11 - 0s - 44ms/step - loss: 7.4523e-04 - val_loss: 7.1154e-04\n",
            "Epoch 63/100\n",
            "11/11 - 1s - 58ms/step - loss: 7.3550e-04 - val_loss: 6.3148e-04\n",
            "Epoch 64/100\n",
            "11/11 - 0s - 45ms/step - loss: 7.2613e-04 - val_loss: 6.1692e-04\n",
            "Epoch 65/100\n",
            "11/11 - 0s - 45ms/step - loss: 7.1716e-04 - val_loss: 8.0432e-04\n",
            "Epoch 66/100\n",
            "11/11 - 1s - 60ms/step - loss: 6.8572e-04 - val_loss: 0.0011\n",
            "Epoch 67/100\n",
            "11/11 - 0s - 45ms/step - loss: 7.0250e-04 - val_loss: 9.2582e-04\n",
            "Epoch 68/100\n",
            "11/11 - 0s - 45ms/step - loss: 7.0166e-04 - val_loss: 8.3237e-04\n",
            "Epoch 69/100\n",
            "11/11 - 0s - 44ms/step - loss: 6.9128e-04 - val_loss: 0.0011\n",
            "Epoch 70/100\n",
            "11/11 - 0s - 45ms/step - loss: 7.0807e-04 - val_loss: 0.0013\n",
            "Epoch 71/100\n",
            "11/11 - 1s - 78ms/step - loss: 7.0819e-04 - val_loss: 8.7001e-04\n",
            "Epoch 72/100\n",
            "11/11 - 1s - 74ms/step - loss: 7.5207e-04 - val_loss: 5.6530e-04\n",
            "Epoch 73/100\n",
            "11/11 - 1s - 80ms/step - loss: 6.8902e-04 - val_loss: 5.6312e-04\n",
            "Epoch 74/100\n",
            "11/11 - 1s - 91ms/step - loss: 7.2741e-04 - val_loss: 9.4287e-04\n",
            "Epoch 75/100\n",
            "11/11 - 0s - 44ms/step - loss: 7.3913e-04 - val_loss: 9.1753e-04\n",
            "Epoch 76/100\n",
            "11/11 - 1s - 46ms/step - loss: 6.6939e-04 - val_loss: 9.2157e-04\n",
            "Epoch 77/100\n",
            "11/11 - 0s - 44ms/step - loss: 6.7328e-04 - val_loss: 7.7292e-04\n",
            "Epoch 78/100\n",
            "11/11 - 1s - 56ms/step - loss: 6.5618e-04 - val_loss: 8.2703e-04\n",
            "Epoch 79/100\n",
            "11/11 - 1s - 58ms/step - loss: 6.9159e-04 - val_loss: 5.4710e-04\n",
            "Epoch 80/100\n",
            "11/11 - 0s - 44ms/step - loss: 6.5235e-04 - val_loss: 8.0942e-04\n",
            "Epoch 81/100\n",
            "11/11 - 1s - 47ms/step - loss: 6.6273e-04 - val_loss: 5.5914e-04\n",
            "Epoch 82/100\n",
            "11/11 - 1s - 54ms/step - loss: 6.5753e-04 - val_loss: 5.3056e-04\n",
            "Epoch 83/100\n",
            "11/11 - 1s - 58ms/step - loss: 6.7062e-04 - val_loss: 6.7550e-04\n",
            "Epoch 84/100\n",
            "11/11 - 0s - 43ms/step - loss: 6.2357e-04 - val_loss: 9.2236e-04\n",
            "Epoch 85/100\n",
            "11/11 - 1s - 46ms/step - loss: 7.1426e-04 - val_loss: 6.7302e-04\n",
            "Epoch 86/100\n",
            "11/11 - 0s - 45ms/step - loss: 7.1593e-04 - val_loss: 5.0871e-04\n",
            "Epoch 87/100\n",
            "11/11 - 1s - 46ms/step - loss: 6.4556e-04 - val_loss: 6.6627e-04\n",
            "Epoch 88/100\n",
            "11/11 - 1s - 55ms/step - loss: 6.5075e-04 - val_loss: 5.0208e-04\n",
            "Epoch 89/100\n",
            "11/11 - 1s - 58ms/step - loss: 7.6304e-04 - val_loss: 5.0091e-04\n",
            "Epoch 90/100\n",
            "11/11 - 0s - 45ms/step - loss: 6.4004e-04 - val_loss: 5.4539e-04\n",
            "Epoch 91/100\n",
            "11/11 - 0s - 45ms/step - loss: 6.6878e-04 - val_loss: 5.2387e-04\n",
            "Epoch 92/100\n",
            "11/11 - 1s - 71ms/step - loss: 6.8273e-04 - val_loss: 0.0011\n",
            "Epoch 93/100\n",
            "11/11 - 1s - 128ms/step - loss: 6.5632e-04 - val_loss: 0.0011\n",
            "Epoch 94/100\n",
            "11/11 - 1s - 93ms/step - loss: 6.5641e-04 - val_loss: 7.1426e-04\n",
            "Epoch 95/100\n",
            "11/11 - 1s - 50ms/step - loss: 6.4254e-04 - val_loss: 4.9932e-04\n",
            "Epoch 96/100\n",
            "11/11 - 1s - 57ms/step - loss: 5.9797e-04 - val_loss: 6.6549e-04\n",
            "Epoch 97/100\n",
            "11/11 - 0s - 45ms/step - loss: 5.7375e-04 - val_loss: 7.5312e-04\n",
            "Epoch 98/100\n",
            "11/11 - 0s - 44ms/step - loss: 6.3185e-04 - val_loss: 7.8780e-04\n",
            "Epoch 99/100\n",
            "11/11 - 1s - 46ms/step - loss: 5.9534e-04 - val_loss: 6.0238e-04\n",
            "Epoch 100/100\n",
            "11/11 - 0s - 44ms/step - loss: 5.9393e-04 - val_loss: 7.5957e-04\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 152ms/step\n",
            "Training with Epochs: 100, Batch Size: 64\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 - 4s - 720ms/step - loss: 0.0366 - val_loss: 0.0066\n",
            "Epoch 2/100\n",
            "6/6 - 0s - 82ms/step - loss: 0.0061 - val_loss: 0.0288\n",
            "Epoch 3/100\n",
            "6/6 - 1s - 106ms/step - loss: 0.0058 - val_loss: 0.0030\n",
            "Epoch 4/100\n",
            "6/6 - 1s - 167ms/step - loss: 0.0043 - val_loss: 0.0037\n",
            "Epoch 5/100\n",
            "6/6 - 1s - 222ms/step - loss: 0.0027 - val_loss: 0.0099\n",
            "Epoch 6/100\n",
            "6/6 - 1s - 173ms/step - loss: 0.0026 - val_loss: 0.0037\n",
            "Epoch 7/100\n",
            "6/6 - 1s - 115ms/step - loss: 0.0023 - val_loss: 0.0036\n",
            "Epoch 8/100\n",
            "6/6 - 1s - 85ms/step - loss: 0.0021 - val_loss: 0.0049\n",
            "Epoch 9/100\n",
            "6/6 - 1s - 98ms/step - loss: 0.0020 - val_loss: 0.0031\n",
            "Epoch 10/100\n",
            "6/6 - 0s - 82ms/step - loss: 0.0020 - val_loss: 0.0034\n",
            "Epoch 11/100\n",
            "6/6 - 1s - 105ms/step - loss: 0.0019 - val_loss: 0.0035\n",
            "Epoch 12/100\n",
            "6/6 - 1s - 84ms/step - loss: 0.0019 - val_loss: 0.0031\n",
            "Epoch 13/100\n",
            "6/6 - 0s - 82ms/step - loss: 0.0018 - val_loss: 0.0033\n",
            "Epoch 14/100\n",
            "6/6 - 1s - 102ms/step - loss: 0.0018 - val_loss: 0.0027\n",
            "Epoch 15/100\n",
            "6/6 - 0s - 81ms/step - loss: 0.0017 - val_loss: 0.0030\n",
            "Epoch 16/100\n",
            "6/6 - 1s - 85ms/step - loss: 0.0017 - val_loss: 0.0029\n",
            "Epoch 17/100\n",
            "6/6 - 0s - 80ms/step - loss: 0.0017 - val_loss: 0.0029\n",
            "Epoch 18/100\n",
            "6/6 - 1s - 104ms/step - loss: 0.0017 - val_loss: 0.0028\n",
            "Epoch 19/100\n",
            "6/6 - 1s - 107ms/step - loss: 0.0017 - val_loss: 0.0029\n",
            "Epoch 20/100\n",
            "6/6 - 0s - 82ms/step - loss: 0.0016 - val_loss: 0.0028\n",
            "Epoch 21/100\n",
            "6/6 - 1s - 108ms/step - loss: 0.0016 - val_loss: 0.0024\n",
            "Epoch 22/100\n",
            "6/6 - 0s - 82ms/step - loss: 0.0017 - val_loss: 0.0031\n",
            "Epoch 23/100\n",
            "6/6 - 1s - 110ms/step - loss: 0.0017 - val_loss: 0.0021\n",
            "Epoch 24/100\n",
            "6/6 - 1s - 99ms/step - loss: 0.0016 - val_loss: 0.0032\n",
            "Epoch 25/100\n",
            "6/6 - 1s - 129ms/step - loss: 0.0015 - val_loss: 0.0020\n",
            "Epoch 26/100\n",
            "6/6 - 1s - 237ms/step - loss: 0.0016 - val_loss: 0.0033\n",
            "Epoch 27/100\n",
            "6/6 - 1s - 168ms/step - loss: 0.0015 - val_loss: 0.0019\n",
            "Epoch 28/100\n",
            "6/6 - 1s - 119ms/step - loss: 0.0015 - val_loss: 0.0032\n",
            "Epoch 29/100\n",
            "6/6 - 1s - 182ms/step - loss: 0.0015 - val_loss: 0.0021\n",
            "Epoch 30/100\n",
            "6/6 - 1s - 103ms/step - loss: 0.0014 - val_loss: 0.0025\n",
            "Epoch 31/100\n",
            "6/6 - 1s - 103ms/step - loss: 0.0014 - val_loss: 0.0022\n",
            "Epoch 32/100\n",
            "6/6 - 1s - 102ms/step - loss: 0.0013 - val_loss: 0.0026\n",
            "Epoch 33/100\n",
            "6/6 - 1s - 104ms/step - loss: 0.0014 - val_loss: 0.0023\n",
            "Epoch 34/100\n",
            "6/6 - 1s - 105ms/step - loss: 0.0013 - val_loss: 0.0018\n",
            "Epoch 35/100\n",
            "6/6 - 1s - 105ms/step - loss: 0.0015 - val_loss: 0.0030\n",
            "Epoch 36/100\n",
            "6/6 - 1s - 84ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 37/100\n",
            "6/6 - 1s - 104ms/step - loss: 0.0014 - val_loss: 0.0025\n",
            "Epoch 38/100\n",
            "6/6 - 1s - 104ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "Epoch 39/100\n",
            "6/6 - 1s - 103ms/step - loss: 0.0013 - val_loss: 0.0021\n",
            "Epoch 40/100\n",
            "6/6 - 1s - 105ms/step - loss: 0.0013 - val_loss: 0.0025\n",
            "Epoch 41/100\n",
            "6/6 - 1s - 96ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "Epoch 42/100\n",
            "6/6 - 1s - 86ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "Epoch 43/100\n",
            "6/6 - 0s - 80ms/step - loss: 0.0012 - val_loss: 0.0021\n",
            "Epoch 44/100\n",
            "6/6 - 1s - 166ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "Epoch 45/100\n",
            "6/6 - 1s - 158ms/step - loss: 0.0012 - val_loss: 0.0021\n",
            "Epoch 46/100\n",
            "6/6 - 1s - 218ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 47/100\n",
            "6/6 - 1s - 131ms/step - loss: 0.0012 - val_loss: 0.0022\n",
            "Epoch 48/100\n",
            "6/6 - 1s - 108ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 49/100\n",
            "6/6 - 0s - 82ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 50/100\n",
            "6/6 - 1s - 85ms/step - loss: 0.0011 - val_loss: 0.0021\n",
            "Epoch 51/100\n",
            "6/6 - 1s - 103ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 52/100\n",
            "6/6 - 1s - 99ms/step - loss: 0.0011 - val_loss: 0.0018\n",
            "Epoch 53/100\n",
            "6/6 - 1s - 86ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 54/100\n",
            "6/6 - 1s - 99ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 55/100\n",
            "6/6 - 1s - 89ms/step - loss: 0.0011 - val_loss: 0.0019\n",
            "Epoch 56/100\n",
            "6/6 - 1s - 95ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 57/100\n",
            "6/6 - 1s - 89ms/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 58/100\n",
            "6/6 - 0s - 82ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 59/100\n",
            "6/6 - 1s - 87ms/step - loss: 9.6077e-04 - val_loss: 0.0017\n",
            "Epoch 60/100\n",
            "6/6 - 0s - 82ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 61/100\n",
            "6/6 - 1s - 86ms/step - loss: 9.7372e-04 - val_loss: 0.0010\n",
            "Epoch 62/100\n",
            "6/6 - 1s - 98ms/step - loss: 9.7718e-04 - val_loss: 0.0014\n",
            "Epoch 63/100\n",
            "6/6 - 1s - 87ms/step - loss: 9.7276e-04 - val_loss: 0.0011\n",
            "Epoch 64/100\n",
            "6/6 - 1s - 98ms/step - loss: 9.0469e-04 - val_loss: 0.0010\n",
            "Epoch 65/100\n",
            "6/6 - 1s - 172ms/step - loss: 9.4691e-04 - val_loss: 9.2812e-04\n",
            "Epoch 66/100\n",
            "6/6 - 1s - 164ms/step - loss: 9.5296e-04 - val_loss: 9.5011e-04\n",
            "Epoch 67/100\n",
            "6/6 - 1s - 162ms/step - loss: 9.2338e-04 - val_loss: 0.0017\n",
            "Epoch 68/100\n",
            "6/6 - 1s - 168ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 69/100\n",
            "6/6 - 1s - 112ms/step - loss: 9.1599e-04 - val_loss: 8.4640e-04\n",
            "Epoch 70/100\n",
            "6/6 - 1s - 183ms/step - loss: 9.5426e-04 - val_loss: 0.0013\n",
            "Epoch 71/100\n",
            "6/6 - 1s - 85ms/step - loss: 9.1289e-04 - val_loss: 0.0016\n",
            "Epoch 72/100\n",
            "6/6 - 1s - 105ms/step - loss: 9.4154e-04 - val_loss: 7.6401e-04\n",
            "Epoch 73/100\n",
            "6/6 - 0s - 80ms/step - loss: 8.5395e-04 - val_loss: 0.0010\n",
            "Epoch 74/100\n",
            "6/6 - 1s - 83ms/step - loss: 8.8235e-04 - val_loss: 0.0013\n",
            "Epoch 75/100\n",
            "6/6 - 1s - 105ms/step - loss: 8.8510e-04 - val_loss: 7.8271e-04\n",
            "Epoch 76/100\n",
            "6/6 - 0s - 80ms/step - loss: 8.8686e-04 - val_loss: 0.0010\n",
            "Epoch 77/100\n",
            "6/6 - 0s - 78ms/step - loss: 8.6989e-04 - val_loss: 0.0015\n",
            "Epoch 78/100\n",
            "6/6 - 0s - 80ms/step - loss: 9.0648e-04 - val_loss: 7.5174e-04\n",
            "Epoch 79/100\n",
            "6/6 - 1s - 85ms/step - loss: 8.4178e-04 - val_loss: 0.0014\n",
            "Epoch 80/100\n",
            "6/6 - 1s - 99ms/step - loss: 8.2303e-04 - val_loss: 7.8606e-04\n",
            "Epoch 81/100\n",
            "6/6 - 1s - 106ms/step - loss: 8.7873e-04 - val_loss: 7.1384e-04\n",
            "Epoch 82/100\n",
            "6/6 - 1s - 103ms/step - loss: 8.4271e-04 - val_loss: 0.0011\n",
            "Epoch 83/100\n",
            "6/6 - 0s - 82ms/step - loss: 8.1619e-04 - val_loss: 0.0010\n",
            "Epoch 84/100\n",
            "6/6 - 0s - 82ms/step - loss: 8.1348e-04 - val_loss: 0.0010\n",
            "Epoch 85/100\n",
            "6/6 - 1s - 84ms/step - loss: 7.9414e-04 - val_loss: 9.1349e-04\n",
            "Epoch 86/100\n",
            "6/6 - 0s - 82ms/step - loss: 7.8042e-04 - val_loss: 6.9886e-04\n",
            "Epoch 87/100\n",
            "6/6 - 1s - 178ms/step - loss: 9.5291e-04 - val_loss: 0.0014\n",
            "Epoch 88/100\n",
            "6/6 - 1s - 218ms/step - loss: 8.8980e-04 - val_loss: 0.0013\n",
            "Epoch 89/100\n",
            "6/6 - 1s - 139ms/step - loss: 8.1327e-04 - val_loss: 7.0054e-04\n",
            "Epoch 90/100\n",
            "6/6 - 1s - 86ms/step - loss: 7.7504e-04 - val_loss: 9.6720e-04\n",
            "Epoch 91/100\n",
            "6/6 - 1s - 100ms/step - loss: 7.5995e-04 - val_loss: 8.0156e-04\n",
            "Epoch 92/100\n",
            "6/6 - 1s - 102ms/step - loss: 7.8194e-04 - val_loss: 0.0011\n",
            "Epoch 93/100\n",
            "6/6 - 1s - 111ms/step - loss: 7.9839e-04 - val_loss: 7.9220e-04\n",
            "Epoch 94/100\n",
            "6/6 - 1s - 97ms/step - loss: 7.6913e-04 - val_loss: 7.6134e-04\n",
            "Epoch 95/100\n",
            "6/6 - 1s - 104ms/step - loss: 7.7704e-04 - val_loss: 9.3705e-04\n",
            "Epoch 96/100\n",
            "6/6 - 0s - 78ms/step - loss: 7.5968e-04 - val_loss: 8.6188e-04\n",
            "Epoch 97/100\n",
            "6/6 - 1s - 109ms/step - loss: 7.5831e-04 - val_loss: 7.5803e-04\n",
            "Epoch 98/100\n",
            "6/6 - 1s - 101ms/step - loss: 7.2346e-04 - val_loss: 0.0011\n",
            "Epoch 99/100\n",
            "6/6 - 0s - 83ms/step - loss: 8.1861e-04 - val_loss: 7.1838e-04\n",
            "Epoch 100/100\n",
            "6/6 - 0s - 80ms/step - loss: 7.6789e-04 - val_loss: 0.0011\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step\n",
            "Training with Epochs: 100, Batch Size: 128\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 - 4s - 1s/step - loss: 0.0786 - val_loss: 0.0222\n",
            "Epoch 2/100\n",
            "3/3 - 1s - 235ms/step - loss: 0.0111 - val_loss: 0.0170\n",
            "Epoch 3/100\n",
            "3/3 - 1s - 332ms/step - loss: 0.0105 - val_loss: 0.0096\n",
            "Epoch 4/100\n",
            "3/3 - 1s - 203ms/step - loss: 0.0052 - val_loss: 0.0285\n",
            "Epoch 5/100\n",
            "3/3 - 1s - 207ms/step - loss: 0.0089 - val_loss: 0.0207\n",
            "Epoch 6/100\n",
            "3/3 - 1s - 203ms/step - loss: 0.0048 - val_loss: 0.0061\n",
            "Epoch 7/100\n",
            "3/3 - 1s - 215ms/step - loss: 0.0031 - val_loss: 0.0029\n",
            "Epoch 8/100\n",
            "3/3 - 1s - 201ms/step - loss: 0.0049 - val_loss: 0.0028\n",
            "Epoch 9/100\n",
            "3/3 - 1s - 208ms/step - loss: 0.0031 - val_loss: 0.0061\n",
            "Epoch 10/100\n",
            "3/3 - 1s - 209ms/step - loss: 0.0026 - val_loss: 0.0105\n",
            "Epoch 11/100\n",
            "3/3 - 0s - 131ms/step - loss: 0.0033 - val_loss: 0.0095\n",
            "Epoch 12/100\n",
            "3/3 - 0s - 132ms/step - loss: 0.0027 - val_loss: 0.0054\n",
            "Epoch 13/100\n",
            "3/3 - 0s - 131ms/step - loss: 0.0023 - val_loss: 0.0032\n",
            "Epoch 14/100\n",
            "3/3 - 0s - 134ms/step - loss: 0.0025 - val_loss: 0.0031\n",
            "Epoch 15/100\n",
            "3/3 - 0s - 130ms/step - loss: 0.0023 - val_loss: 0.0043\n",
            "Epoch 16/100\n",
            "3/3 - 0s - 135ms/step - loss: 0.0020 - val_loss: 0.0057\n",
            "Epoch 17/100\n",
            "3/3 - 0s - 124ms/step - loss: 0.0021 - val_loss: 0.0051\n",
            "Epoch 18/100\n",
            "3/3 - 1s - 216ms/step - loss: 0.0021 - val_loss: 0.0037\n",
            "Epoch 19/100\n",
            "3/3 - 1s - 203ms/step - loss: 0.0020 - val_loss: 0.0030\n",
            "Epoch 20/100\n",
            "3/3 - 1s - 208ms/step - loss: 0.0020 - val_loss: 0.0030\n",
            "Epoch 21/100\n",
            "3/3 - 0s - 146ms/step - loss: 0.0019 - val_loss: 0.0037\n",
            "Epoch 22/100\n",
            "3/3 - 1s - 281ms/step - loss: 0.0019 - val_loss: 0.0040\n",
            "Epoch 23/100\n",
            "3/3 - 1s - 232ms/step - loss: 0.0019 - val_loss: 0.0035\n",
            "Epoch 24/100\n",
            "3/3 - 1s - 229ms/step - loss: 0.0019 - val_loss: 0.0030\n",
            "Epoch 25/100\n",
            "3/3 - 1s - 333ms/step - loss: 0.0019 - val_loss: 0.0029\n",
            "Epoch 26/100\n",
            "3/3 - 0s - 132ms/step - loss: 0.0019 - val_loss: 0.0033\n",
            "Epoch 27/100\n",
            "3/3 - 1s - 212ms/step - loss: 0.0018 - val_loss: 0.0035\n",
            "Epoch 28/100\n",
            "3/3 - 0s - 127ms/step - loss: 0.0018 - val_loss: 0.0033\n",
            "Epoch 29/100\n",
            "3/3 - 1s - 214ms/step - loss: 0.0018 - val_loss: 0.0030\n",
            "Epoch 30/100\n",
            "3/3 - 0s - 132ms/step - loss: 0.0017 - val_loss: 0.0030\n",
            "Epoch 31/100\n",
            "3/3 - 0s - 129ms/step - loss: 0.0018 - val_loss: 0.0033\n",
            "Epoch 32/100\n",
            "3/3 - 0s - 136ms/step - loss: 0.0018 - val_loss: 0.0032\n",
            "Epoch 33/100\n",
            "3/3 - 1s - 197ms/step - loss: 0.0019 - val_loss: 0.0030\n",
            "Epoch 34/100\n",
            "3/3 - 0s - 134ms/step - loss: 0.0018 - val_loss: 0.0029\n",
            "Epoch 35/100\n",
            "3/3 - 0s - 132ms/step - loss: 0.0018 - val_loss: 0.0030\n",
            "Epoch 36/100\n",
            "3/3 - 0s - 133ms/step - loss: 0.0017 - val_loss: 0.0031\n",
            "Epoch 37/100\n",
            "3/3 - 0s - 129ms/step - loss: 0.0017 - val_loss: 0.0029\n",
            "Epoch 38/100\n",
            "3/3 - 0s - 127ms/step - loss: 0.0017 - val_loss: 0.0030\n",
            "Epoch 39/100\n",
            "3/3 - 0s - 147ms/step - loss: 0.0017 - val_loss: 0.0032\n",
            "Epoch 40/100\n",
            "3/3 - 1s - 185ms/step - loss: 0.0017 - val_loss: 0.0029\n",
            "Epoch 41/100\n",
            "3/3 - 0s - 139ms/step - loss: 0.0017 - val_loss: 0.0030\n",
            "Epoch 42/100\n",
            "3/3 - 0s - 127ms/step - loss: 0.0017 - val_loss: 0.0030\n",
            "Epoch 43/100\n",
            "3/3 - 0s - 130ms/step - loss: 0.0016 - val_loss: 0.0029\n",
            "Epoch 44/100\n",
            "3/3 - 0s - 135ms/step - loss: 0.0016 - val_loss: 0.0027\n",
            "Epoch 45/100\n",
            "3/3 - 0s - 124ms/step - loss: 0.0016 - val_loss: 0.0027\n",
            "Epoch 46/100\n",
            "3/3 - 0s - 136ms/step - loss: 0.0016 - val_loss: 0.0029\n",
            "Epoch 47/100\n",
            "3/3 - 1s - 285ms/step - loss: 0.0016 - val_loss: 0.0030\n",
            "Epoch 48/100\n",
            "3/3 - 1s - 431ms/step - loss: 0.0015 - val_loss: 0.0028\n",
            "Epoch 49/100\n",
            "3/3 - 1s - 233ms/step - loss: 0.0016 - val_loss: 0.0028\n",
            "Epoch 50/100\n",
            "3/3 - 1s - 240ms/step - loss: 0.0015 - val_loss: 0.0028\n",
            "Epoch 51/100\n",
            "3/3 - 1s - 310ms/step - loss: 0.0016 - val_loss: 0.0027\n",
            "Epoch 52/100\n",
            "3/3 - 1s - 212ms/step - loss: 0.0015 - val_loss: 0.0027\n",
            "Epoch 53/100\n",
            "3/3 - 0s - 126ms/step - loss: 0.0016 - val_loss: 0.0029\n",
            "Epoch 54/100\n",
            "3/3 - 1s - 215ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 55/100\n",
            "3/3 - 1s - 202ms/step - loss: 0.0016 - val_loss: 0.0025\n",
            "Epoch 56/100\n",
            "3/3 - 0s - 129ms/step - loss: 0.0015 - val_loss: 0.0030\n",
            "Epoch 57/100\n",
            "3/3 - 0s - 125ms/step - loss: 0.0015 - val_loss: 0.0026\n",
            "Epoch 58/100\n",
            "3/3 - 1s - 209ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "Epoch 59/100\n",
            "3/3 - 0s - 128ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 60/100\n",
            "3/3 - 1s - 211ms/step - loss: 0.0015 - val_loss: 0.0028\n",
            "Epoch 61/100\n",
            "3/3 - 0s - 131ms/step - loss: 0.0014 - val_loss: 0.0026\n",
            "Epoch 62/100\n",
            "3/3 - 1s - 210ms/step - loss: 0.0014 - val_loss: 0.0022\n",
            "Epoch 63/100\n",
            "3/3 - 0s - 128ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 64/100\n",
            "3/3 - 1s - 212ms/step - loss: 0.0015 - val_loss: 0.0027\n",
            "Epoch 65/100\n",
            "3/3 - 1s - 206ms/step - loss: 0.0014 - val_loss: 0.0022\n",
            "Epoch 66/100\n",
            "3/3 - 0s - 127ms/step - loss: 0.0014 - val_loss: 0.0024\n",
            "Epoch 67/100\n",
            "3/3 - 1s - 211ms/step - loss: 0.0014 - val_loss: 0.0026\n",
            "Epoch 68/100\n",
            "3/3 - 0s - 128ms/step - loss: 0.0015 - val_loss: 0.0023\n",
            "Epoch 69/100\n",
            "3/3 - 1s - 240ms/step - loss: 0.0014 - val_loss: 0.0021\n",
            "Epoch 70/100\n",
            "3/3 - 1s - 224ms/step - loss: 0.0014 - val_loss: 0.0026\n",
            "Epoch 71/100\n",
            "3/3 - 1s - 429ms/step - loss: 0.0014 - val_loss: 0.0022\n",
            "Epoch 72/100\n",
            "3/3 - 1s - 324ms/step - loss: 0.0014 - val_loss: 0.0020\n",
            "Epoch 73/100\n",
            "3/3 - 1s - 208ms/step - loss: 0.0014 - val_loss: 0.0026\n",
            "Epoch 74/100\n",
            "3/3 - 1s - 204ms/step - loss: 0.0013 - val_loss: 0.0022\n",
            "Epoch 75/100\n",
            "3/3 - 1s - 210ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "Epoch 76/100\n",
            "3/3 - 0s - 128ms/step - loss: 0.0013 - val_loss: 0.0023\n",
            "Epoch 77/100\n",
            "3/3 - 1s - 214ms/step - loss: 0.0014 - val_loss: 0.0024\n",
            "Epoch 78/100\n",
            "3/3 - 1s - 194ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "Epoch 79/100\n",
            "3/3 - 1s - 211ms/step - loss: 0.0013 - val_loss: 0.0019\n",
            "Epoch 80/100\n",
            "3/3 - 1s - 208ms/step - loss: 0.0013 - val_loss: 0.0024\n",
            "Epoch 81/100\n",
            "3/3 - 0s - 125ms/step - loss: 0.0013 - val_loss: 0.0021\n",
            "Epoch 82/100\n",
            "3/3 - 0s - 129ms/step - loss: 0.0013 - val_loss: 0.0019\n",
            "Epoch 83/100\n",
            "3/3 - 0s - 134ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "Epoch 84/100\n",
            "3/3 - 1s - 200ms/step - loss: 0.0013 - val_loss: 0.0023\n",
            "Epoch 85/100\n",
            "3/3 - 1s - 208ms/step - loss: 0.0014 - val_loss: 0.0019\n",
            "Epoch 86/100\n",
            "3/3 - 1s - 210ms/step - loss: 0.0012 - val_loss: 0.0019\n",
            "Epoch 87/100\n",
            "3/3 - 1s - 203ms/step - loss: 0.0012 - val_loss: 0.0022\n",
            "Epoch 88/100\n",
            "3/3 - 1s - 206ms/step - loss: 0.0012 - val_loss: 0.0019\n",
            "Epoch 89/100\n",
            "3/3 - 0s - 126ms/step - loss: 0.0013 - val_loss: 0.0019\n",
            "Epoch 90/100\n",
            "3/3 - 1s - 219ms/step - loss: 0.0012 - val_loss: 0.0020\n",
            "Epoch 91/100\n",
            "3/3 - 1s - 221ms/step - loss: 0.0012 - val_loss: 0.0019\n",
            "Epoch 92/100\n",
            "3/3 - 1s - 430ms/step - loss: 0.0012 - val_loss: 0.0020\n",
            "Epoch 93/100\n",
            "3/3 - 1s - 234ms/step - loss: 0.0012 - val_loss: 0.0019\n",
            "Epoch 94/100\n",
            "3/3 - 0s - 161ms/step - loss: 0.0012 - val_loss: 0.0019\n",
            "Epoch 95/100\n",
            "3/3 - 0s - 150ms/step - loss: 0.0012 - val_loss: 0.0019\n",
            "Epoch 96/100\n",
            "3/3 - 1s - 211ms/step - loss: 0.0012 - val_loss: 0.0017\n",
            "Epoch 97/100\n",
            "3/3 - 0s - 152ms/step - loss: 0.0012 - val_loss: 0.0017\n",
            "Epoch 98/100\n",
            "3/3 - 0s - 158ms/step - loss: 0.0011 - val_loss: 0.0019\n",
            "Epoch 99/100\n",
            "3/3 - 1s - 207ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "Epoch 100/100\n",
            "3/3 - 0s - 162ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 190ms/step\n",
            "Training with Epochs: 150, Batch Size: 16\n",
            "Epoch 1/150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 - 5s - 231ms/step - loss: 0.0212 - val_loss: 0.0026\n",
            "Epoch 2/150\n",
            "22/22 - 1s - 59ms/step - loss: 0.0032 - val_loss: 0.0031\n",
            "Epoch 3/150\n",
            "22/22 - 1s - 56ms/step - loss: 0.0021 - val_loss: 0.0040\n",
            "Epoch 4/150\n",
            "22/22 - 1s - 58ms/step - loss: 0.0017 - val_loss: 0.0036\n",
            "Epoch 5/150\n",
            "22/22 - 2s - 93ms/step - loss: 0.0018 - val_loss: 0.0028\n",
            "Epoch 6/150\n",
            "22/22 - 1s - 58ms/step - loss: 0.0016 - val_loss: 0.0026\n",
            "Epoch 7/150\n",
            "22/22 - 1s - 58ms/step - loss: 0.0015 - val_loss: 0.0023\n",
            "Epoch 8/150\n",
            "22/22 - 1s - 58ms/step - loss: 0.0015 - val_loss: 0.0037\n",
            "Epoch 9/150\n",
            "22/22 - 1s - 56ms/step - loss: 0.0014 - val_loss: 0.0019\n",
            "Epoch 10/150\n",
            "22/22 - 1s - 33ms/step - loss: 0.0014 - val_loss: 0.0029\n",
            "Epoch 11/150\n",
            "22/22 - 1s - 34ms/step - loss: 0.0013 - val_loss: 0.0021\n",
            "Epoch 12/150\n",
            "22/22 - 1s - 33ms/step - loss: 0.0013 - val_loss: 0.0012\n",
            "Epoch 13/150\n",
            "22/22 - 1s - 66ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 14/150\n",
            "22/22 - 2s - 69ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 15/150\n",
            "22/22 - 1s - 56ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 16/150\n",
            "22/22 - 1s - 43ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 17/150\n",
            "22/22 - 1s - 48ms/step - loss: 0.0011 - val_loss: 0.0010\n",
            "Epoch 18/150\n",
            "22/22 - 1s - 34ms/step - loss: 0.0010 - val_loss: 0.0022\n",
            "Epoch 19/150\n",
            "22/22 - 1s - 34ms/step - loss: 0.0011 - val_loss: 0.0019\n",
            "Epoch 20/150\n",
            "22/22 - 1s - 33ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 21/150\n",
            "22/22 - 1s - 34ms/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 22/150\n",
            "22/22 - 1s - 34ms/step - loss: 0.0010 - val_loss: 7.9401e-04\n",
            "Epoch 23/150\n",
            "22/22 - 1s - 58ms/step - loss: 0.0010 - val_loss: 8.1822e-04\n",
            "Epoch 24/150\n",
            "22/22 - 1s - 35ms/step - loss: 8.2429e-04 - val_loss: 8.7100e-04\n",
            "Epoch 25/150\n",
            "22/22 - 1s - 34ms/step - loss: 9.2924e-04 - val_loss: 8.9002e-04\n",
            "Epoch 26/150\n",
            "22/22 - 1s - 34ms/step - loss: 8.4774e-04 - val_loss: 7.2228e-04\n",
            "Epoch 27/150\n",
            "22/22 - 1s - 34ms/step - loss: 9.6223e-04 - val_loss: 0.0024\n",
            "Epoch 28/150\n",
            "22/22 - 1s - 46ms/step - loss: 9.6449e-04 - val_loss: 8.7749e-04\n",
            "Epoch 29/150\n",
            "22/22 - 1s - 65ms/step - loss: 9.7407e-04 - val_loss: 7.6043e-04\n",
            "Epoch 30/150\n",
            "22/22 - 1s - 57ms/step - loss: 8.6763e-04 - val_loss: 0.0016\n",
            "Epoch 31/150\n",
            "22/22 - 1s - 44ms/step - loss: 7.9392e-04 - val_loss: 9.3438e-04\n",
            "Epoch 32/150\n",
            "22/22 - 1s - 34ms/step - loss: 7.0417e-04 - val_loss: 8.7047e-04\n",
            "Epoch 33/150\n",
            "22/22 - 1s - 57ms/step - loss: 7.3860e-04 - val_loss: 7.2667e-04\n",
            "Epoch 34/150\n",
            "22/22 - 1s - 34ms/step - loss: 7.9662e-04 - val_loss: 7.1677e-04\n",
            "Epoch 35/150\n",
            "22/22 - 1s - 35ms/step - loss: 8.0648e-04 - val_loss: 6.6698e-04\n",
            "Epoch 36/150\n",
            "22/22 - 1s - 57ms/step - loss: 8.7333e-04 - val_loss: 9.2347e-04\n",
            "Epoch 37/150\n",
            "22/22 - 1s - 57ms/step - loss: 9.1491e-04 - val_loss: 0.0012\n",
            "Epoch 38/150\n",
            "22/22 - 1s - 58ms/step - loss: 9.7061e-04 - val_loss: 0.0012\n",
            "Epoch 39/150\n",
            "22/22 - 1s - 58ms/step - loss: 7.0028e-04 - val_loss: 8.4886e-04\n",
            "Epoch 40/150\n",
            "22/22 - 1s - 63ms/step - loss: 7.1064e-04 - val_loss: 5.7257e-04\n",
            "Epoch 41/150\n",
            "22/22 - 1s - 55ms/step - loss: 8.6259e-04 - val_loss: 9.1213e-04\n",
            "Epoch 42/150\n",
            "22/22 - 1s - 57ms/step - loss: 8.2976e-04 - val_loss: 7.1338e-04\n",
            "Epoch 43/150\n",
            "22/22 - 1s - 46ms/step - loss: 7.4609e-04 - val_loss: 9.0982e-04\n",
            "Epoch 44/150\n",
            "22/22 - 1s - 34ms/step - loss: 7.6668e-04 - val_loss: 9.7485e-04\n",
            "Epoch 45/150\n",
            "22/22 - 1s - 57ms/step - loss: 6.8131e-04 - val_loss: 6.1498e-04\n",
            "Epoch 46/150\n",
            "22/22 - 1s - 59ms/step - loss: 6.8555e-04 - val_loss: 6.5757e-04\n",
            "Epoch 47/150\n",
            "22/22 - 1s - 55ms/step - loss: 6.4571e-04 - val_loss: 6.9034e-04\n",
            "Epoch 48/150\n",
            "22/22 - 1s - 34ms/step - loss: 6.8391e-04 - val_loss: 5.8197e-04\n",
            "Epoch 49/150\n",
            "22/22 - 1s - 58ms/step - loss: 6.7159e-04 - val_loss: 5.8619e-04\n",
            "Epoch 50/150\n",
            "22/22 - 1s - 34ms/step - loss: 7.0054e-04 - val_loss: 6.1529e-04\n",
            "Epoch 51/150\n",
            "22/22 - 1s - 58ms/step - loss: 6.6774e-04 - val_loss: 8.9846e-04\n",
            "Epoch 52/150\n",
            "22/22 - 1s - 34ms/step - loss: 6.5942e-04 - val_loss: 5.3247e-04\n",
            "Epoch 53/150\n",
            "22/22 - 1s - 48ms/step - loss: 7.0848e-04 - val_loss: 4.9275e-04\n",
            "Epoch 54/150\n",
            "22/22 - 1s - 65ms/step - loss: 6.7156e-04 - val_loss: 5.0541e-04\n",
            "Epoch 55/150\n",
            "22/22 - 1s - 59ms/step - loss: 7.1463e-04 - val_loss: 5.1616e-04\n",
            "Epoch 56/150\n",
            "22/22 - 1s - 41ms/step - loss: 6.3345e-04 - val_loss: 0.0012\n",
            "Epoch 57/150\n",
            "22/22 - 1s - 51ms/step - loss: 5.9104e-04 - val_loss: 0.0011\n",
            "Epoch 58/150\n",
            "22/22 - 1s - 34ms/step - loss: 6.1133e-04 - val_loss: 9.2199e-04\n",
            "Epoch 59/150\n",
            "22/22 - 1s - 34ms/step - loss: 7.6626e-04 - val_loss: 8.4208e-04\n",
            "Epoch 60/150\n",
            "22/22 - 1s - 58ms/step - loss: 6.3600e-04 - val_loss: 5.7117e-04\n",
            "Epoch 61/150\n",
            "22/22 - 1s - 34ms/step - loss: 6.7277e-04 - val_loss: 7.6365e-04\n",
            "Epoch 62/150\n",
            "22/22 - 1s - 33ms/step - loss: 6.4095e-04 - val_loss: 0.0013\n",
            "Epoch 63/150\n",
            "22/22 - 1s - 58ms/step - loss: 6.8511e-04 - val_loss: 9.3414e-04\n",
            "Epoch 64/150\n",
            "22/22 - 1s - 58ms/step - loss: 7.4095e-04 - val_loss: 0.0015\n",
            "Epoch 65/150\n",
            "22/22 - 1s - 57ms/step - loss: 8.1705e-04 - val_loss: 4.5853e-04\n",
            "Epoch 66/150\n",
            "22/22 - 2s - 78ms/step - loss: 6.5713e-04 - val_loss: 5.1178e-04\n",
            "Epoch 67/150\n",
            "22/22 - 1s - 59ms/step - loss: 7.3727e-04 - val_loss: 7.4289e-04\n",
            "Epoch 68/150\n",
            "22/22 - 2s - 93ms/step - loss: 6.1631e-04 - val_loss: 4.8797e-04\n",
            "Epoch 69/150\n",
            "22/22 - 1s - 34ms/step - loss: 6.1083e-04 - val_loss: 4.5986e-04\n",
            "Epoch 70/150\n",
            "22/22 - 1s - 58ms/step - loss: 5.8682e-04 - val_loss: 7.0672e-04\n",
            "Epoch 71/150\n",
            "22/22 - 1s - 57ms/step - loss: 6.1675e-04 - val_loss: 4.3786e-04\n",
            "Epoch 72/150\n",
            "22/22 - 1s - 58ms/step - loss: 6.4200e-04 - val_loss: 8.8106e-04\n",
            "Epoch 73/150\n",
            "22/22 - 1s - 56ms/step - loss: 6.1626e-04 - val_loss: 0.0017\n",
            "Epoch 74/150\n",
            "22/22 - 1s - 58ms/step - loss: 6.5318e-04 - val_loss: 9.8380e-04\n",
            "Epoch 75/150\n",
            "22/22 - 1s - 57ms/step - loss: 5.7772e-04 - val_loss: 5.5574e-04\n",
            "Epoch 76/150\n",
            "22/22 - 1s - 54ms/step - loss: 5.6043e-04 - val_loss: 9.6583e-04\n",
            "Epoch 77/150\n",
            "22/22 - 1s - 58ms/step - loss: 7.1744e-04 - val_loss: 7.9934e-04\n",
            "Epoch 78/150\n",
            "22/22 - 1s - 59ms/step - loss: 5.5532e-04 - val_loss: 4.2087e-04\n",
            "Epoch 79/150\n",
            "22/22 - 1s - 39ms/step - loss: 6.2810e-04 - val_loss: 4.3569e-04\n",
            "Epoch 80/150\n",
            "22/22 - 1s - 52ms/step - loss: 6.6658e-04 - val_loss: 8.6017e-04\n",
            "Epoch 81/150\n",
            "22/22 - 1s - 35ms/step - loss: 5.8588e-04 - val_loss: 4.1998e-04\n",
            "Epoch 82/150\n",
            "22/22 - 1s - 56ms/step - loss: 5.8641e-04 - val_loss: 4.4736e-04\n",
            "Epoch 83/150\n",
            "22/22 - 1s - 58ms/step - loss: 6.7946e-04 - val_loss: 6.2147e-04\n",
            "Epoch 84/150\n",
            "22/22 - 1s - 33ms/step - loss: 5.4968e-04 - val_loss: 6.1306e-04\n",
            "Epoch 85/150\n",
            "22/22 - 1s - 57ms/step - loss: 5.8903e-04 - val_loss: 4.1122e-04\n",
            "Epoch 86/150\n",
            "22/22 - 1s - 58ms/step - loss: 5.5264e-04 - val_loss: 5.2138e-04\n",
            "Epoch 87/150\n",
            "22/22 - 1s - 34ms/step - loss: 5.7928e-04 - val_loss: 4.6163e-04\n",
            "Epoch 88/150\n",
            "22/22 - 1s - 33ms/step - loss: 5.5814e-04 - val_loss: 8.6569e-04\n",
            "Epoch 89/150\n",
            "22/22 - 2s - 78ms/step - loss: 5.3112e-04 - val_loss: 3.8948e-04\n",
            "Epoch 90/150\n",
            "22/22 - 1s - 55ms/step - loss: 5.7908e-04 - val_loss: 5.2058e-04\n",
            "Epoch 91/150\n",
            "22/22 - 1s - 49ms/step - loss: 5.9419e-04 - val_loss: 5.5051e-04\n",
            "Epoch 92/150\n",
            "22/22 - 1s - 34ms/step - loss: 6.3852e-04 - val_loss: 8.6396e-04\n",
            "Epoch 93/150\n",
            "22/22 - 1s - 57ms/step - loss: 6.0887e-04 - val_loss: 0.0013\n",
            "Epoch 94/150\n",
            "22/22 - 1s - 34ms/step - loss: 5.9432e-04 - val_loss: 6.2883e-04\n",
            "Epoch 95/150\n",
            "22/22 - 1s - 33ms/step - loss: 5.5728e-04 - val_loss: 5.3841e-04\n",
            "Epoch 96/150\n",
            "22/22 - 1s - 34ms/step - loss: 5.4269e-04 - val_loss: 4.5715e-04\n",
            "Epoch 97/150\n",
            "22/22 - 1s - 34ms/step - loss: 5.6245e-04 - val_loss: 4.4147e-04\n",
            "Epoch 98/150\n",
            "22/22 - 1s - 35ms/step - loss: 5.5122e-04 - val_loss: 3.7965e-04\n",
            "Epoch 99/150\n",
            "22/22 - 1s - 57ms/step - loss: 6.8623e-04 - val_loss: 4.8737e-04\n",
            "Epoch 100/150\n",
            "22/22 - 1s - 57ms/step - loss: 5.6930e-04 - val_loss: 4.4898e-04\n",
            "Epoch 101/150\n",
            "22/22 - 1s - 36ms/step - loss: 5.3710e-04 - val_loss: 3.7368e-04\n",
            "Epoch 102/150\n",
            "22/22 - 2s - 71ms/step - loss: 5.6328e-04 - val_loss: 3.9784e-04\n",
            "Epoch 103/150\n",
            "22/22 - 1s - 61ms/step - loss: 6.0466e-04 - val_loss: 7.0125e-04\n",
            "Epoch 104/150\n",
            "22/22 - 1s - 60ms/step - loss: 6.0629e-04 - val_loss: 7.0607e-04\n",
            "Epoch 105/150\n",
            "22/22 - 1s - 40ms/step - loss: 5.3783e-04 - val_loss: 3.8146e-04\n",
            "Epoch 106/150\n",
            "22/22 - 1s - 33ms/step - loss: 5.6672e-04 - val_loss: 3.8240e-04\n",
            "Epoch 107/150\n",
            "22/22 - 1s - 57ms/step - loss: 6.1864e-04 - val_loss: 3.6746e-04\n",
            "Epoch 108/150\n",
            "22/22 - 1s - 33ms/step - loss: 5.9337e-04 - val_loss: 6.2467e-04\n",
            "Epoch 109/150\n",
            "22/22 - 1s - 59ms/step - loss: 5.4978e-04 - val_loss: 0.0012\n",
            "Epoch 110/150\n",
            "22/22 - 1s - 56ms/step - loss: 7.2996e-04 - val_loss: 0.0019\n",
            "Epoch 111/150\n",
            "22/22 - 1s - 57ms/step - loss: 6.5457e-04 - val_loss: 5.9452e-04\n",
            "Epoch 112/150\n",
            "22/22 - 1s - 34ms/step - loss: 5.4078e-04 - val_loss: 4.2196e-04\n",
            "Epoch 113/150\n",
            "22/22 - 1s - 57ms/step - loss: 4.9926e-04 - val_loss: 3.5410e-04\n",
            "Epoch 114/150\n",
            "22/22 - 1s - 68ms/step - loss: 5.3006e-04 - val_loss: 3.7845e-04\n",
            "Epoch 115/150\n",
            "22/22 - 1s - 67ms/step - loss: 5.7555e-04 - val_loss: 5.4004e-04\n",
            "Epoch 116/150\n",
            "22/22 - 1s - 59ms/step - loss: 5.4751e-04 - val_loss: 3.6229e-04\n",
            "Epoch 117/150\n",
            "22/22 - 1s - 40ms/step - loss: 5.1484e-04 - val_loss: 8.5081e-04\n",
            "Epoch 118/150\n",
            "22/22 - 1s - 34ms/step - loss: 5.3329e-04 - val_loss: 3.4538e-04\n",
            "Epoch 119/150\n",
            "22/22 - 1s - 34ms/step - loss: 5.7597e-04 - val_loss: 3.8583e-04\n",
            "Epoch 120/150\n",
            "22/22 - 1s - 34ms/step - loss: 5.8884e-04 - val_loss: 3.7207e-04\n",
            "Epoch 121/150\n",
            "22/22 - 1s - 57ms/step - loss: 5.9792e-04 - val_loss: 3.4566e-04\n",
            "Epoch 122/150\n",
            "22/22 - 1s - 58ms/step - loss: 4.8575e-04 - val_loss: 6.8773e-04\n",
            "Epoch 123/150\n",
            "22/22 - 1s - 57ms/step - loss: 4.8242e-04 - val_loss: 4.4774e-04\n",
            "Epoch 124/150\n",
            "22/22 - 1s - 57ms/step - loss: 4.9987e-04 - val_loss: 4.7747e-04\n",
            "Epoch 125/150\n",
            "22/22 - 1s - 57ms/step - loss: 4.9639e-04 - val_loss: 5.2449e-04\n",
            "Epoch 126/150\n",
            "22/22 - 1s - 34ms/step - loss: 5.4712e-04 - val_loss: 3.4078e-04\n",
            "Epoch 127/150\n",
            "22/22 - 1s - 50ms/step - loss: 4.9869e-04 - val_loss: 6.3960e-04\n",
            "Epoch 128/150\n",
            "22/22 - 1s - 55ms/step - loss: 4.7843e-04 - val_loss: 4.1143e-04\n",
            "Epoch 129/150\n",
            "22/22 - 1s - 61ms/step - loss: 5.5823e-04 - val_loss: 4.9316e-04\n",
            "Epoch 130/150\n",
            "22/22 - 2s - 91ms/step - loss: 4.7413e-04 - val_loss: 3.8836e-04\n",
            "Epoch 131/150\n",
            "22/22 - 1s - 58ms/step - loss: 5.9120e-04 - val_loss: 3.4284e-04\n",
            "Epoch 132/150\n",
            "22/22 - 1s - 57ms/step - loss: 5.4199e-04 - val_loss: 3.3754e-04\n",
            "Epoch 133/150\n",
            "22/22 - 1s - 34ms/step - loss: 4.9477e-04 - val_loss: 6.1786e-04\n",
            "Epoch 134/150\n",
            "22/22 - 1s - 57ms/step - loss: 5.9610e-04 - val_loss: 0.0015\n",
            "Epoch 135/150\n",
            "22/22 - 1s - 34ms/step - loss: 5.3996e-04 - val_loss: 4.5788e-04\n",
            "Epoch 136/150\n",
            "22/22 - 1s - 57ms/step - loss: 5.3698e-04 - val_loss: 6.0555e-04\n",
            "Epoch 137/150\n",
            "22/22 - 1s - 34ms/step - loss: 4.8889e-04 - val_loss: 6.8127e-04\n",
            "Epoch 138/150\n",
            "22/22 - 2s - 69ms/step - loss: 5.1359e-04 - val_loss: 4.6389e-04\n",
            "Epoch 139/150\n",
            "22/22 - 1s - 57ms/step - loss: 4.8637e-04 - val_loss: 3.6719e-04\n",
            "Epoch 140/150\n",
            "22/22 - 1s - 57ms/step - loss: 5.4589e-04 - val_loss: 4.4187e-04\n",
            "Epoch 141/150\n",
            "22/22 - 1s - 44ms/step - loss: 8.1406e-04 - val_loss: 0.0011\n",
            "Epoch 142/150\n",
            "22/22 - 1s - 34ms/step - loss: 7.0060e-04 - val_loss: 5.8001e-04\n",
            "Epoch 143/150\n",
            "22/22 - 1s - 34ms/step - loss: 5.6333e-04 - val_loss: 5.8298e-04\n",
            "Epoch 144/150\n",
            "22/22 - 1s - 57ms/step - loss: 6.4389e-04 - val_loss: 5.4666e-04\n",
            "Epoch 145/150\n",
            "22/22 - 1s - 57ms/step - loss: 5.3721e-04 - val_loss: 6.5627e-04\n",
            "Epoch 146/150\n",
            "22/22 - 1s - 59ms/step - loss: 5.0331e-04 - val_loss: 4.0471e-04\n",
            "Epoch 147/150\n",
            "22/22 - 1s - 60ms/step - loss: 5.3645e-04 - val_loss: 3.2736e-04\n",
            "Epoch 148/150\n",
            "22/22 - 1s - 57ms/step - loss: 4.5948e-04 - val_loss: 4.0370e-04\n",
            "Epoch 149/150\n",
            "22/22 - 1s - 57ms/step - loss: 5.0236e-04 - val_loss: 3.2631e-04\n",
            "Epoch 150/150\n",
            "22/22 - 1s - 50ms/step - loss: 5.1045e-04 - val_loss: 9.2769e-04\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226ms/step\n",
            "Training with Epochs: 150, Batch Size: 32\n",
            "Epoch 1/150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11/11 - 4s - 388ms/step - loss: 0.0345 - val_loss: 0.0234\n",
            "Epoch 2/150\n",
            "11/11 - 0s - 45ms/step - loss: 0.0059 - val_loss: 0.0030\n",
            "Epoch 3/150\n",
            "11/11 - 1s - 58ms/step - loss: 0.0034 - val_loss: 0.0096\n",
            "Epoch 4/150\n",
            "11/11 - 1s - 45ms/step - loss: 0.0025 - val_loss: 0.0035\n",
            "Epoch 5/150\n",
            "11/11 - 1s - 47ms/step - loss: 0.0022 - val_loss: 0.0037\n",
            "Epoch 6/150\n",
            "11/11 - 0s - 45ms/step - loss: 0.0020 - val_loss: 0.0044\n",
            "Epoch 7/150\n",
            "11/11 - 1s - 46ms/step - loss: 0.0019 - val_loss: 0.0041\n",
            "Epoch 8/150\n",
            "11/11 - 1s - 56ms/step - loss: 0.0018 - val_loss: 0.0028\n",
            "Epoch 9/150\n",
            "11/11 - 1s - 56ms/step - loss: 0.0018 - val_loss: 0.0042\n",
            "Epoch 10/150\n",
            "11/11 - 1s - 47ms/step - loss: 0.0019 - val_loss: 0.0035\n",
            "Epoch 11/150\n",
            "11/11 - 0s - 45ms/step - loss: 0.0018 - val_loss: 0.0031\n",
            "Epoch 12/150\n",
            "11/11 - 1s - 59ms/step - loss: 0.0018 - val_loss: 0.0039\n",
            "Epoch 13/150\n",
            "11/11 - 1s - 55ms/step - loss: 0.0016 - val_loss: 0.0036\n",
            "Epoch 14/150\n",
            "11/11 - 1s - 74ms/step - loss: 0.0016 - val_loss: 0.0023\n",
            "Epoch 15/150\n",
            "11/11 - 1s - 124ms/step - loss: 0.0016 - val_loss: 0.0022\n",
            "Epoch 16/150\n",
            "11/11 - 1s - 78ms/step - loss: 0.0016 - val_loss: 0.0027\n",
            "Epoch 17/150\n",
            "11/11 - 1s - 81ms/step - loss: 0.0014 - val_loss: 0.0028\n",
            "Epoch 18/150\n",
            "11/11 - 1s - 52ms/step - loss: 0.0015 - val_loss: 0.0026\n",
            "Epoch 19/150\n",
            "11/11 - 1s - 52ms/step - loss: 0.0014 - val_loss: 0.0034\n",
            "Epoch 20/150\n",
            "11/11 - 1s - 47ms/step - loss: 0.0015 - val_loss: 0.0047\n",
            "Epoch 21/150\n",
            "11/11 - 0s - 45ms/step - loss: 0.0015 - val_loss: 0.0043\n",
            "Epoch 22/150\n",
            "11/11 - 1s - 46ms/step - loss: 0.0014 - val_loss: 0.0023\n",
            "Epoch 23/150\n",
            "11/11 - 1s - 46ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 24/150\n",
            "11/11 - 1s - 48ms/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 25/150\n",
            "11/11 - 1s - 46ms/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 26/150\n",
            "11/11 - 1s - 47ms/step - loss: 0.0012 - val_loss: 0.0018\n",
            "Epoch 27/150\n",
            "11/11 - 1s - 46ms/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 28/150\n",
            "11/11 - 1s - 57ms/step - loss: 0.0013 - val_loss: 0.0024\n",
            "Epoch 29/150\n",
            "11/11 - 1s - 55ms/step - loss: 0.0013 - val_loss: 0.0031\n",
            "Epoch 30/150\n",
            "11/11 - 1s - 58ms/step - loss: 0.0012 - val_loss: 0.0020\n",
            "Epoch 31/150\n",
            "11/11 - 1s - 46ms/step - loss: 0.0011 - val_loss: 0.0018\n",
            "Epoch 32/150\n",
            "11/11 - 1s - 47ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 33/150\n",
            "11/11 - 1s - 46ms/step - loss: 0.0011 - val_loss: 0.0017\n",
            "Epoch 34/150\n",
            "11/11 - 1s - 47ms/step - loss: 0.0011 - val_loss: 0.0016\n",
            "Epoch 35/150\n",
            "11/11 - 1s - 57ms/step - loss: 0.0010 - val_loss: 0.0015\n",
            "Epoch 36/150\n",
            "11/11 - 1s - 59ms/step - loss: 9.9580e-04 - val_loss: 0.0013\n",
            "Epoch 37/150\n",
            "11/11 - 1s - 134ms/step - loss: 9.8985e-04 - val_loss: 0.0013\n",
            "Epoch 38/150\n",
            "11/11 - 1s - 94ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 39/150\n",
            "11/11 - 1s - 104ms/step - loss: 9.5053e-04 - val_loss: 0.0014\n",
            "Epoch 40/150\n",
            "11/11 - 1s - 55ms/step - loss: 9.5060e-04 - val_loss: 0.0013\n",
            "Epoch 41/150\n",
            "11/11 - 1s - 47ms/step - loss: 9.1325e-04 - val_loss: 0.0013\n",
            "Epoch 42/150\n",
            "11/11 - 1s - 46ms/step - loss: 9.2113e-04 - val_loss: 9.9875e-04\n",
            "Epoch 43/150\n",
            "11/11 - 1s - 58ms/step - loss: 9.0883e-04 - val_loss: 8.5266e-04\n",
            "Epoch 44/150\n",
            "11/11 - 1s - 56ms/step - loss: 8.4093e-04 - val_loss: 0.0010\n",
            "Epoch 45/150\n",
            "11/11 - 1s - 50ms/step - loss: 9.2152e-04 - val_loss: 7.8114e-04\n",
            "Epoch 46/150\n",
            "11/11 - 1s - 54ms/step - loss: 8.6376e-04 - val_loss: 9.0323e-04\n",
            "Epoch 47/150\n",
            "11/11 - 1s - 55ms/step - loss: 8.4383e-04 - val_loss: 9.3402e-04\n",
            "Epoch 48/150\n",
            "11/11 - 1s - 48ms/step - loss: 8.6935e-04 - val_loss: 0.0011\n",
            "Epoch 49/150\n",
            "11/11 - 1s - 46ms/step - loss: 9.1480e-04 - val_loss: 0.0011\n",
            "Epoch 50/150\n",
            "11/11 - 1s - 58ms/step - loss: 9.0970e-04 - val_loss: 0.0013\n",
            "Epoch 51/150\n",
            "11/11 - 1s - 54ms/step - loss: 7.7369e-04 - val_loss: 7.7442e-04\n",
            "Epoch 52/150\n",
            "11/11 - 1s - 47ms/step - loss: 8.2502e-04 - val_loss: 7.8969e-04\n",
            "Epoch 53/150\n",
            "11/11 - 1s - 57ms/step - loss: 8.8120e-04 - val_loss: 0.0019\n",
            "Epoch 54/150\n",
            "11/11 - 1s - 67ms/step - loss: 0.0010 - val_loss: 0.0020\n",
            "Epoch 55/150\n",
            "11/11 - 1s - 133ms/step - loss: 8.0152e-04 - val_loss: 7.6407e-04\n",
            "Epoch 56/150\n",
            "11/11 - 1s - 82ms/step - loss: 8.2112e-04 - val_loss: 6.9372e-04\n",
            "Epoch 57/150\n",
            "11/11 - 1s - 65ms/step - loss: 7.8075e-04 - val_loss: 6.6387e-04\n",
            "Epoch 58/150\n",
            "11/11 - 1s - 48ms/step - loss: 8.1633e-04 - val_loss: 7.6354e-04\n",
            "Epoch 59/150\n",
            "11/11 - 1s - 46ms/step - loss: 7.7313e-04 - val_loss: 9.9378e-04\n",
            "Epoch 60/150\n",
            "11/11 - 1s - 57ms/step - loss: 8.4650e-04 - val_loss: 6.4541e-04\n",
            "Epoch 61/150\n",
            "11/11 - 1s - 56ms/step - loss: 8.3729e-04 - val_loss: 0.0013\n",
            "Epoch 62/150\n",
            "11/11 - 0s - 45ms/step - loss: 7.9734e-04 - val_loss: 0.0013\n",
            "Epoch 63/150\n",
            "11/11 - 1s - 60ms/step - loss: 7.3682e-04 - val_loss: 0.0012\n",
            "Epoch 64/150\n",
            "11/11 - 1s - 46ms/step - loss: 7.4600e-04 - val_loss: 9.4167e-04\n",
            "Epoch 65/150\n",
            "11/11 - 1s - 48ms/step - loss: 7.1550e-04 - val_loss: 7.0000e-04\n",
            "Epoch 66/150\n",
            "11/11 - 0s - 45ms/step - loss: 7.2742e-04 - val_loss: 0.0010\n",
            "Epoch 67/150\n",
            "11/11 - 1s - 58ms/step - loss: 7.3813e-04 - val_loss: 9.4734e-04\n",
            "Epoch 68/150\n",
            "11/11 - 1s - 46ms/step - loss: 7.3837e-04 - val_loss: 6.5678e-04\n",
            "Epoch 69/150\n",
            "11/11 - 1s - 57ms/step - loss: 6.8393e-04 - val_loss: 9.8746e-04\n",
            "Epoch 70/150\n",
            "11/11 - 1s - 46ms/step - loss: 7.5777e-04 - val_loss: 6.4755e-04\n",
            "Epoch 71/150\n",
            "11/11 - 1s - 56ms/step - loss: 7.4327e-04 - val_loss: 5.7830e-04\n",
            "Epoch 72/150\n",
            "11/11 - 1s - 58ms/step - loss: 6.5709e-04 - val_loss: 8.7202e-04\n",
            "Epoch 73/150\n",
            "11/11 - 1s - 56ms/step - loss: 7.0821e-04 - val_loss: 0.0010\n",
            "Epoch 74/150\n",
            "11/11 - 1s - 47ms/step - loss: 7.3076e-04 - val_loss: 0.0012\n",
            "Epoch 75/150\n",
            "11/11 - 1s - 72ms/step - loss: 7.7152e-04 - val_loss: 6.7223e-04\n",
            "Epoch 76/150\n",
            "11/11 - 1s - 74ms/step - loss: 6.9042e-04 - val_loss: 5.8298e-04\n",
            "Epoch 77/150\n",
            "11/11 - 1s - 76ms/step - loss: 6.5067e-04 - val_loss: 7.8010e-04\n",
            "Epoch 78/150\n",
            "11/11 - 1s - 92ms/step - loss: 7.1246e-04 - val_loss: 5.9276e-04\n",
            "Epoch 79/150\n",
            "11/11 - 1s - 52ms/step - loss: 6.7436e-04 - val_loss: 7.9084e-04\n",
            "Epoch 80/150\n",
            "11/11 - 1s - 46ms/step - loss: 6.3407e-04 - val_loss: 7.1261e-04\n",
            "Epoch 81/150\n",
            "11/11 - 1s - 57ms/step - loss: 7.0259e-04 - val_loss: 7.0769e-04\n",
            "Epoch 82/150\n",
            "11/11 - 1s - 46ms/step - loss: 6.7176e-04 - val_loss: 6.0691e-04\n",
            "Epoch 83/150\n",
            "11/11 - 1s - 48ms/step - loss: 6.6812e-04 - val_loss: 5.4382e-04\n",
            "Epoch 84/150\n",
            "11/11 - 1s - 47ms/step - loss: 6.3981e-04 - val_loss: 8.0700e-04\n",
            "Epoch 85/150\n",
            "11/11 - 1s - 47ms/step - loss: 6.5215e-04 - val_loss: 7.0078e-04\n",
            "Epoch 86/150\n",
            "11/11 - 1s - 46ms/step - loss: 6.9538e-04 - val_loss: 5.9717e-04\n",
            "Epoch 87/150\n",
            "11/11 - 1s - 56ms/step - loss: 6.8102e-04 - val_loss: 5.2008e-04\n",
            "Epoch 88/150\n",
            "11/11 - 0s - 45ms/step - loss: 7.7303e-04 - val_loss: 8.7350e-04\n",
            "Epoch 89/150\n",
            "11/11 - 1s - 47ms/step - loss: 6.3340e-04 - val_loss: 6.4568e-04\n",
            "Epoch 90/150\n",
            "11/11 - 1s - 46ms/step - loss: 6.0409e-04 - val_loss: 6.3862e-04\n",
            "Epoch 91/150\n",
            "11/11 - 1s - 48ms/step - loss: 7.5094e-04 - val_loss: 5.2409e-04\n",
            "Epoch 92/150\n",
            "11/11 - 1s - 53ms/step - loss: 6.2278e-04 - val_loss: 9.9963e-04\n",
            "Epoch 93/150\n",
            "11/11 - 1s - 47ms/step - loss: 6.7888e-04 - val_loss: 5.3595e-04\n",
            "Epoch 94/150\n",
            "11/11 - 1s - 48ms/step - loss: 6.5667e-04 - val_loss: 5.5465e-04\n",
            "Epoch 95/150\n",
            "11/11 - 1s - 56ms/step - loss: 6.2780e-04 - val_loss: 0.0013\n",
            "Epoch 96/150\n",
            "11/11 - 1s - 73ms/step - loss: 6.9481e-04 - val_loss: 8.2291e-04\n",
            "Epoch 97/150\n",
            "11/11 - 1s - 126ms/step - loss: 6.1168e-04 - val_loss: 5.3373e-04\n",
            "Epoch 98/150\n",
            "11/11 - 1s - 95ms/step - loss: 5.8239e-04 - val_loss: 5.5257e-04\n",
            "Epoch 99/150\n",
            "11/11 - 1s - 49ms/step - loss: 6.3921e-04 - val_loss: 7.0438e-04\n",
            "Epoch 100/150\n",
            "11/11 - 1s - 46ms/step - loss: 5.9611e-04 - val_loss: 8.3340e-04\n",
            "Epoch 101/150\n",
            "11/11 - 1s - 48ms/step - loss: 6.2589e-04 - val_loss: 6.7375e-04\n",
            "Epoch 102/150\n",
            "11/11 - 1s - 54ms/step - loss: 5.9016e-04 - val_loss: 4.7453e-04\n",
            "Epoch 103/150\n",
            "11/11 - 1s - 58ms/step - loss: 6.1542e-04 - val_loss: 5.0849e-04\n",
            "Epoch 104/150\n",
            "11/11 - 1s - 54ms/step - loss: 6.2518e-04 - val_loss: 0.0011\n",
            "Epoch 105/150\n",
            "11/11 - 1s - 47ms/step - loss: 6.7782e-04 - val_loss: 5.6237e-04\n",
            "Epoch 106/150\n",
            "11/11 - 1s - 57ms/step - loss: 6.0961e-04 - val_loss: 4.8018e-04\n",
            "Epoch 107/150\n",
            "11/11 - 0s - 45ms/step - loss: 7.0628e-04 - val_loss: 4.5879e-04\n",
            "Epoch 108/150\n",
            "11/11 - 1s - 58ms/step - loss: 6.2395e-04 - val_loss: 8.9350e-04\n",
            "Epoch 109/150\n",
            "11/11 - 1s - 46ms/step - loss: 6.5870e-04 - val_loss: 4.5532e-04\n",
            "Epoch 110/150\n",
            "11/11 - 1s - 58ms/step - loss: 5.7884e-04 - val_loss: 7.4567e-04\n",
            "Epoch 111/150\n",
            "11/11 - 1s - 54ms/step - loss: 6.1940e-04 - val_loss: 4.5414e-04\n",
            "Epoch 112/150\n",
            "11/11 - 1s - 57ms/step - loss: 6.2521e-04 - val_loss: 5.0583e-04\n",
            "Epoch 113/150\n",
            "11/11 - 1s - 57ms/step - loss: 5.6957e-04 - val_loss: 4.9695e-04\n",
            "Epoch 114/150\n",
            "11/11 - 0s - 45ms/step - loss: 5.9616e-04 - val_loss: 4.4131e-04\n",
            "Epoch 115/150\n",
            "11/11 - 1s - 55ms/step - loss: 6.3145e-04 - val_loss: 4.5726e-04\n",
            "Epoch 116/150\n",
            "11/11 - 1s - 75ms/step - loss: 5.5645e-04 - val_loss: 8.1063e-04\n",
            "Epoch 117/150\n",
            "11/11 - 1s - 116ms/step - loss: 5.5779e-04 - val_loss: 7.0831e-04\n",
            "Epoch 118/150\n",
            "11/11 - 1s - 87ms/step - loss: 5.3646e-04 - val_loss: 4.3351e-04\n",
            "Epoch 119/150\n",
            "11/11 - 1s - 55ms/step - loss: 5.9000e-04 - val_loss: 6.5797e-04\n",
            "Epoch 120/150\n",
            "11/11 - 1s - 58ms/step - loss: 5.9485e-04 - val_loss: 4.8281e-04\n",
            "Epoch 121/150\n",
            "11/11 - 1s - 46ms/step - loss: 5.7550e-04 - val_loss: 7.9193e-04\n",
            "Epoch 122/150\n",
            "11/11 - 1s - 46ms/step - loss: 5.7929e-04 - val_loss: 4.4068e-04\n",
            "Epoch 123/150\n",
            "11/11 - 1s - 46ms/step - loss: 5.9844e-04 - val_loss: 5.6960e-04\n",
            "Epoch 124/150\n",
            "11/11 - 1s - 46ms/step - loss: 5.7400e-04 - val_loss: 4.1906e-04\n",
            "Epoch 125/150\n",
            "11/11 - 1s - 46ms/step - loss: 5.6478e-04 - val_loss: 5.2847e-04\n",
            "Epoch 126/150\n",
            "11/11 - 1s - 46ms/step - loss: 5.9789e-04 - val_loss: 4.2559e-04\n",
            "Epoch 127/150\n",
            "11/11 - 1s - 46ms/step - loss: 6.3990e-04 - val_loss: 7.4963e-04\n",
            "Epoch 128/150\n",
            "11/11 - 1s - 47ms/step - loss: 5.8775e-04 - val_loss: 7.4226e-04\n",
            "Epoch 129/150\n",
            "11/11 - 1s - 55ms/step - loss: 5.6132e-04 - val_loss: 8.3411e-04\n",
            "Epoch 130/150\n",
            "11/11 - 1s - 46ms/step - loss: 5.7328e-04 - val_loss: 4.2989e-04\n",
            "Epoch 131/150\n",
            "11/11 - 1s - 56ms/step - loss: 5.6224e-04 - val_loss: 4.2300e-04\n",
            "Epoch 132/150\n",
            "11/11 - 1s - 46ms/step - loss: 5.6692e-04 - val_loss: 5.0231e-04\n",
            "Epoch 133/150\n",
            "11/11 - 1s - 57ms/step - loss: 5.7515e-04 - val_loss: 9.2298e-04\n",
            "Epoch 134/150\n",
            "11/11 - 1s - 46ms/step - loss: 6.1222e-04 - val_loss: 4.2044e-04\n",
            "Epoch 135/150\n",
            "11/11 - 1s - 46ms/step - loss: 6.2297e-04 - val_loss: 4.9273e-04\n",
            "Epoch 136/150\n",
            "11/11 - 1s - 80ms/step - loss: 5.9949e-04 - val_loss: 0.0012\n",
            "Epoch 137/150\n",
            "11/11 - 1s - 76ms/step - loss: 6.3170e-04 - val_loss: 4.9294e-04\n",
            "Epoch 138/150\n",
            "11/11 - 1s - 78ms/step - loss: 5.1654e-04 - val_loss: 4.8684e-04\n",
            "Epoch 139/150\n",
            "11/11 - 1s - 92ms/step - loss: 5.6993e-04 - val_loss: 8.9535e-04\n",
            "Epoch 140/150\n",
            "11/11 - 1s - 52ms/step - loss: 5.9649e-04 - val_loss: 4.0591e-04\n",
            "Epoch 141/150\n",
            "11/11 - 0s - 45ms/step - loss: 5.9029e-04 - val_loss: 5.3174e-04\n",
            "Epoch 142/150\n",
            "11/11 - 1s - 59ms/step - loss: 5.7472e-04 - val_loss: 5.3951e-04\n",
            "Epoch 143/150\n",
            "11/11 - 1s - 55ms/step - loss: 5.5624e-04 - val_loss: 0.0011\n",
            "Epoch 144/150\n",
            "11/11 - 1s - 47ms/step - loss: 5.7654e-04 - val_loss: 4.3019e-04\n",
            "Epoch 145/150\n",
            "11/11 - 1s - 46ms/step - loss: 5.4962e-04 - val_loss: 4.1847e-04\n",
            "Epoch 146/150\n",
            "11/11 - 1s - 59ms/step - loss: 5.1500e-04 - val_loss: 3.9329e-04\n",
            "Epoch 147/150\n",
            "11/11 - 0s - 45ms/step - loss: 5.3166e-04 - val_loss: 4.8820e-04\n",
            "Epoch 148/150\n",
            "11/11 - 1s - 57ms/step - loss: 5.3887e-04 - val_loss: 5.3990e-04\n",
            "Epoch 149/150\n",
            "11/11 - 1s - 47ms/step - loss: 5.4306e-04 - val_loss: 0.0011\n",
            "Epoch 150/150\n",
            "11/11 - 1s - 46ms/step - loss: 6.6733e-04 - val_loss: 4.0675e-04\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 584ms/step\n",
            "Training with Epochs: 150, Batch Size: 64\n",
            "Epoch 1/150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 - 5s - 849ms/step - loss: 0.0437 - val_loss: 0.0094\n",
            "Epoch 2/150\n",
            "6/6 - 0s - 83ms/step - loss: 0.0067 - val_loss: 0.0295\n",
            "Epoch 3/150\n",
            "6/6 - 1s - 107ms/step - loss: 0.0062 - val_loss: 0.0034\n",
            "Epoch 4/150\n",
            "6/6 - 1s - 98ms/step - loss: 0.0044 - val_loss: 0.0042\n",
            "Epoch 5/150\n",
            "6/6 - 1s - 86ms/step - loss: 0.0030 - val_loss: 0.0108\n",
            "Epoch 6/150\n",
            "6/6 - 0s - 81ms/step - loss: 0.0029 - val_loss: 0.0036\n",
            "Epoch 7/150\n",
            "6/6 - 1s - 84ms/step - loss: 0.0026 - val_loss: 0.0043\n",
            "Epoch 8/150\n",
            "6/6 - 1s - 87ms/step - loss: 0.0022 - val_loss: 0.0054\n",
            "Epoch 9/150\n",
            "6/6 - 1s - 103ms/step - loss: 0.0021 - val_loss: 0.0033\n",
            "Epoch 10/150\n",
            "6/6 - 1s - 102ms/step - loss: 0.0021 - val_loss: 0.0042\n",
            "Epoch 11/150\n",
            "6/6 - 1s - 101ms/step - loss: 0.0021 - val_loss: 0.0035\n",
            "Epoch 12/150\n",
            "6/6 - 1s - 106ms/step - loss: 0.0020 - val_loss: 0.0032\n",
            "Epoch 13/150\n",
            "6/6 - 1s - 103ms/step - loss: 0.0019 - val_loss: 0.0035\n",
            "Epoch 14/150\n",
            "6/6 - 1s - 87ms/step - loss: 0.0019 - val_loss: 0.0031\n",
            "Epoch 15/150\n",
            "6/6 - 1s - 84ms/step - loss: 0.0019 - val_loss: 0.0035\n",
            "Epoch 16/150\n",
            "6/6 - 1s - 87ms/step - loss: 0.0019 - val_loss: 0.0034\n",
            "Epoch 17/150\n",
            "6/6 - 1s - 83ms/step - loss: 0.0018 - val_loss: 0.0031\n",
            "Epoch 18/150\n",
            "6/6 - 1s - 138ms/step - loss: 0.0018 - val_loss: 0.0035\n",
            "Epoch 19/150\n",
            "6/6 - 1s - 249ms/step - loss: 0.0018 - val_loss: 0.0033\n",
            "Epoch 20/150\n",
            "6/6 - 1s - 166ms/step - loss: 0.0018 - val_loss: 0.0029\n",
            "Epoch 21/150\n",
            "6/6 - 1s - 142ms/step - loss: 0.0018 - val_loss: 0.0031\n",
            "Epoch 22/150\n",
            "6/6 - 1s - 153ms/step - loss: 0.0017 - val_loss: 0.0030\n",
            "Epoch 23/150\n",
            "6/6 - 1s - 84ms/step - loss: 0.0017 - val_loss: 0.0029\n",
            "Epoch 24/150\n",
            "6/6 - 1s - 83ms/step - loss: 0.0016 - val_loss: 0.0031\n",
            "Epoch 25/150\n",
            "6/6 - 1s - 103ms/step - loss: 0.0016 - val_loss: 0.0025\n",
            "Epoch 26/150\n",
            "6/6 - 1s - 106ms/step - loss: 0.0017 - val_loss: 0.0027\n",
            "Epoch 27/150\n",
            "6/6 - 1s - 110ms/step - loss: 0.0017 - val_loss: 0.0037\n",
            "Epoch 28/150\n",
            "6/6 - 1s - 84ms/step - loss: 0.0016 - val_loss: 0.0020\n",
            "Epoch 29/150\n",
            "6/6 - 1s - 103ms/step - loss: 0.0017 - val_loss: 0.0044\n",
            "Epoch 30/150\n",
            "6/6 - 1s - 88ms/step - loss: 0.0018 - val_loss: 0.0021\n",
            "Epoch 31/150\n",
            "6/6 - 1s - 87ms/step - loss: 0.0016 - val_loss: 0.0025\n",
            "Epoch 32/150\n",
            "6/6 - 1s - 86ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "Epoch 33/150\n",
            "6/6 - 1s - 102ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "Epoch 34/150\n",
            "6/6 - 1s - 104ms/step - loss: 0.0014 - val_loss: 0.0019\n",
            "Epoch 35/150\n",
            "6/6 - 1s - 85ms/step - loss: 0.0015 - val_loss: 0.0030\n",
            "Epoch 36/150\n",
            "6/6 - 1s - 91ms/step - loss: 0.0014 - val_loss: 0.0018\n",
            "Epoch 37/150\n",
            "6/6 - 1s - 99ms/step - loss: 0.0015 - val_loss: 0.0034\n",
            "Epoch 38/150\n",
            "6/6 - 1s - 110ms/step - loss: 0.0014 - val_loss: 0.0017\n",
            "Epoch 39/150\n",
            "6/6 - 2s - 255ms/step - loss: 0.0014 - val_loss: 0.0023\n",
            "Epoch 40/150\n",
            "6/6 - 1s - 188ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "Epoch 41/150\n",
            "6/6 - 1s - 164ms/step - loss: 0.0013 - val_loss: 0.0018\n",
            "Epoch 42/150\n",
            "6/6 - 1s - 98ms/step - loss: 0.0014 - val_loss: 0.0033\n",
            "Epoch 43/150\n",
            "6/6 - 1s - 84ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 44/150\n",
            "6/6 - 1s - 86ms/step - loss: 0.0013 - val_loss: 0.0022\n",
            "Epoch 45/150\n",
            "6/6 - 1s - 104ms/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 46/150\n",
            "6/6 - 1s - 88ms/step - loss: 0.0012 - val_loss: 0.0017\n",
            "Epoch 47/150\n",
            "6/6 - 1s - 85ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "Epoch 48/150\n",
            "6/6 - 1s - 101ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 49/150\n",
            "6/6 - 1s - 101ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 50/150\n",
            "6/6 - 1s - 84ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "Epoch 51/150\n",
            "6/6 - 1s - 84ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 52/150\n",
            "6/6 - 1s - 105ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 53/150\n",
            "6/6 - 1s - 105ms/step - loss: 0.0011 - val_loss: 0.0019\n",
            "Epoch 54/150\n",
            "6/6 - 1s - 107ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 55/150\n",
            "6/6 - 1s - 100ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 56/150\n",
            "6/6 - 1s - 103ms/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 57/150\n",
            "6/6 - 1s - 132ms/step - loss: 0.0011 - val_loss: 0.0017\n",
            "Epoch 58/150\n",
            "6/6 - 1s - 171ms/step - loss: 0.0011 - val_loss: 0.0017\n",
            "Epoch 59/150\n",
            "6/6 - 1s - 207ms/step - loss: 0.0011 - val_loss: 9.7030e-04\n",
            "Epoch 60/150\n",
            "6/6 - 1s - 170ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 61/150\n",
            "6/6 - 1s - 131ms/step - loss: 0.0010 - val_loss: 0.0010\n",
            "Epoch 62/150\n",
            "6/6 - 1s - 100ms/step - loss: 0.0010 - val_loss: 0.0017\n",
            "Epoch 63/150\n",
            "6/6 - 1s - 88ms/step - loss: 0.0011 - val_loss: 9.9129e-04\n",
            "Epoch 64/150\n",
            "6/6 - 1s - 98ms/step - loss: 9.7223e-04 - val_loss: 0.0012\n",
            "Epoch 65/150\n",
            "6/6 - 1s - 87ms/step - loss: 9.8707e-04 - val_loss: 0.0014\n",
            "Epoch 66/150\n",
            "6/6 - 1s - 103ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 67/150\n",
            "6/6 - 1s - 86ms/step - loss: 9.8397e-04 - val_loss: 9.5510e-04\n",
            "Epoch 68/150\n",
            "6/6 - 1s - 99ms/step - loss: 9.7412e-04 - val_loss: 0.0010\n",
            "Epoch 69/150\n",
            "6/6 - 1s - 87ms/step - loss: 9.1465e-04 - val_loss: 0.0015\n",
            "Epoch 70/150\n",
            "6/6 - 0s - 80ms/step - loss: 9.0866e-04 - val_loss: 8.1039e-04\n",
            "Epoch 71/150\n",
            "6/6 - 1s - 88ms/step - loss: 9.5849e-04 - val_loss: 8.9337e-04\n",
            "Epoch 72/150\n",
            "6/6 - 1s - 101ms/step - loss: 9.0885e-04 - val_loss: 0.0011\n",
            "Epoch 73/150\n",
            "6/6 - 1s - 88ms/step - loss: 8.7111e-04 - val_loss: 0.0016\n",
            "Epoch 74/150\n",
            "6/6 - 1s - 87ms/step - loss: 0.0010 - val_loss: 9.1626e-04\n",
            "Epoch 75/150\n",
            "6/6 - 1s - 87ms/step - loss: 9.4577e-04 - val_loss: 0.0011\n",
            "Epoch 76/150\n",
            "6/6 - 1s - 87ms/step - loss: 8.8194e-04 - val_loss: 9.1198e-04\n",
            "Epoch 77/150\n",
            "6/6 - 1s - 98ms/step - loss: 8.1904e-04 - val_loss: 8.9216e-04\n",
            "Epoch 78/150\n",
            "6/6 - 1s - 99ms/step - loss: 8.4801e-04 - val_loss: 0.0010\n",
            "Epoch 79/150\n",
            "6/6 - 1s - 168ms/step - loss: 8.8169e-04 - val_loss: 9.8545e-04\n",
            "Epoch 80/150\n",
            "6/6 - 1s - 200ms/step - loss: 8.7014e-04 - val_loss: 8.9464e-04\n",
            "Epoch 81/150\n",
            "6/6 - 1s - 157ms/step - loss: 9.3802e-04 - val_loss: 7.4276e-04\n",
            "Epoch 82/150\n",
            "6/6 - 1s - 89ms/step - loss: 8.8250e-04 - val_loss: 0.0011\n",
            "Epoch 83/150\n",
            "6/6 - 1s - 86ms/step - loss: 8.6055e-04 - val_loss: 0.0013\n",
            "Epoch 84/150\n",
            "6/6 - 1s - 101ms/step - loss: 8.1919e-04 - val_loss: 7.3655e-04\n",
            "Epoch 85/150\n",
            "6/6 - 1s - 104ms/step - loss: 7.8911e-04 - val_loss: 7.7471e-04\n",
            "Epoch 86/150\n",
            "6/6 - 1s - 91ms/step - loss: 8.8545e-04 - val_loss: 8.0698e-04\n",
            "Epoch 87/150\n",
            "6/6 - 1s - 84ms/step - loss: 8.8815e-04 - val_loss: 0.0013\n",
            "Epoch 88/150\n",
            "6/6 - 1s - 106ms/step - loss: 8.1082e-04 - val_loss: 6.8458e-04\n",
            "Epoch 89/150\n",
            "6/6 - 1s - 100ms/step - loss: 7.5264e-04 - val_loss: 0.0015\n",
            "Epoch 90/150\n",
            "6/6 - 1s - 107ms/step - loss: 8.3628e-04 - val_loss: 7.6486e-04\n",
            "Epoch 91/150\n",
            "6/6 - 1s - 84ms/step - loss: 7.2667e-04 - val_loss: 7.7148e-04\n",
            "Epoch 92/150\n",
            "6/6 - 1s - 105ms/step - loss: 8.0462e-04 - val_loss: 0.0012\n",
            "Epoch 93/150\n",
            "6/6 - 1s - 84ms/step - loss: 8.0580e-04 - val_loss: 6.8617e-04\n",
            "Epoch 94/150\n",
            "6/6 - 1s - 101ms/step - loss: 8.5201e-04 - val_loss: 8.4278e-04\n",
            "Epoch 95/150\n",
            "6/6 - 1s - 107ms/step - loss: 7.7716e-04 - val_loss: 7.3344e-04\n",
            "Epoch 96/150\n",
            "6/6 - 1s - 87ms/step - loss: 7.6446e-04 - val_loss: 7.3205e-04\n",
            "Epoch 97/150\n",
            "6/6 - 1s - 87ms/step - loss: 7.5654e-04 - val_loss: 8.3419e-04\n",
            "Epoch 98/150\n",
            "6/6 - 1s - 132ms/step - loss: 8.0828e-04 - val_loss: 6.3496e-04\n",
            "Epoch 99/150\n",
            "6/6 - 2s - 250ms/step - loss: 9.0997e-04 - val_loss: 0.0012\n",
            "Epoch 100/150\n",
            "6/6 - 1s - 170ms/step - loss: 7.8539e-04 - val_loss: 0.0017\n",
            "Epoch 101/150\n",
            "6/6 - 1s - 138ms/step - loss: 8.0186e-04 - val_loss: 6.4468e-04\n",
            "Epoch 102/150\n",
            "6/6 - 1s - 85ms/step - loss: 7.8967e-04 - val_loss: 9.6651e-04\n",
            "Epoch 103/150\n",
            "6/6 - 1s - 104ms/step - loss: 7.4500e-04 - val_loss: 6.2300e-04\n",
            "Epoch 104/150\n",
            "6/6 - 1s - 84ms/step - loss: 8.0625e-04 - val_loss: 7.6214e-04\n",
            "Epoch 105/150\n",
            "6/6 - 1s - 87ms/step - loss: 7.7516e-04 - val_loss: 8.0738e-04\n",
            "Epoch 106/150\n",
            "6/6 - 0s - 81ms/step - loss: 7.8353e-04 - val_loss: 7.7237e-04\n",
            "Epoch 107/150\n",
            "6/6 - 0s - 83ms/step - loss: 7.4809e-04 - val_loss: 7.8405e-04\n",
            "Epoch 108/150\n",
            "6/6 - 1s - 106ms/step - loss: 7.0267e-04 - val_loss: 0.0011\n",
            "Epoch 109/150\n",
            "6/6 - 1s - 85ms/step - loss: 7.7679e-04 - val_loss: 0.0011\n",
            "Epoch 110/150\n",
            "6/6 - 1s - 105ms/step - loss: 7.6556e-04 - val_loss: 6.3085e-04\n",
            "Epoch 111/150\n",
            "6/6 - 1s - 88ms/step - loss: 6.9999e-04 - val_loss: 6.9707e-04\n",
            "Epoch 112/150\n",
            "6/6 - 1s - 85ms/step - loss: 7.7966e-04 - val_loss: 6.0133e-04\n",
            "Epoch 113/150\n",
            "6/6 - 1s - 85ms/step - loss: 7.7412e-04 - val_loss: 7.5093e-04\n",
            "Epoch 114/150\n",
            "6/6 - 0s - 83ms/step - loss: 7.3045e-04 - val_loss: 0.0012\n",
            "Epoch 115/150\n",
            "6/6 - 1s - 104ms/step - loss: 7.1390e-04 - val_loss: 0.0013\n",
            "Epoch 116/150\n",
            "6/6 - 1s - 85ms/step - loss: 7.4401e-04 - val_loss: 5.7450e-04\n",
            "Epoch 117/150\n",
            "6/6 - 1s - 86ms/step - loss: 7.3260e-04 - val_loss: 5.6186e-04\n",
            "Epoch 118/150\n",
            "6/6 - 1s - 107ms/step - loss: 7.5584e-04 - val_loss: 9.6016e-04\n",
            "Epoch 119/150\n",
            "6/6 - 1s - 91ms/step - loss: 8.1743e-04 - val_loss: 0.0012\n",
            "Epoch 120/150\n",
            "6/6 - 1s - 161ms/step - loss: 7.6712e-04 - val_loss: 9.1245e-04\n",
            "Epoch 121/150\n",
            "6/6 - 1s - 163ms/step - loss: 6.8493e-04 - val_loss: 0.0011\n",
            "Epoch 122/150\n",
            "6/6 - 1s - 159ms/step - loss: 8.3037e-04 - val_loss: 6.7133e-04\n",
            "Epoch 123/150\n",
            "6/6 - 1s - 175ms/step - loss: 6.6736e-04 - val_loss: 6.4121e-04\n",
            "Epoch 124/150\n",
            "6/6 - 1s - 104ms/step - loss: 8.7608e-04 - val_loss: 0.0012\n",
            "Epoch 125/150\n",
            "6/6 - 1s - 84ms/step - loss: 7.8600e-04 - val_loss: 0.0013\n",
            "Epoch 126/150\n",
            "6/6 - 1s - 106ms/step - loss: 7.1507e-04 - val_loss: 5.7638e-04\n",
            "Epoch 127/150\n",
            "6/6 - 1s - 100ms/step - loss: 7.0000e-04 - val_loss: 6.5268e-04\n",
            "Epoch 128/150\n",
            "6/6 - 1s - 87ms/step - loss: 6.6758e-04 - val_loss: 7.0608e-04\n",
            "Epoch 129/150\n",
            "6/6 - 1s - 99ms/step - loss: 7.1502e-04 - val_loss: 5.5941e-04\n",
            "Epoch 130/150\n",
            "6/6 - 1s - 101ms/step - loss: 6.9468e-04 - val_loss: 8.3268e-04\n",
            "Epoch 131/150\n",
            "6/6 - 1s - 90ms/step - loss: 7.0021e-04 - val_loss: 9.0516e-04\n",
            "Epoch 132/150\n",
            "6/6 - 0s - 83ms/step - loss: 6.9649e-04 - val_loss: 5.8350e-04\n",
            "Epoch 133/150\n",
            "6/6 - 1s - 87ms/step - loss: 6.6563e-04 - val_loss: 5.7249e-04\n",
            "Epoch 134/150\n",
            "6/6 - 0s - 83ms/step - loss: 6.8906e-04 - val_loss: 7.0846e-04\n",
            "Epoch 135/150\n",
            "6/6 - 1s - 107ms/step - loss: 6.1174e-04 - val_loss: 9.2610e-04\n",
            "Epoch 136/150\n",
            "6/6 - 0s - 81ms/step - loss: 6.7211e-04 - val_loss: 5.8959e-04\n",
            "Epoch 137/150\n",
            "6/6 - 1s - 85ms/step - loss: 7.0166e-04 - val_loss: 5.4063e-04\n",
            "Epoch 138/150\n",
            "6/6 - 1s - 85ms/step - loss: 6.9344e-04 - val_loss: 9.3892e-04\n",
            "Epoch 139/150\n",
            "6/6 - 1s - 104ms/step - loss: 6.6790e-04 - val_loss: 7.1804e-04\n",
            "Epoch 140/150\n",
            "6/6 - 1s - 88ms/step - loss: 6.8325e-04 - val_loss: 5.4896e-04\n",
            "Epoch 141/150\n",
            "6/6 - 1s - 88ms/step - loss: 6.3409e-04 - val_loss: 6.7958e-04\n",
            "Epoch 142/150\n",
            "6/6 - 1s - 135ms/step - loss: 6.4467e-04 - val_loss: 5.0328e-04\n",
            "Epoch 143/150\n",
            "6/6 - 1s - 229ms/step - loss: 7.0621e-04 - val_loss: 5.8074e-04\n",
            "Epoch 144/150\n",
            "6/6 - 1s - 174ms/step - loss: 6.9751e-04 - val_loss: 0.0012\n",
            "Epoch 145/150\n",
            "6/6 - 0s - 80ms/step - loss: 6.9100e-04 - val_loss: 5.6553e-04\n",
            "Epoch 146/150\n",
            "6/6 - 1s - 84ms/step - loss: 6.6898e-04 - val_loss: 5.5124e-04\n",
            "Epoch 147/150\n",
            "6/6 - 0s - 81ms/step - loss: 6.2862e-04 - val_loss: 4.9328e-04\n",
            "Epoch 148/150\n",
            "6/6 - 1s - 109ms/step - loss: 6.4055e-04 - val_loss: 8.0199e-04\n",
            "Epoch 149/150\n",
            "6/6 - 1s - 85ms/step - loss: 6.2962e-04 - val_loss: 7.0073e-04\n",
            "Epoch 150/150\n",
            "6/6 - 1s - 104ms/step - loss: 5.9791e-04 - val_loss: 4.8951e-04\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 149ms/step\n",
            "Training with Epochs: 150, Batch Size: 128\n",
            "Epoch 1/150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 - 4s - 1s/step - loss: 0.0792 - val_loss: 0.0165\n",
            "Epoch 2/150\n",
            "3/3 - 0s - 130ms/step - loss: 0.0132 - val_loss: 0.0153\n",
            "Epoch 3/150\n",
            "3/3 - 1s - 214ms/step - loss: 0.0096 - val_loss: 0.0128\n",
            "Epoch 4/150\n",
            "3/3 - 1s - 201ms/step - loss: 0.0061 - val_loss: 0.0298\n",
            "Epoch 5/150\n",
            "3/3 - 1s - 294ms/step - loss: 0.0094 - val_loss: 0.0214\n",
            "Epoch 6/150\n",
            "3/3 - 1s - 429ms/step - loss: 0.0052 - val_loss: 0.0067\n",
            "Epoch 7/150\n",
            "3/3 - 1s - 321ms/step - loss: 0.0031 - val_loss: 0.0029\n",
            "Epoch 8/150\n",
            "3/3 - 0s - 135ms/step - loss: 0.0049 - val_loss: 0.0029\n",
            "Epoch 9/150\n",
            "3/3 - 1s - 209ms/step - loss: 0.0034 - val_loss: 0.0055\n",
            "Epoch 10/150\n",
            "3/3 - 0s - 133ms/step - loss: 0.0026 - val_loss: 0.0098\n",
            "Epoch 11/150\n",
            "3/3 - 1s - 203ms/step - loss: 0.0032 - val_loss: 0.0095\n",
            "Epoch 12/150\n",
            "3/3 - 1s - 213ms/step - loss: 0.0029 - val_loss: 0.0059\n",
            "Epoch 13/150\n",
            "3/3 - 0s - 130ms/step - loss: 0.0022 - val_loss: 0.0035\n",
            "Epoch 14/150\n",
            "3/3 - 1s - 211ms/step - loss: 0.0024 - val_loss: 0.0031\n",
            "Epoch 15/150\n",
            "3/3 - 1s - 206ms/step - loss: 0.0023 - val_loss: 0.0039\n",
            "Epoch 16/150\n",
            "3/3 - 0s - 136ms/step - loss: 0.0021 - val_loss: 0.0051\n",
            "Epoch 17/150\n",
            "3/3 - 1s - 203ms/step - loss: 0.0022 - val_loss: 0.0050\n",
            "Epoch 18/150\n",
            "3/3 - 0s - 135ms/step - loss: 0.0021 - val_loss: 0.0041\n",
            "Epoch 19/150\n",
            "3/3 - 1s - 205ms/step - loss: 0.0020 - val_loss: 0.0033\n",
            "Epoch 20/150\n",
            "3/3 - 1s - 205ms/step - loss: 0.0019 - val_loss: 0.0031\n",
            "Epoch 21/150\n",
            "3/3 - 0s - 132ms/step - loss: 0.0020 - val_loss: 0.0034\n",
            "Epoch 22/150\n",
            "3/3 - 0s - 137ms/step - loss: 0.0020 - val_loss: 0.0037\n",
            "Epoch 23/150\n",
            "3/3 - 1s - 204ms/step - loss: 0.0020 - val_loss: 0.0036\n",
            "Epoch 24/150\n",
            "3/3 - 0s - 132ms/step - loss: 0.0019 - val_loss: 0.0031\n",
            "Epoch 25/150\n",
            "3/3 - 1s - 205ms/step - loss: 0.0018 - val_loss: 0.0029\n",
            "Epoch 26/150\n",
            "3/3 - 1s - 305ms/step - loss: 0.0019 - val_loss: 0.0031\n",
            "Epoch 27/150\n",
            "3/3 - 1s - 430ms/step - loss: 0.0019 - val_loss: 0.0032\n",
            "Epoch 28/150\n",
            "3/3 - 1s - 312ms/step - loss: 0.0018 - val_loss: 0.0032\n",
            "Epoch 29/150\n",
            "3/3 - 0s - 133ms/step - loss: 0.0019 - val_loss: 0.0029\n",
            "Epoch 30/150\n",
            "3/3 - 1s - 200ms/step - loss: 0.0019 - val_loss: 0.0030\n",
            "Epoch 31/150\n",
            "3/3 - 1s - 209ms/step - loss: 0.0018 - val_loss: 0.0032\n",
            "Epoch 32/150\n",
            "3/3 - 1s - 212ms/step - loss: 0.0018 - val_loss: 0.0031\n",
            "Epoch 33/150\n",
            "3/3 - 1s - 203ms/step - loss: 0.0018 - val_loss: 0.0029\n",
            "Epoch 34/150\n",
            "3/3 - 0s - 136ms/step - loss: 0.0019 - val_loss: 0.0028\n",
            "Epoch 35/150\n",
            "3/3 - 1s - 208ms/step - loss: 0.0017 - val_loss: 0.0032\n",
            "Epoch 36/150\n",
            "3/3 - 1s - 206ms/step - loss: 0.0017 - val_loss: 0.0032\n",
            "Epoch 37/150\n",
            "3/3 - 0s - 133ms/step - loss: 0.0017 - val_loss: 0.0031\n",
            "Epoch 38/150\n",
            "3/3 - 0s - 132ms/step - loss: 0.0017 - val_loss: 0.0029\n",
            "Epoch 39/150\n",
            "3/3 - 0s - 128ms/step - loss: 0.0017 - val_loss: 0.0029\n",
            "Epoch 40/150\n",
            "3/3 - 0s - 130ms/step - loss: 0.0017 - val_loss: 0.0031\n",
            "Epoch 41/150\n",
            "3/3 - 0s - 131ms/step - loss: 0.0017 - val_loss: 0.0031\n",
            "Epoch 42/150\n",
            "3/3 - 1s - 205ms/step - loss: 0.0017 - val_loss: 0.0028\n",
            "Epoch 43/150\n",
            "3/3 - 0s - 133ms/step - loss: 0.0017 - val_loss: 0.0027\n",
            "Epoch 44/150\n",
            "3/3 - 0s - 133ms/step - loss: 0.0017 - val_loss: 0.0030\n",
            "Epoch 45/150\n",
            "3/3 - 0s - 127ms/step - loss: 0.0017 - val_loss: 0.0032\n",
            "Epoch 46/150\n",
            "3/3 - 0s - 133ms/step - loss: 0.0016 - val_loss: 0.0028\n",
            "Epoch 47/150\n",
            "3/3 - 0s - 129ms/step - loss: 0.0016 - val_loss: 0.0026\n",
            "Epoch 48/150\n",
            "3/3 - 1s - 285ms/step - loss: 0.0016 - val_loss: 0.0028\n",
            "Epoch 49/150\n",
            "3/3 - 1s - 440ms/step - loss: 0.0017 - val_loss: 0.0030\n",
            "Epoch 50/150\n",
            "3/3 - 1s - 322ms/step - loss: 0.0017 - val_loss: 0.0028\n",
            "Epoch 51/150\n",
            "3/3 - 1s - 209ms/step - loss: 0.0016 - val_loss: 0.0025\n",
            "Epoch 52/150\n",
            "3/3 - 1s - 208ms/step - loss: 0.0016 - val_loss: 0.0028\n",
            "Epoch 53/150\n",
            "3/3 - 1s - 204ms/step - loss: 0.0016 - val_loss: 0.0031\n",
            "Epoch 54/150\n",
            "3/3 - 1s - 215ms/step - loss: 0.0015 - val_loss: 0.0027\n",
            "Epoch 55/150\n",
            "3/3 - 0s - 130ms/step - loss: 0.0016 - val_loss: 0.0025\n",
            "Epoch 56/150\n",
            "3/3 - 1s - 216ms/step - loss: 0.0015 - val_loss: 0.0026\n",
            "Epoch 57/150\n",
            "3/3 - 0s - 131ms/step - loss: 0.0015 - val_loss: 0.0028\n",
            "Epoch 58/150\n",
            "3/3 - 0s - 137ms/step - loss: 0.0015 - val_loss: 0.0028\n",
            "Epoch 59/150\n",
            "3/3 - 0s - 131ms/step - loss: 0.0015 - val_loss: 0.0023\n",
            "Epoch 60/150\n",
            "3/3 - 0s - 129ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 61/150\n",
            "3/3 - 1s - 210ms/step - loss: 0.0015 - val_loss: 0.0028\n",
            "Epoch 62/150\n",
            "3/3 - 1s - 217ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 63/150\n",
            "3/3 - 1s - 195ms/step - loss: 0.0014 - val_loss: 0.0023\n",
            "Epoch 64/150\n",
            "3/3 - 0s - 137ms/step - loss: 0.0014 - val_loss: 0.0025\n",
            "Epoch 65/150\n",
            "3/3 - 1s - 199ms/step - loss: 0.0015 - val_loss: 0.0026\n",
            "Epoch 66/150\n",
            "3/3 - 0s - 134ms/step - loss: 0.0014 - val_loss: 0.0024\n",
            "Epoch 67/150\n",
            "3/3 - 0s - 129ms/step - loss: 0.0015 - val_loss: 0.0023\n",
            "Epoch 68/150\n",
            "3/3 - 1s - 214ms/step - loss: 0.0014 - val_loss: 0.0025\n",
            "Epoch 69/150\n",
            "3/3 - 1s - 197ms/step - loss: 0.0014 - val_loss: 0.0022\n",
            "Epoch 70/150\n",
            "3/3 - 1s - 247ms/step - loss: 0.0015 - val_loss: 0.0023\n",
            "Epoch 71/150\n",
            "3/3 - 1s - 219ms/step - loss: 0.0014 - val_loss: 0.0026\n",
            "Epoch 72/150\n",
            "3/3 - 1s - 226ms/step - loss: 0.0014 - val_loss: 0.0022\n",
            "Epoch 73/150\n",
            "3/3 - 1s - 332ms/step - loss: 0.0013 - val_loss: 0.0022\n",
            "Epoch 74/150\n",
            "3/3 - 0s - 132ms/step - loss: 0.0013 - val_loss: 0.0022\n",
            "Epoch 75/150\n",
            "3/3 - 0s - 134ms/step - loss: 0.0014 - val_loss: 0.0025\n",
            "Epoch 76/150\n",
            "3/3 - 0s - 131ms/step - loss: 0.0014 - val_loss: 0.0023\n",
            "Epoch 77/150\n",
            "3/3 - 0s - 135ms/step - loss: 0.0013 - val_loss: 0.0022\n",
            "Epoch 78/150\n",
            "3/3 - 0s - 132ms/step - loss: 0.0014 - val_loss: 0.0021\n",
            "Epoch 79/150\n",
            "3/3 - 1s - 211ms/step - loss: 0.0014 - val_loss: 0.0024\n",
            "Epoch 80/150\n",
            "3/3 - 0s - 131ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "Epoch 81/150\n",
            "3/3 - 0s - 131ms/step - loss: 0.0014 - val_loss: 0.0021\n",
            "Epoch 82/150\n",
            "3/3 - 1s - 208ms/step - loss: 0.0014 - val_loss: 0.0023\n",
            "Epoch 83/150\n",
            "3/3 - 0s - 130ms/step - loss: 0.0013 - val_loss: 0.0018\n",
            "Epoch 84/150\n",
            "3/3 - 0s - 135ms/step - loss: 0.0013 - val_loss: 0.0022\n",
            "Epoch 85/150\n",
            "3/3 - 0s - 130ms/step - loss: 0.0013 - val_loss: 0.0023\n",
            "Epoch 86/150\n",
            "3/3 - 1s - 211ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "Epoch 87/150\n",
            "3/3 - 0s - 126ms/step - loss: 0.0013 - val_loss: 0.0025\n",
            "Epoch 88/150\n",
            "3/3 - 1s - 213ms/step - loss: 0.0013 - val_loss: 0.0025\n",
            "Epoch 89/150\n",
            "3/3 - 1s - 205ms/step - loss: 0.0012 - val_loss: 0.0017\n",
            "Epoch 90/150\n",
            "3/3 - 0s - 132ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "Epoch 91/150\n",
            "3/3 - 1s - 208ms/step - loss: 0.0013 - val_loss: 0.0029\n",
            "Epoch 92/150\n",
            "3/3 - 1s - 201ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "Epoch 93/150\n",
            "3/3 - 0s - 165ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "Epoch 94/150\n",
            "3/3 - 1s - 219ms/step - loss: 0.0013 - val_loss: 0.0024\n",
            "Epoch 95/150\n",
            "3/3 - 1s - 224ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 96/150\n",
            "3/3 - 1s - 224ms/step - loss: 0.0013 - val_loss: 0.0018\n",
            "Epoch 97/150\n",
            "3/3 - 1s - 236ms/step - loss: 0.0013 - val_loss: 0.0028\n",
            "Epoch 98/150\n",
            "3/3 - 1s - 244ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 99/150\n",
            "3/3 - 1s - 203ms/step - loss: 0.0013 - val_loss: 0.0021\n",
            "Epoch 100/150\n",
            "3/3 - 0s - 145ms/step - loss: 0.0013 - val_loss: 0.0023\n",
            "Epoch 101/150\n",
            "3/3 - 0s - 130ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 102/150\n",
            "3/3 - 0s - 130ms/step - loss: 0.0012 - val_loss: 0.0020\n",
            "Epoch 103/150\n",
            "3/3 - 1s - 206ms/step - loss: 0.0012 - val_loss: 0.0026\n",
            "Epoch 104/150\n",
            "3/3 - 0s - 135ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 105/150\n",
            "3/3 - 0s - 131ms/step - loss: 0.0012 - val_loss: 0.0018\n",
            "Epoch 106/150\n",
            "3/3 - 1s - 208ms/step - loss: 0.0012 - val_loss: 0.0022\n",
            "Epoch 107/150\n",
            "3/3 - 1s - 208ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 108/150\n",
            "3/3 - 0s - 134ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "Epoch 109/150\n",
            "3/3 - 1s - 205ms/step - loss: 0.0011 - val_loss: 0.0019\n",
            "Epoch 110/150\n",
            "3/3 - 0s - 135ms/step - loss: 0.0011 - val_loss: 0.0019\n",
            "Epoch 111/150\n",
            "3/3 - 1s - 201ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 112/150\n",
            "3/3 - 0s - 136ms/step - loss: 0.0011 - val_loss: 0.0017\n",
            "Epoch 113/150\n",
            "3/3 - 1s - 200ms/step - loss: 0.0011 - val_loss: 0.0018\n",
            "Epoch 114/150\n",
            "3/3 - 1s - 218ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 115/150\n",
            "3/3 - 0s - 128ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 116/150\n",
            "3/3 - 1s - 212ms/step - loss: 0.0011 - val_loss: 0.0019\n",
            "Epoch 117/150\n",
            "3/3 - 0s - 124ms/step - loss: 0.0011 - val_loss: 0.0016\n",
            "Epoch 118/150\n",
            "3/3 - 1s - 231ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 119/150\n",
            "3/3 - 1s - 268ms/step - loss: 0.0011 - val_loss: 0.0018\n",
            "Epoch 120/150\n",
            "3/3 - 1s - 219ms/step - loss: 0.0010 - val_loss: 0.0015\n",
            "Epoch 121/150\n",
            "3/3 - 1s - 225ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 122/150\n",
            "3/3 - 1s - 367ms/step - loss: 0.0010 - val_loss: 0.0015\n",
            "Epoch 123/150\n",
            "3/3 - 1s - 170ms/step - loss: 0.0011 - val_loss: 0.0016\n",
            "Epoch 124/150\n",
            "3/3 - 0s - 140ms/step - loss: 9.8570e-04 - val_loss: 0.0013\n",
            "Epoch 125/150\n",
            "3/3 - 1s - 198ms/step - loss: 9.9853e-04 - val_loss: 0.0017\n",
            "Epoch 126/150\n",
            "3/3 - 0s - 137ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 127/150\n",
            "3/3 - 0s - 130ms/step - loss: 9.9527e-04 - val_loss: 0.0012\n",
            "Epoch 128/150\n",
            "3/3 - 0s - 135ms/step - loss: 0.0010 - val_loss: 0.0016\n",
            "Epoch 129/150\n",
            "3/3 - 1s - 209ms/step - loss: 9.3986e-04 - val_loss: 0.0014\n",
            "Epoch 130/150\n",
            "3/3 - 1s - 207ms/step - loss: 9.6715e-04 - val_loss: 0.0012\n",
            "Epoch 131/150\n",
            "3/3 - 0s - 131ms/step - loss: 9.5975e-04 - val_loss: 0.0014\n",
            "Epoch 132/150\n",
            "3/3 - 0s - 139ms/step - loss: 0.0010 - val_loss: 0.0015\n",
            "Epoch 133/150\n",
            "3/3 - 1s - 194ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 134/150\n",
            "3/3 - 0s - 134ms/step - loss: 9.8373e-04 - val_loss: 0.0014\n",
            "Epoch 135/150\n",
            "3/3 - 1s - 205ms/step - loss: 9.5892e-04 - val_loss: 0.0014\n",
            "Epoch 136/150\n",
            "3/3 - 1s - 209ms/step - loss: 9.7101e-04 - val_loss: 0.0013\n",
            "Epoch 137/150\n",
            "3/3 - 1s - 203ms/step - loss: 9.4514e-04 - val_loss: 0.0014\n",
            "Epoch 138/150\n",
            "3/3 - 1s - 204ms/step - loss: 9.8748e-04 - val_loss: 0.0011\n",
            "Epoch 139/150\n",
            "3/3 - 0s - 132ms/step - loss: 9.5679e-04 - val_loss: 0.0015\n",
            "Epoch 140/150\n",
            "3/3 - 1s - 206ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 141/150\n",
            "3/3 - 1s - 245ms/step - loss: 0.0010 - val_loss: 9.8319e-04\n",
            "Epoch 142/150\n",
            "3/3 - 1s - 228ms/step - loss: 8.8371e-04 - val_loss: 0.0018\n",
            "Epoch 143/150\n",
            "3/3 - 1s - 418ms/step - loss: 9.4445e-04 - val_loss: 9.5106e-04\n",
            "Epoch 144/150\n",
            "3/3 - 1s - 330ms/step - loss: 9.8300e-04 - val_loss: 0.0013\n",
            "Epoch 145/150\n",
            "3/3 - 1s - 205ms/step - loss: 8.6122e-04 - val_loss: 0.0015\n",
            "Epoch 146/150\n",
            "3/3 - 1s - 211ms/step - loss: 9.9184e-04 - val_loss: 9.7529e-04\n",
            "Epoch 147/150\n",
            "3/3 - 1s - 201ms/step - loss: 9.0384e-04 - val_loss: 0.0013\n",
            "Epoch 148/150\n",
            "3/3 - 0s - 136ms/step - loss: 9.3385e-04 - val_loss: 0.0013\n",
            "Epoch 149/150\n",
            "3/3 - 0s - 132ms/step - loss: 8.7840e-04 - val_loss: 0.0010\n",
            "Epoch 150/150\n",
            "3/3 - 0s - 133ms/step - loss: 9.1954e-04 - val_loss: 0.0012\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step\n",
            "Training with Epochs: 200, Batch Size: 16\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 - 4s - 183ms/step - loss: 0.0155 - val_loss: 0.0029\n",
            "Epoch 2/200\n",
            "22/22 - 1s - 40ms/step - loss: 0.0026 - val_loss: 0.0032\n",
            "Epoch 3/200\n",
            "22/22 - 2s - 75ms/step - loss: 0.0021 - val_loss: 0.0054\n",
            "Epoch 4/200\n",
            "22/22 - 1s - 61ms/step - loss: 0.0019 - val_loss: 0.0027\n",
            "Epoch 5/200\n",
            "22/22 - 1s - 59ms/step - loss: 0.0019 - val_loss: 0.0032\n",
            "Epoch 6/200\n",
            "22/22 - 1s - 44ms/step - loss: 0.0018 - val_loss: 0.0028\n",
            "Epoch 7/200\n",
            "22/22 - 1s - 34ms/step - loss: 0.0017 - val_loss: 0.0036\n",
            "Epoch 8/200\n",
            "22/22 - 1s - 34ms/step - loss: 0.0017 - val_loss: 0.0033\n",
            "Epoch 9/200\n",
            "22/22 - 1s - 34ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 10/200\n",
            "22/22 - 1s - 57ms/step - loss: 0.0015 - val_loss: 0.0030\n",
            "Epoch 11/200\n",
            "22/22 - 1s - 34ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "Epoch 12/200\n",
            "22/22 - 1s - 57ms/step - loss: 0.0014 - val_loss: 0.0036\n",
            "Epoch 13/200\n",
            "22/22 - 1s - 58ms/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 14/200\n",
            "22/22 - 1s - 33ms/step - loss: 0.0013 - val_loss: 0.0018\n",
            "Epoch 15/200\n",
            "22/22 - 1s - 34ms/step - loss: 0.0012 - val_loss: 0.0018\n",
            "Epoch 16/200\n",
            "22/22 - 1s - 58ms/step - loss: 0.0012 - val_loss: 0.0024\n",
            "Epoch 17/200\n",
            "22/22 - 2s - 76ms/step - loss: 0.0013 - val_loss: 0.0018\n",
            "Epoch 18/200\n",
            "22/22 - 1s - 60ms/step - loss: 0.0012 - val_loss: 0.0010\n",
            "Epoch 19/200\n",
            "22/22 - 1s - 41ms/step - loss: 0.0013 - val_loss: 0.0010\n",
            "Epoch 20/200\n",
            "22/22 - 1s - 51ms/step - loss: 0.0010 - val_loss: 0.0019\n",
            "Epoch 21/200\n",
            "22/22 - 1s - 56ms/step - loss: 0.0010 - val_loss: 0.0018\n",
            "Epoch 22/200\n",
            "22/22 - 1s - 33ms/step - loss: 0.0011 - val_loss: 9.0293e-04\n",
            "Epoch 23/200\n",
            "22/22 - 1s - 34ms/step - loss: 0.0010 - val_loss: 0.0019\n",
            "Epoch 24/200\n",
            "22/22 - 1s - 34ms/step - loss: 0.0011 - val_loss: 0.0025\n",
            "Epoch 25/200\n",
            "22/22 - 1s - 57ms/step - loss: 9.2775e-04 - val_loss: 9.8221e-04\n",
            "Epoch 26/200\n",
            "22/22 - 1s - 57ms/step - loss: 8.5528e-04 - val_loss: 0.0015\n",
            "Epoch 27/200\n",
            "22/22 - 1s - 57ms/step - loss: 9.6931e-04 - val_loss: 0.0014\n",
            "Epoch 28/200\n",
            "22/22 - 1s - 34ms/step - loss: 8.4483e-04 - val_loss: 0.0012\n",
            "Epoch 29/200\n",
            "22/22 - 1s - 45ms/step - loss: 8.3586e-04 - val_loss: 6.9734e-04\n",
            "Epoch 30/200\n",
            "22/22 - 1s - 54ms/step - loss: 8.6677e-04 - val_loss: 0.0011\n",
            "Epoch 31/200\n",
            "22/22 - 1s - 59ms/step - loss: 8.3199e-04 - val_loss: 9.8735e-04\n",
            "Epoch 32/200\n",
            "22/22 - 1s - 45ms/step - loss: 7.8059e-04 - val_loss: 6.7094e-04\n",
            "Epoch 33/200\n",
            "22/22 - 1s - 35ms/step - loss: 7.1595e-04 - val_loss: 6.3343e-04\n",
            "Epoch 34/200\n",
            "22/22 - 1s - 56ms/step - loss: 8.3050e-04 - val_loss: 0.0011\n",
            "Epoch 35/200\n",
            "22/22 - 1s - 34ms/step - loss: 9.3656e-04 - val_loss: 0.0010\n",
            "Epoch 36/200\n",
            "22/22 - 1s - 36ms/step - loss: 7.9547e-04 - val_loss: 7.3478e-04\n",
            "Epoch 37/200\n",
            "22/22 - 1s - 33ms/step - loss: 7.4989e-04 - val_loss: 6.6222e-04\n",
            "Epoch 38/200\n",
            "22/22 - 1s - 34ms/step - loss: 9.3729e-04 - val_loss: 6.9645e-04\n",
            "Epoch 39/200\n",
            "22/22 - 1s - 35ms/step - loss: 8.0285e-04 - val_loss: 5.8660e-04\n",
            "Epoch 40/200\n",
            "22/22 - 1s - 34ms/step - loss: 7.4406e-04 - val_loss: 6.2218e-04\n",
            "Epoch 41/200\n",
            "22/22 - 1s - 34ms/step - loss: 8.3027e-04 - val_loss: 0.0013\n",
            "Epoch 42/200\n",
            "22/22 - 1s - 34ms/step - loss: 7.7812e-04 - val_loss: 9.1567e-04\n",
            "Epoch 43/200\n",
            "22/22 - 1s - 35ms/step - loss: 7.6949e-04 - val_loss: 9.3832e-04\n",
            "Epoch 44/200\n",
            "22/22 - 2s - 71ms/step - loss: 7.0989e-04 - val_loss: 0.0016\n",
            "Epoch 45/200\n",
            "22/22 - 1s - 64ms/step - loss: 7.4550e-04 - val_loss: 7.8469e-04\n",
            "Epoch 46/200\n",
            "22/22 - 1s - 58ms/step - loss: 6.8203e-04 - val_loss: 0.0012\n",
            "Epoch 47/200\n",
            "22/22 - 1s - 42ms/step - loss: 7.7293e-04 - val_loss: 5.2794e-04\n",
            "Epoch 48/200\n",
            "22/22 - 1s - 34ms/step - loss: 7.1611e-04 - val_loss: 5.4076e-04\n",
            "Epoch 49/200\n",
            "22/22 - 1s - 34ms/step - loss: 7.4333e-04 - val_loss: 0.0013\n",
            "Epoch 50/200\n",
            "22/22 - 1s - 56ms/step - loss: 8.6533e-04 - val_loss: 8.0200e-04\n",
            "Epoch 51/200\n",
            "22/22 - 1s - 58ms/step - loss: 6.4871e-04 - val_loss: 9.4490e-04\n",
            "Epoch 52/200\n",
            "22/22 - 1s - 57ms/step - loss: 6.3823e-04 - val_loss: 5.8320e-04\n",
            "Epoch 53/200\n",
            "22/22 - 1s - 33ms/step - loss: 6.3762e-04 - val_loss: 5.0233e-04\n",
            "Epoch 54/200\n",
            "22/22 - 1s - 58ms/step - loss: 6.3878e-04 - val_loss: 5.7322e-04\n",
            "Epoch 55/200\n",
            "22/22 - 1s - 58ms/step - loss: 6.2530e-04 - val_loss: 4.8634e-04\n",
            "Epoch 56/200\n",
            "22/22 - 1s - 66ms/step - loss: 7.0292e-04 - val_loss: 6.2240e-04\n",
            "Epoch 57/200\n",
            "22/22 - 1s - 56ms/step - loss: 6.8471e-04 - val_loss: 0.0013\n",
            "Epoch 58/200\n",
            "22/22 - 1s - 55ms/step - loss: 6.7614e-04 - val_loss: 4.7377e-04\n",
            "Epoch 59/200\n",
            "22/22 - 1s - 47ms/step - loss: 6.5086e-04 - val_loss: 5.5439e-04\n",
            "Epoch 60/200\n",
            "22/22 - 1s - 48ms/step - loss: 7.4725e-04 - val_loss: 6.4182e-04\n",
            "Epoch 61/200\n",
            "22/22 - 1s - 57ms/step - loss: 6.1506e-04 - val_loss: 5.9772e-04\n",
            "Epoch 62/200\n",
            "22/22 - 1s - 57ms/step - loss: 6.4667e-04 - val_loss: 4.5603e-04\n",
            "Epoch 63/200\n",
            "22/22 - 1s - 57ms/step - loss: 6.3173e-04 - val_loss: 0.0011\n",
            "Epoch 64/200\n",
            "22/22 - 1s - 59ms/step - loss: 6.3662e-04 - val_loss: 5.6906e-04\n",
            "Epoch 65/200\n",
            "22/22 - 1s - 56ms/step - loss: 5.6916e-04 - val_loss: 4.8898e-04\n",
            "Epoch 66/200\n",
            "22/22 - 1s - 34ms/step - loss: 7.5788e-04 - val_loss: 4.5309e-04\n",
            "Epoch 67/200\n",
            "22/22 - 1s - 33ms/step - loss: 7.1464e-04 - val_loss: 0.0010\n",
            "Epoch 68/200\n",
            "22/22 - 1s - 37ms/step - loss: 6.8373e-04 - val_loss: 0.0013\n",
            "Epoch 69/200\n",
            "22/22 - 1s - 57ms/step - loss: 5.9802e-04 - val_loss: 5.1381e-04\n",
            "Epoch 70/200\n",
            "22/22 - 1s - 59ms/step - loss: 6.1733e-04 - val_loss: 4.6145e-04\n",
            "Epoch 71/200\n",
            "22/22 - 1s - 57ms/step - loss: 6.4977e-04 - val_loss: 4.9025e-04\n",
            "Epoch 72/200\n",
            "22/22 - 1s - 34ms/step - loss: 6.6503e-04 - val_loss: 4.7805e-04\n",
            "Epoch 73/200\n",
            "22/22 - 1s - 35ms/step - loss: 6.5274e-04 - val_loss: 4.7711e-04\n",
            "Epoch 74/200\n",
            "22/22 - 1s - 34ms/step - loss: 6.1058e-04 - val_loss: 4.1887e-04\n",
            "Epoch 75/200\n",
            "22/22 - 1s - 58ms/step - loss: 6.4446e-04 - val_loss: 6.1721e-04\n",
            "Epoch 76/200\n",
            "22/22 - 1s - 56ms/step - loss: 6.1752e-04 - val_loss: 5.0698e-04\n",
            "Epoch 77/200\n",
            "22/22 - 1s - 35ms/step - loss: 5.7232e-04 - val_loss: 4.6356e-04\n",
            "Epoch 78/200\n",
            "22/22 - 1s - 34ms/step - loss: 5.8186e-04 - val_loss: 4.7982e-04\n",
            "Epoch 79/200\n",
            "22/22 - 1s - 58ms/step - loss: 6.4392e-04 - val_loss: 4.5621e-04\n",
            "Epoch 80/200\n",
            "22/22 - 1s - 56ms/step - loss: 9.5375e-04 - val_loss: 4.3534e-04\n",
            "Epoch 81/200\n",
            "22/22 - 1s - 35ms/step - loss: 6.1041e-04 - val_loss: 4.2205e-04\n",
            "Epoch 82/200\n",
            "22/22 - 2s - 78ms/step - loss: 6.1114e-04 - val_loss: 5.2553e-04\n",
            "Epoch 83/200\n",
            "22/22 - 1s - 59ms/step - loss: 6.1838e-04 - val_loss: 6.3495e-04\n",
            "Epoch 84/200\n",
            "22/22 - 1s - 55ms/step - loss: 5.3259e-04 - val_loss: 0.0012\n",
            "Epoch 85/200\n",
            "22/22 - 1s - 37ms/step - loss: 5.9610e-04 - val_loss: 4.1620e-04\n",
            "Epoch 86/200\n",
            "22/22 - 1s - 57ms/step - loss: 5.4108e-04 - val_loss: 4.4778e-04\n",
            "Epoch 87/200\n",
            "22/22 - 1s - 34ms/step - loss: 5.8478e-04 - val_loss: 5.1547e-04\n",
            "Epoch 88/200\n",
            "22/22 - 1s - 57ms/step - loss: 5.4755e-04 - val_loss: 9.5864e-04\n",
            "Epoch 89/200\n",
            "22/22 - 1s - 57ms/step - loss: 5.6769e-04 - val_loss: 4.1926e-04\n",
            "Epoch 90/200\n",
            "22/22 - 1s - 35ms/step - loss: 6.4447e-04 - val_loss: 6.2247e-04\n",
            "Epoch 91/200\n",
            "22/22 - 1s - 57ms/step - loss: 5.7151e-04 - val_loss: 3.8554e-04\n",
            "Epoch 92/200\n",
            "22/22 - 1s - 59ms/step - loss: 6.0248e-04 - val_loss: 3.7823e-04\n",
            "Epoch 93/200\n",
            "22/22 - 1s - 58ms/step - loss: 5.9439e-04 - val_loss: 3.8196e-04\n",
            "Epoch 94/200\n",
            "22/22 - 1s - 57ms/step - loss: 6.7118e-04 - val_loss: 4.7306e-04\n",
            "Epoch 95/200\n",
            "22/22 - 1s - 58ms/step - loss: 5.6387e-04 - val_loss: 8.2942e-04\n",
            "Epoch 96/200\n",
            "22/22 - 1s - 61ms/step - loss: 5.7260e-04 - val_loss: 5.5970e-04\n",
            "Epoch 97/200\n",
            "22/22 - 2s - 90ms/step - loss: 6.1298e-04 - val_loss: 8.2694e-04\n",
            "Epoch 98/200\n",
            "22/22 - 1s - 35ms/step - loss: 5.7220e-04 - val_loss: 6.2229e-04\n",
            "Epoch 99/200\n",
            "22/22 - 1s - 35ms/step - loss: 5.5291e-04 - val_loss: 4.2594e-04\n",
            "Epoch 100/200\n",
            "22/22 - 1s - 57ms/step - loss: 5.4657e-04 - val_loss: 5.0449e-04\n",
            "Epoch 101/200\n",
            "22/22 - 1s - 35ms/step - loss: 5.6939e-04 - val_loss: 0.0013\n",
            "Epoch 102/200\n",
            "22/22 - 1s - 57ms/step - loss: 6.3588e-04 - val_loss: 0.0011\n",
            "Epoch 103/200\n",
            "22/22 - 1s - 34ms/step - loss: 5.6254e-04 - val_loss: 6.6730e-04\n",
            "Epoch 104/200\n",
            "22/22 - 1s - 57ms/step - loss: 5.2942e-04 - val_loss: 3.7094e-04\n",
            "Epoch 105/200\n",
            "22/22 - 1s - 61ms/step - loss: 5.0115e-04 - val_loss: 4.8092e-04\n",
            "Epoch 106/200\n",
            "22/22 - 1s - 56ms/step - loss: 5.3422e-04 - val_loss: 3.7805e-04\n",
            "Epoch 107/200\n",
            "22/22 - 1s - 57ms/step - loss: 5.8804e-04 - val_loss: 6.4577e-04\n",
            "Epoch 108/200\n",
            "22/22 - 1s - 58ms/step - loss: 5.1166e-04 - val_loss: 6.2317e-04\n",
            "Epoch 109/200\n",
            "22/22 - 1s - 34ms/step - loss: 5.3633e-04 - val_loss: 3.4937e-04\n",
            "Epoch 110/200\n",
            "22/22 - 1s - 34ms/step - loss: 5.8442e-04 - val_loss: 3.5724e-04\n",
            "Epoch 111/200\n",
            "22/22 - 1s - 34ms/step - loss: 5.1158e-04 - val_loss: 3.6976e-04\n",
            "Epoch 112/200\n",
            "22/22 - 1s - 35ms/step - loss: 6.0547e-04 - val_loss: 5.0413e-04\n",
            "Epoch 113/200\n",
            "22/22 - 1s - 56ms/step - loss: 5.5464e-04 - val_loss: 4.4458e-04\n",
            "Epoch 114/200\n",
            "22/22 - 1s - 57ms/step - loss: 5.6296e-04 - val_loss: 0.0012\n",
            "Epoch 115/200\n",
            "22/22 - 1s - 57ms/step - loss: 5.4114e-04 - val_loss: 3.3722e-04\n",
            "Epoch 116/200\n",
            "22/22 - 1s - 58ms/step - loss: 5.5164e-04 - val_loss: 5.0095e-04\n",
            "Epoch 117/200\n",
            "22/22 - 1s - 57ms/step - loss: 6.1196e-04 - val_loss: 4.0622e-04\n",
            "Epoch 118/200\n",
            "22/22 - 1s - 37ms/step - loss: 5.7736e-04 - val_loss: 3.3711e-04\n",
            "Epoch 119/200\n",
            "22/22 - 2s - 75ms/step - loss: 5.1788e-04 - val_loss: 4.4419e-04\n",
            "Epoch 120/200\n",
            "22/22 - 1s - 60ms/step - loss: 5.0895e-04 - val_loss: 7.0196e-04\n",
            "Epoch 121/200\n",
            "22/22 - 2s - 92ms/step - loss: 4.9927e-04 - val_loss: 4.8004e-04\n",
            "Epoch 122/200\n",
            "22/22 - 1s - 58ms/step - loss: 5.0306e-04 - val_loss: 9.2224e-04\n",
            "Epoch 123/200\n",
            "22/22 - 1s - 35ms/step - loss: 6.2775e-04 - val_loss: 8.8897e-04\n",
            "Epoch 124/200\n",
            "22/22 - 1s - 57ms/step - loss: 4.8076e-04 - val_loss: 4.2280e-04\n",
            "Epoch 125/200\n",
            "22/22 - 1s - 57ms/step - loss: 4.9990e-04 - val_loss: 3.4103e-04\n",
            "Epoch 126/200\n",
            "22/22 - 1s - 57ms/step - loss: 4.9173e-04 - val_loss: 4.8546e-04\n",
            "Epoch 127/200\n",
            "22/22 - 1s - 34ms/step - loss: 4.8464e-04 - val_loss: 3.5630e-04\n",
            "Epoch 128/200\n",
            "22/22 - 1s - 34ms/step - loss: 5.5204e-04 - val_loss: 4.4190e-04\n",
            "Epoch 129/200\n",
            "22/22 - 1s - 33ms/step - loss: 5.6894e-04 - val_loss: 4.6750e-04\n",
            "Epoch 130/200\n",
            "22/22 - 1s - 51ms/step - loss: 4.9322e-04 - val_loss: 8.3448e-04\n",
            "Epoch 131/200\n",
            "22/22 - 1s - 56ms/step - loss: 5.0272e-04 - val_loss: 6.8601e-04\n",
            "Epoch 132/200\n",
            "22/22 - 1s - 60ms/step - loss: 5.3661e-04 - val_loss: 8.7047e-04\n",
            "Epoch 133/200\n",
            "22/22 - 2s - 92ms/step - loss: 5.0942e-04 - val_loss: 3.1210e-04\n",
            "Epoch 134/200\n",
            "22/22 - 1s - 57ms/step - loss: 5.1572e-04 - val_loss: 6.0552e-04\n",
            "Epoch 135/200\n",
            "22/22 - 1s - 56ms/step - loss: 5.8281e-04 - val_loss: 3.2020e-04\n",
            "Epoch 136/200\n",
            "22/22 - 1s - 34ms/step - loss: 4.9723e-04 - val_loss: 3.5567e-04\n",
            "Epoch 137/200\n",
            "22/22 - 1s - 57ms/step - loss: 5.0769e-04 - val_loss: 3.2208e-04\n",
            "Epoch 138/200\n",
            "22/22 - 1s - 34ms/step - loss: 5.0851e-04 - val_loss: 6.7072e-04\n",
            "Epoch 139/200\n",
            "22/22 - 1s - 58ms/step - loss: 5.3151e-04 - val_loss: 7.9341e-04\n",
            "Epoch 140/200\n",
            "22/22 - 1s - 34ms/step - loss: 5.0239e-04 - val_loss: 3.1346e-04\n",
            "Epoch 141/200\n",
            "22/22 - 2s - 69ms/step - loss: 5.2877e-04 - val_loss: 4.9445e-04\n",
            "Epoch 142/200\n",
            "22/22 - 1s - 57ms/step - loss: 4.6019e-04 - val_loss: 3.3921e-04\n",
            "Epoch 143/200\n",
            "22/22 - 1s - 59ms/step - loss: 5.1822e-04 - val_loss: 6.4008e-04\n",
            "Epoch 144/200\n",
            "22/22 - 2s - 93ms/step - loss: 5.0879e-04 - val_loss: 3.5892e-04\n",
            "Epoch 145/200\n",
            "22/22 - 1s - 34ms/step - loss: 5.3743e-04 - val_loss: 8.4338e-04\n",
            "Epoch 146/200\n",
            "22/22 - 1s - 35ms/step - loss: 4.9431e-04 - val_loss: 4.1984e-04\n",
            "Epoch 147/200\n",
            "22/22 - 1s - 56ms/step - loss: 5.3957e-04 - val_loss: 9.4603e-04\n",
            "Epoch 148/200\n",
            "22/22 - 1s - 58ms/step - loss: 4.6709e-04 - val_loss: 5.7918e-04\n",
            "Epoch 149/200\n",
            "22/22 - 1s - 57ms/step - loss: 6.0546e-04 - val_loss: 6.0516e-04\n",
            "Epoch 150/200\n",
            "22/22 - 1s - 57ms/step - loss: 5.1477e-04 - val_loss: 5.5184e-04\n",
            "Epoch 151/200\n",
            "22/22 - 1s - 35ms/step - loss: 5.0052e-04 - val_loss: 3.7263e-04\n",
            "Epoch 152/200\n",
            "22/22 - 1s - 35ms/step - loss: 4.9669e-04 - val_loss: 3.8468e-04\n",
            "Epoch 153/200\n",
            "22/22 - 2s - 78ms/step - loss: 4.8747e-04 - val_loss: 3.9615e-04\n",
            "Epoch 154/200\n",
            "22/22 - 1s - 59ms/step - loss: 4.9738e-04 - val_loss: 3.7601e-04\n",
            "Epoch 155/200\n",
            "22/22 - 1s - 43ms/step - loss: 5.1506e-04 - val_loss: 3.3170e-04\n",
            "Epoch 156/200\n",
            "22/22 - 1s - 52ms/step - loss: 5.0548e-04 - val_loss: 3.1179e-04\n",
            "Epoch 157/200\n",
            "22/22 - 1s - 34ms/step - loss: 4.6051e-04 - val_loss: 3.7121e-04\n",
            "Epoch 158/200\n",
            "22/22 - 1s - 57ms/step - loss: 4.8943e-04 - val_loss: 3.0332e-04\n",
            "Epoch 159/200\n",
            "22/22 - 1s - 57ms/step - loss: 5.3923e-04 - val_loss: 3.3717e-04\n",
            "Epoch 160/200\n",
            "22/22 - 1s - 58ms/step - loss: 5.1591e-04 - val_loss: 3.3646e-04\n",
            "Epoch 161/200\n",
            "22/22 - 1s - 35ms/step - loss: 4.7072e-04 - val_loss: 3.1593e-04\n",
            "Epoch 162/200\n",
            "22/22 - 1s - 57ms/step - loss: 5.5115e-04 - val_loss: 7.3898e-04\n",
            "Epoch 163/200\n",
            "22/22 - 1s - 57ms/step - loss: 5.1986e-04 - val_loss: 3.1924e-04\n",
            "Epoch 164/200\n",
            "22/22 - 2s - 77ms/step - loss: 5.3275e-04 - val_loss: 3.5325e-04\n",
            "Epoch 165/200\n",
            "22/22 - 1s - 56ms/step - loss: 4.9405e-04 - val_loss: 9.7373e-04\n",
            "Epoch 166/200\n",
            "22/22 - 1s - 59ms/step - loss: 5.5752e-04 - val_loss: 6.3046e-04\n",
            "Epoch 167/200\n",
            "22/22 - 2s - 92ms/step - loss: 4.5989e-04 - val_loss: 3.7442e-04\n",
            "Epoch 168/200\n",
            "22/22 - 1s - 34ms/step - loss: 4.9125e-04 - val_loss: 4.5258e-04\n",
            "Epoch 169/200\n",
            "22/22 - 1s - 34ms/step - loss: 4.7179e-04 - val_loss: 4.8885e-04\n",
            "Epoch 170/200\n",
            "22/22 - 1s - 57ms/step - loss: 5.5593e-04 - val_loss: 6.7055e-04\n",
            "Epoch 171/200\n",
            "22/22 - 1s - 57ms/step - loss: 4.9691e-04 - val_loss: 5.6615e-04\n",
            "Epoch 172/200\n",
            "22/22 - 1s - 57ms/step - loss: 4.4406e-04 - val_loss: 3.0158e-04\n",
            "Epoch 173/200\n",
            "22/22 - 1s - 34ms/step - loss: 4.5839e-04 - val_loss: 3.6188e-04\n",
            "Epoch 174/200\n",
            "22/22 - 1s - 57ms/step - loss: 4.6626e-04 - val_loss: 3.0691e-04\n",
            "Epoch 175/200\n",
            "22/22 - 1s - 35ms/step - loss: 4.6076e-04 - val_loss: 3.5657e-04\n",
            "Epoch 176/200\n",
            "22/22 - 1s - 54ms/step - loss: 4.7244e-04 - val_loss: 6.2816e-04\n",
            "Epoch 177/200\n",
            "22/22 - 1s - 55ms/step - loss: 4.5087e-04 - val_loss: 6.4658e-04\n",
            "Epoch 178/200\n",
            "22/22 - 1s - 58ms/step - loss: 4.7526e-04 - val_loss: 5.8674e-04\n",
            "Epoch 179/200\n",
            "22/22 - 1s - 42ms/step - loss: 5.7719e-04 - val_loss: 6.2874e-04\n",
            "Epoch 180/200\n",
            "22/22 - 1s - 34ms/step - loss: 5.0468e-04 - val_loss: 5.6356e-04\n",
            "Epoch 181/200\n",
            "22/22 - 1s - 34ms/step - loss: 5.5772e-04 - val_loss: 4.8187e-04\n",
            "Epoch 182/200\n",
            "22/22 - 1s - 34ms/step - loss: 5.0003e-04 - val_loss: 5.2779e-04\n",
            "Epoch 183/200\n",
            "22/22 - 1s - 33ms/step - loss: 5.0200e-04 - val_loss: 7.9168e-04\n",
            "Epoch 184/200\n",
            "22/22 - 1s - 35ms/step - loss: 4.7938e-04 - val_loss: 3.3902e-04\n",
            "Epoch 185/200\n",
            "22/22 - 1s - 34ms/step - loss: 4.6448e-04 - val_loss: 6.7006e-04\n",
            "Epoch 186/200\n",
            "22/22 - 1s - 34ms/step - loss: 4.5571e-04 - val_loss: 5.5326e-04\n",
            "Epoch 187/200\n",
            "22/22 - 1s - 34ms/step - loss: 4.8800e-04 - val_loss: 5.9303e-04\n",
            "Epoch 188/200\n",
            "22/22 - 1s - 34ms/step - loss: 4.7284e-04 - val_loss: 5.2631e-04\n",
            "Epoch 189/200\n",
            "22/22 - 1s - 34ms/step - loss: 5.5816e-04 - val_loss: 3.2276e-04\n",
            "Epoch 190/200\n",
            "22/22 - 1s - 57ms/step - loss: 4.9562e-04 - val_loss: 4.0367e-04\n",
            "Epoch 191/200\n",
            "22/22 - 1s - 35ms/step - loss: 4.8182e-04 - val_loss: 3.0355e-04\n",
            "Epoch 192/200\n",
            "22/22 - 2s - 76ms/step - loss: 4.9900e-04 - val_loss: 3.5206e-04\n",
            "Epoch 193/200\n",
            "22/22 - 1s - 61ms/step - loss: 4.6277e-04 - val_loss: 7.1387e-04\n",
            "Epoch 194/200\n",
            "22/22 - 1s - 41ms/step - loss: 4.6657e-04 - val_loss: 3.0651e-04\n",
            "Epoch 195/200\n",
            "22/22 - 1s - 34ms/step - loss: 4.8354e-04 - val_loss: 3.1356e-04\n",
            "Epoch 196/200\n",
            "22/22 - 1s - 57ms/step - loss: 4.4520e-04 - val_loss: 3.5994e-04\n",
            "Epoch 197/200\n",
            "22/22 - 1s - 58ms/step - loss: 5.0560e-04 - val_loss: 4.0470e-04\n",
            "Epoch 198/200\n",
            "22/22 - 1s - 33ms/step - loss: 4.7046e-04 - val_loss: 5.2706e-04\n",
            "Epoch 199/200\n",
            "22/22 - 1s - 34ms/step - loss: 4.9415e-04 - val_loss: 4.2308e-04\n",
            "Epoch 200/200\n",
            "22/22 - 1s - 58ms/step - loss: 4.6215e-04 - val_loss: 3.0522e-04\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 148ms/step\n",
            "Training with Epochs: 200, Batch Size: 32\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11/11 - 5s - 445ms/step - loss: 0.0179 - val_loss: 0.0242\n",
            "Epoch 2/200\n",
            "11/11 - 2s - 149ms/step - loss: 0.0046 - val_loss: 0.0070\n",
            "Epoch 3/200\n",
            "11/11 - 1s - 55ms/step - loss: 0.0031 - val_loss: 0.0028\n",
            "Epoch 4/200\n",
            "11/11 - 1s - 47ms/step - loss: 0.0024 - val_loss: 0.0050\n",
            "Epoch 5/200\n",
            "11/11 - 1s - 45ms/step - loss: 0.0020 - val_loss: 0.0046\n",
            "Epoch 6/200\n",
            "11/11 - 1s - 56ms/step - loss: 0.0020 - val_loss: 0.0028\n",
            "Epoch 7/200\n",
            "11/11 - 1s - 46ms/step - loss: 0.0018 - val_loss: 0.0030\n",
            "Epoch 8/200\n",
            "11/11 - 1s - 46ms/step - loss: 0.0018 - val_loss: 0.0045\n",
            "Epoch 9/200\n",
            "11/11 - 1s - 47ms/step - loss: 0.0018 - val_loss: 0.0034\n",
            "Epoch 10/200\n",
            "11/11 - 1s - 55ms/step - loss: 0.0017 - val_loss: 0.0024\n",
            "Epoch 11/200\n",
            "11/11 - 1s - 47ms/step - loss: 0.0016 - val_loss: 0.0024\n",
            "Epoch 12/200\n",
            "11/11 - 1s - 54ms/step - loss: 0.0016 - val_loss: 0.0029\n",
            "Epoch 13/200\n",
            "11/11 - 1s - 48ms/step - loss: 0.0016 - val_loss: 0.0030\n",
            "Epoch 14/200\n",
            "11/11 - 1s - 46ms/step - loss: 0.0015 - val_loss: 0.0037\n",
            "Epoch 15/200\n",
            "11/11 - 1s - 58ms/step - loss: 0.0015 - val_loss: 0.0022\n",
            "Epoch 16/200\n",
            "11/11 - 0s - 45ms/step - loss: 0.0015 - val_loss: 0.0019\n",
            "Epoch 17/200\n",
            "11/11 - 1s - 47ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "Epoch 18/200\n",
            "11/11 - 1s - 68ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "Epoch 19/200\n",
            "11/11 - 1s - 75ms/step - loss: 0.0012 - val_loss: 0.0017\n",
            "Epoch 20/200\n",
            "11/11 - 1s - 117ms/step - loss: 0.0013 - val_loss: 0.0018\n",
            "Epoch 21/200\n",
            "11/11 - 1s - 85ms/step - loss: 0.0012 - val_loss: 0.0034\n",
            "Epoch 22/200\n",
            "11/11 - 1s - 57ms/step - loss: 0.0012 - val_loss: 0.0028\n",
            "Epoch 23/200\n",
            "11/11 - 1s - 58ms/step - loss: 0.0012 - val_loss: 0.0017\n",
            "Epoch 24/200\n",
            "11/11 - 1s - 47ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 25/200\n",
            "11/11 - 1s - 57ms/step - loss: 0.0011 - val_loss: 0.0018\n",
            "Epoch 26/200\n",
            "11/11 - 1s - 46ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 27/200\n",
            "11/11 - 1s - 57ms/step - loss: 0.0010 - val_loss: 0.0016\n",
            "Epoch 28/200\n",
            "11/11 - 1s - 47ms/step - loss: 0.0011 - val_loss: 0.0022\n",
            "Epoch 29/200\n",
            "11/11 - 1s - 46ms/step - loss: 9.5432e-04 - val_loss: 0.0017\n",
            "Epoch 30/200\n",
            "11/11 - 1s - 57ms/step - loss: 9.4437e-04 - val_loss: 0.0011\n",
            "Epoch 31/200\n",
            "11/11 - 1s - 56ms/step - loss: 9.8597e-04 - val_loss: 8.6877e-04\n",
            "Epoch 32/200\n",
            "11/11 - 1s - 47ms/step - loss: 0.0010 - val_loss: 0.0015\n",
            "Epoch 33/200\n",
            "11/11 - 1s - 56ms/step - loss: 9.3072e-04 - val_loss: 0.0011\n",
            "Epoch 34/200\n",
            "11/11 - 1s - 47ms/step - loss: 9.5810e-04 - val_loss: 7.9905e-04\n",
            "Epoch 35/200\n",
            "11/11 - 1s - 46ms/step - loss: 9.0905e-04 - val_loss: 0.0015\n",
            "Epoch 36/200\n",
            "11/11 - 1s - 47ms/step - loss: 8.7913e-04 - val_loss: 0.0012\n",
            "Epoch 37/200\n",
            "11/11 - 1s - 54ms/step - loss: 9.3770e-04 - val_loss: 9.1340e-04\n",
            "Epoch 38/200\n",
            "11/11 - 1s - 74ms/step - loss: 9.1099e-04 - val_loss: 7.4095e-04\n",
            "Epoch 39/200\n",
            "11/11 - 1s - 126ms/step - loss: 7.9099e-04 - val_loss: 7.8855e-04\n",
            "Epoch 40/200\n",
            "11/11 - 1s - 100ms/step - loss: 8.8923e-04 - val_loss: 7.0606e-04\n",
            "Epoch 41/200\n",
            "11/11 - 0s - 45ms/step - loss: 8.7195e-04 - val_loss: 9.9694e-04\n",
            "Epoch 42/200\n",
            "11/11 - 1s - 57ms/step - loss: 8.0287e-04 - val_loss: 0.0011\n",
            "Epoch 43/200\n",
            "11/11 - 1s - 57ms/step - loss: 7.6616e-04 - val_loss: 7.9686e-04\n",
            "Epoch 44/200\n",
            "11/11 - 1s - 49ms/step - loss: 7.8009e-04 - val_loss: 0.0010\n",
            "Epoch 45/200\n",
            "11/11 - 0s - 45ms/step - loss: 8.1441e-04 - val_loss: 0.0013\n",
            "Epoch 46/200\n",
            "11/11 - 1s - 47ms/step - loss: 7.7394e-04 - val_loss: 8.8093e-04\n",
            "Epoch 47/200\n",
            "11/11 - 1s - 47ms/step - loss: 7.2653e-04 - val_loss: 0.0013\n",
            "Epoch 48/200\n",
            "11/11 - 1s - 59ms/step - loss: 7.4505e-04 - val_loss: 6.3543e-04\n",
            "Epoch 49/200\n",
            "11/11 - 1s - 53ms/step - loss: 7.6109e-04 - val_loss: 0.0016\n",
            "Epoch 50/200\n",
            "11/11 - 1s - 47ms/step - loss: 9.6897e-04 - val_loss: 0.0018\n",
            "Epoch 51/200\n",
            "11/11 - 0s - 45ms/step - loss: 8.9054e-04 - val_loss: 8.6776e-04\n",
            "Epoch 52/200\n",
            "11/11 - 1s - 48ms/step - loss: 0.0011 - val_loss: 7.5392e-04\n",
            "Epoch 53/200\n",
            "11/11 - 1s - 58ms/step - loss: 7.7981e-04 - val_loss: 7.1670e-04\n",
            "Epoch 54/200\n",
            "11/11 - 1s - 46ms/step - loss: 7.4444e-04 - val_loss: 8.3787e-04\n",
            "Epoch 55/200\n",
            "11/11 - 1s - 58ms/step - loss: 7.3330e-04 - val_loss: 6.4720e-04\n",
            "Epoch 56/200\n",
            "11/11 - 1s - 46ms/step - loss: 7.0167e-04 - val_loss: 5.7787e-04\n",
            "Epoch 57/200\n",
            "11/11 - 1s - 47ms/step - loss: 6.8512e-04 - val_loss: 5.7062e-04\n",
            "Epoch 58/200\n",
            "11/11 - 1s - 66ms/step - loss: 7.2897e-04 - val_loss: 7.2793e-04\n",
            "Epoch 59/200\n",
            "11/11 - 1s - 127ms/step - loss: 6.7845e-04 - val_loss: 8.0182e-04\n",
            "Epoch 60/200\n",
            "11/11 - 1s - 92ms/step - loss: 7.2391e-04 - val_loss: 0.0016\n",
            "Epoch 61/200\n",
            "11/11 - 0s - 45ms/step - loss: 8.1114e-04 - val_loss: 0.0012\n",
            "Epoch 62/200\n",
            "11/11 - 1s - 58ms/step - loss: 6.7782e-04 - val_loss: 9.6978e-04\n",
            "Epoch 63/200\n",
            "11/11 - 1s - 45ms/step - loss: 6.5003e-04 - val_loss: 5.4277e-04\n",
            "Epoch 64/200\n",
            "11/11 - 1s - 57ms/step - loss: 6.3324e-04 - val_loss: 5.7295e-04\n",
            "Epoch 65/200\n",
            "11/11 - 1s - 46ms/step - loss: 6.4688e-04 - val_loss: 7.0040e-04\n",
            "Epoch 66/200\n",
            "11/11 - 1s - 56ms/step - loss: 6.8300e-04 - val_loss: 6.7927e-04\n",
            "Epoch 67/200\n",
            "11/11 - 1s - 48ms/step - loss: 6.4889e-04 - val_loss: 7.9830e-04\n",
            "Epoch 68/200\n",
            "11/11 - 1s - 55ms/step - loss: 6.1002e-04 - val_loss: 6.8660e-04\n",
            "Epoch 69/200\n",
            "11/11 - 1s - 58ms/step - loss: 6.7148e-04 - val_loss: 5.3877e-04\n",
            "Epoch 70/200\n",
            "11/11 - 1s - 46ms/step - loss: 6.4139e-04 - val_loss: 0.0012\n",
            "Epoch 71/200\n",
            "11/11 - 1s - 57ms/step - loss: 7.3938e-04 - val_loss: 7.1337e-04\n",
            "Epoch 72/200\n",
            "11/11 - 1s - 47ms/step - loss: 6.5467e-04 - val_loss: 5.1465e-04\n",
            "Epoch 73/200\n",
            "11/11 - 1s - 55ms/step - loss: 6.6159e-04 - val_loss: 7.3962e-04\n",
            "Epoch 74/200\n",
            "11/11 - 1s - 49ms/step - loss: 6.1356e-04 - val_loss: 5.8640e-04\n",
            "Epoch 75/200\n",
            "11/11 - 0s - 45ms/step - loss: 6.1844e-04 - val_loss: 5.5343e-04\n",
            "Epoch 76/200\n",
            "11/11 - 1s - 47ms/step - loss: 6.7996e-04 - val_loss: 0.0010\n",
            "Epoch 77/200\n",
            "11/11 - 1s - 54ms/step - loss: 6.2679e-04 - val_loss: 9.3967e-04\n",
            "Epoch 78/200\n",
            "11/11 - 1s - 74ms/step - loss: 6.3926e-04 - val_loss: 5.5054e-04\n",
            "Epoch 79/200\n",
            "11/11 - 1s - 74ms/step - loss: 6.1270e-04 - val_loss: 5.2289e-04\n",
            "Epoch 80/200\n",
            "11/11 - 1s - 120ms/step - loss: 6.0314e-04 - val_loss: 4.7344e-04\n",
            "Epoch 81/200\n",
            "11/11 - 1s - 83ms/step - loss: 6.4554e-04 - val_loss: 0.0015\n",
            "Epoch 82/200\n",
            "11/11 - 0s - 45ms/step - loss: 6.3305e-04 - val_loss: 6.7776e-04\n",
            "Epoch 83/200\n",
            "11/11 - 1s - 58ms/step - loss: 5.9119e-04 - val_loss: 4.6632e-04\n",
            "Epoch 84/200\n",
            "11/11 - 1s - 56ms/step - loss: 5.9190e-04 - val_loss: 4.7282e-04\n",
            "Epoch 85/200\n",
            "11/11 - 1s - 57ms/step - loss: 7.0184e-04 - val_loss: 0.0015\n",
            "Epoch 86/200\n",
            "11/11 - 1s - 57ms/step - loss: 6.9520e-04 - val_loss: 7.8970e-04\n",
            "Epoch 87/200\n",
            "11/11 - 0s - 45ms/step - loss: 6.1622e-04 - val_loss: 5.0917e-04\n",
            "Epoch 88/200\n",
            "11/11 - 1s - 47ms/step - loss: 7.2798e-04 - val_loss: 4.5797e-04\n",
            "Epoch 89/200\n",
            "11/11 - 1s - 46ms/step - loss: 6.0137e-04 - val_loss: 9.7253e-04\n",
            "Epoch 90/200\n",
            "11/11 - 1s - 47ms/step - loss: 6.4802e-04 - val_loss: 8.6981e-04\n",
            "Epoch 91/200\n",
            "11/11 - 1s - 56ms/step - loss: 6.4855e-04 - val_loss: 5.7160e-04\n",
            "Epoch 92/200\n",
            "11/11 - 1s - 48ms/step - loss: 6.2345e-04 - val_loss: 4.5077e-04\n",
            "Epoch 93/200\n",
            "11/11 - 1s - 47ms/step - loss: 5.9852e-04 - val_loss: 5.3104e-04\n",
            "Epoch 94/200\n",
            "11/11 - 1s - 47ms/step - loss: 5.6978e-04 - val_loss: 5.2661e-04\n",
            "Epoch 95/200\n",
            "11/11 - 1s - 55ms/step - loss: 6.1429e-04 - val_loss: 8.9948e-04\n",
            "Epoch 96/200\n",
            "11/11 - 1s - 47ms/step - loss: 5.5580e-04 - val_loss: 5.1296e-04\n",
            "Epoch 97/200\n",
            "11/11 - 1s - 55ms/step - loss: 5.8191e-04 - val_loss: 5.4801e-04\n",
            "Epoch 98/200\n",
            "11/11 - 1s - 71ms/step - loss: 5.9628e-04 - val_loss: 6.4840e-04\n",
            "Epoch 99/200\n",
            "11/11 - 1s - 130ms/step - loss: 6.1510e-04 - val_loss: 4.9259e-04\n",
            "Epoch 100/200\n",
            "11/11 - 1s - 78ms/step - loss: 6.4354e-04 - val_loss: 5.0932e-04\n",
            "Epoch 101/200\n",
            "11/11 - 1s - 83ms/step - loss: 6.0388e-04 - val_loss: 5.4634e-04\n",
            "Epoch 102/200\n",
            "11/11 - 1s - 47ms/step - loss: 5.6839e-04 - val_loss: 7.4597e-04\n",
            "Epoch 103/200\n",
            "11/11 - 1s - 56ms/step - loss: 5.7600e-04 - val_loss: 5.0939e-04\n",
            "Epoch 104/200\n",
            "11/11 - 1s - 56ms/step - loss: 6.9339e-04 - val_loss: 4.1995e-04\n",
            "Epoch 105/200\n",
            "11/11 - 1s - 58ms/step - loss: 5.9124e-04 - val_loss: 4.8722e-04\n",
            "Epoch 106/200\n",
            "11/11 - 1s - 55ms/step - loss: 5.7629e-04 - val_loss: 6.4234e-04\n",
            "Epoch 107/200\n",
            "11/11 - 1s - 47ms/step - loss: 5.8314e-04 - val_loss: 6.0716e-04\n",
            "Epoch 108/200\n",
            "11/11 - 0s - 45ms/step - loss: 5.8212e-04 - val_loss: 4.1232e-04\n",
            "Epoch 109/200\n",
            "11/11 - 1s - 58ms/step - loss: 5.8771e-04 - val_loss: 6.0627e-04\n",
            "Epoch 110/200\n",
            "11/11 - 0s - 45ms/step - loss: 5.6300e-04 - val_loss: 8.4962e-04\n",
            "Epoch 111/200\n",
            "11/11 - 1s - 57ms/step - loss: 5.5930e-04 - val_loss: 4.2864e-04\n",
            "Epoch 112/200\n",
            "11/11 - 1s - 47ms/step - loss: 5.5420e-04 - val_loss: 8.1857e-04\n",
            "Epoch 113/200\n",
            "11/11 - 1s - 55ms/step - loss: 5.4596e-04 - val_loss: 4.9370e-04\n",
            "Epoch 114/200\n",
            "11/11 - 1s - 48ms/step - loss: 5.5066e-04 - val_loss: 6.8412e-04\n",
            "Epoch 115/200\n",
            "11/11 - 0s - 45ms/step - loss: 5.6489e-04 - val_loss: 5.3396e-04\n",
            "Epoch 116/200\n",
            "11/11 - 1s - 58ms/step - loss: 5.3221e-04 - val_loss: 5.0751e-04\n",
            "Epoch 117/200\n",
            "11/11 - 1s - 54ms/step - loss: 5.5426e-04 - val_loss: 8.0288e-04\n",
            "Epoch 118/200\n",
            "11/11 - 1s - 63ms/step - loss: 5.5720e-04 - val_loss: 5.3973e-04\n",
            "Epoch 119/200\n",
            "11/11 - 1s - 73ms/step - loss: 5.3748e-04 - val_loss: 7.0502e-04\n",
            "Epoch 120/200\n",
            "11/11 - 1s - 76ms/step - loss: 5.7161e-04 - val_loss: 5.3968e-04\n",
            "Epoch 121/200\n",
            "11/11 - 1s - 104ms/step - loss: 5.6725e-04 - val_loss: 5.1218e-04\n",
            "Epoch 122/200\n",
            "11/11 - 1s - 98ms/step - loss: 6.0336e-04 - val_loss: 4.9552e-04\n",
            "Epoch 123/200\n",
            "11/11 - 1s - 56ms/step - loss: 5.9345e-04 - val_loss: 0.0012\n",
            "Epoch 124/200\n",
            "11/11 - 1s - 57ms/step - loss: 6.5624e-04 - val_loss: 5.2438e-04\n",
            "Epoch 125/200\n",
            "11/11 - 1s - 56ms/step - loss: 5.5630e-04 - val_loss: 4.1518e-04\n",
            "Epoch 126/200\n",
            "11/11 - 1s - 55ms/step - loss: 5.4965e-04 - val_loss: 5.4813e-04\n",
            "Epoch 127/200\n",
            "11/11 - 1s - 46ms/step - loss: 5.7439e-04 - val_loss: 8.0984e-04\n",
            "Epoch 128/200\n",
            "11/11 - 0s - 45ms/step - loss: 5.6856e-04 - val_loss: 7.4825e-04\n",
            "Epoch 129/200\n",
            "11/11 - 1s - 47ms/step - loss: 5.8071e-04 - val_loss: 4.6993e-04\n",
            "Epoch 130/200\n",
            "11/11 - 1s - 46ms/step - loss: 5.8422e-04 - val_loss: 4.7524e-04\n",
            "Epoch 131/200\n",
            "11/11 - 1s - 47ms/step - loss: 5.5370e-04 - val_loss: 3.8836e-04\n",
            "Epoch 132/200\n",
            "11/11 - 1s - 48ms/step - loss: 5.5606e-04 - val_loss: 6.9527e-04\n",
            "Epoch 133/200\n",
            "11/11 - 1s - 55ms/step - loss: 5.5250e-04 - val_loss: 3.9418e-04\n",
            "Epoch 134/200\n",
            "11/11 - 1s - 46ms/step - loss: 5.5242e-04 - val_loss: 3.7678e-04\n",
            "Epoch 135/200\n",
            "11/11 - 1s - 46ms/step - loss: 5.1970e-04 - val_loss: 6.7719e-04\n",
            "Epoch 136/200\n",
            "11/11 - 0s - 45ms/step - loss: 5.7902e-04 - val_loss: 5.3200e-04\n",
            "Epoch 137/200\n",
            "11/11 - 1s - 47ms/step - loss: 5.4018e-04 - val_loss: 3.7588e-04\n",
            "Epoch 138/200\n",
            "11/11 - 1s - 69ms/step - loss: 5.2739e-04 - val_loss: 3.7455e-04\n",
            "Epoch 139/200\n",
            "11/11 - 1s - 131ms/step - loss: 5.2825e-04 - val_loss: 3.8628e-04\n",
            "Epoch 140/200\n",
            "11/11 - 1s - 77ms/step - loss: 5.1586e-04 - val_loss: 4.2026e-04\n",
            "Epoch 141/200\n",
            "11/11 - 1s - 75ms/step - loss: 5.0248e-04 - val_loss: 4.3806e-04\n",
            "Epoch 142/200\n",
            "11/11 - 1s - 88ms/step - loss: 5.2324e-04 - val_loss: 4.9972e-04\n",
            "Epoch 143/200\n",
            "11/11 - 1s - 47ms/step - loss: 5.3239e-04 - val_loss: 4.2021e-04\n",
            "Epoch 144/200\n",
            "11/11 - 1s - 46ms/step - loss: 5.3756e-04 - val_loss: 3.9149e-04\n",
            "Epoch 145/200\n",
            "11/11 - 1s - 47ms/step - loss: 5.4355e-04 - val_loss: 4.8830e-04\n",
            "Epoch 146/200\n",
            "11/11 - 1s - 55ms/step - loss: 5.3657e-04 - val_loss: 6.4746e-04\n",
            "Epoch 147/200\n",
            "11/11 - 1s - 46ms/step - loss: 5.3953e-04 - val_loss: 3.5525e-04\n",
            "Epoch 148/200\n",
            "11/11 - 1s - 46ms/step - loss: 5.0816e-04 - val_loss: 4.4736e-04\n",
            "Epoch 149/200\n",
            "11/11 - 1s - 57ms/step - loss: 5.0703e-04 - val_loss: 4.3832e-04\n",
            "Epoch 150/200\n",
            "11/11 - 1s - 46ms/step - loss: 5.2300e-04 - val_loss: 5.5321e-04\n",
            "Epoch 151/200\n",
            "11/11 - 0s - 45ms/step - loss: 6.0972e-04 - val_loss: 3.6197e-04\n",
            "Epoch 152/200\n",
            "11/11 - 0s - 45ms/step - loss: 5.5018e-04 - val_loss: 3.6810e-04\n",
            "Epoch 153/200\n",
            "11/11 - 1s - 56ms/step - loss: 5.2319e-04 - val_loss: 8.9322e-04\n",
            "Epoch 154/200\n",
            "11/11 - 1s - 59ms/step - loss: 5.7334e-04 - val_loss: 4.0593e-04\n",
            "Epoch 155/200\n",
            "11/11 - 1s - 53ms/step - loss: 5.4706e-04 - val_loss: 3.4495e-04\n",
            "Epoch 156/200\n",
            "11/11 - 1s - 59ms/step - loss: 5.4928e-04 - val_loss: 6.1429e-04\n",
            "Epoch 157/200\n",
            "11/11 - 1s - 46ms/step - loss: 5.3025e-04 - val_loss: 3.4292e-04\n",
            "Epoch 158/200\n",
            "11/11 - 1s - 48ms/step - loss: 5.0963e-04 - val_loss: 3.5726e-04\n",
            "Epoch 159/200\n",
            "11/11 - 1s - 84ms/step - loss: 5.0834e-04 - val_loss: 3.5153e-04\n",
            "Epoch 160/200\n",
            "11/11 - 1s - 74ms/step - loss: 4.9561e-04 - val_loss: 4.9269e-04\n",
            "Epoch 161/200\n",
            "11/11 - 1s - 124ms/step - loss: 4.6009e-04 - val_loss: 4.5864e-04\n",
            "Epoch 162/200\n",
            "11/11 - 1s - 47ms/step - loss: 5.0546e-04 - val_loss: 7.0395e-04\n",
            "Epoch 163/200\n",
            "11/11 - 1s - 46ms/step - loss: 5.2097e-04 - val_loss: 4.2458e-04\n",
            "Epoch 164/200\n",
            "11/11 - 1s - 56ms/step - loss: 6.1972e-04 - val_loss: 3.5532e-04\n",
            "Epoch 165/200\n",
            "11/11 - 1s - 47ms/step - loss: 5.2020e-04 - val_loss: 8.1663e-04\n",
            "Epoch 166/200\n",
            "11/11 - 1s - 56ms/step - loss: 5.2860e-04 - val_loss: 3.4929e-04\n",
            "Epoch 167/200\n",
            "11/11 - 1s - 55ms/step - loss: 4.9638e-04 - val_loss: 4.3548e-04\n",
            "Epoch 168/200\n",
            "11/11 - 1s - 58ms/step - loss: 4.9187e-04 - val_loss: 3.9754e-04\n",
            "Epoch 169/200\n",
            "11/11 - 1s - 55ms/step - loss: 4.9867e-04 - val_loss: 3.2593e-04\n",
            "Epoch 170/200\n",
            "11/11 - 1s - 47ms/step - loss: 5.4279e-04 - val_loss: 4.9036e-04\n",
            "Epoch 171/200\n",
            "11/11 - 1s - 46ms/step - loss: 5.2522e-04 - val_loss: 4.0731e-04\n",
            "Epoch 172/200\n",
            "11/11 - 1s - 47ms/step - loss: 4.8701e-04 - val_loss: 3.5754e-04\n",
            "Epoch 173/200\n",
            "11/11 - 0s - 45ms/step - loss: 4.8643e-04 - val_loss: 3.7109e-04\n",
            "Epoch 174/200\n",
            "11/11 - 1s - 47ms/step - loss: 4.9497e-04 - val_loss: 3.4051e-04\n",
            "Epoch 175/200\n",
            "11/11 - 1s - 46ms/step - loss: 5.2341e-04 - val_loss: 4.7415e-04\n",
            "Epoch 176/200\n",
            "11/11 - 1s - 47ms/step - loss: 5.0124e-04 - val_loss: 6.1415e-04\n",
            "Epoch 177/200\n",
            "11/11 - 1s - 46ms/step - loss: 4.4582e-04 - val_loss: 3.2630e-04\n",
            "Epoch 178/200\n",
            "11/11 - 1s - 46ms/step - loss: 4.7499e-04 - val_loss: 3.2182e-04\n",
            "Epoch 179/200\n",
            "11/11 - 1s - 46ms/step - loss: 5.4522e-04 - val_loss: 6.8974e-04\n",
            "Epoch 180/200\n",
            "11/11 - 1s - 65ms/step - loss: 5.1853e-04 - val_loss: 3.1754e-04\n",
            "Epoch 181/200\n",
            "11/11 - 1s - 75ms/step - loss: 4.9607e-04 - val_loss: 3.1830e-04\n",
            "Epoch 182/200\n",
            "11/11 - 1s - 117ms/step - loss: 4.9450e-04 - val_loss: 4.2077e-04\n",
            "Epoch 183/200\n",
            "11/11 - 1s - 78ms/step - loss: 4.9022e-04 - val_loss: 3.0983e-04\n",
            "Epoch 184/200\n",
            "11/11 - 1s - 46ms/step - loss: 5.4175e-04 - val_loss: 3.2403e-04\n",
            "Epoch 185/200\n",
            "11/11 - 1s - 59ms/step - loss: 4.9419e-04 - val_loss: 4.6558e-04\n",
            "Epoch 186/200\n",
            "11/11 - 1s - 55ms/step - loss: 4.6064e-04 - val_loss: 3.2092e-04\n",
            "Epoch 187/200\n",
            "11/11 - 1s - 57ms/step - loss: 4.6056e-04 - val_loss: 3.3890e-04\n",
            "Epoch 188/200\n",
            "11/11 - 1s - 55ms/step - loss: 5.3620e-04 - val_loss: 6.2040e-04\n",
            "Epoch 189/200\n",
            "11/11 - 1s - 55ms/step - loss: 4.9091e-04 - val_loss: 3.9796e-04\n",
            "Epoch 190/200\n",
            "11/11 - 1s - 47ms/step - loss: 4.8534e-04 - val_loss: 4.8792e-04\n",
            "Epoch 191/200\n",
            "11/11 - 1s - 46ms/step - loss: 5.0480e-04 - val_loss: 4.9686e-04\n",
            "Epoch 192/200\n",
            "11/11 - 1s - 47ms/step - loss: 5.1762e-04 - val_loss: 3.1308e-04\n",
            "Epoch 193/200\n",
            "11/11 - 1s - 47ms/step - loss: 5.3865e-04 - val_loss: 3.7494e-04\n",
            "Epoch 194/200\n",
            "11/11 - 1s - 57ms/step - loss: 5.2509e-04 - val_loss: 7.7901e-04\n",
            "Epoch 195/200\n",
            "11/11 - 1s - 45ms/step - loss: 5.2814e-04 - val_loss: 3.3039e-04\n",
            "Epoch 196/200\n",
            "11/11 - 1s - 56ms/step - loss: 5.3506e-04 - val_loss: 6.1112e-04\n",
            "Epoch 197/200\n",
            "11/11 - 1s - 58ms/step - loss: 5.0885e-04 - val_loss: 3.2571e-04\n",
            "Epoch 198/200\n",
            "11/11 - 0s - 45ms/step - loss: 4.7117e-04 - val_loss: 4.8143e-04\n",
            "Epoch 199/200\n",
            "11/11 - 1s - 47ms/step - loss: 4.8724e-04 - val_loss: 5.5363e-04\n",
            "Epoch 200/200\n",
            "11/11 - 1s - 56ms/step - loss: 4.9036e-04 - val_loss: 3.1739e-04\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 210ms/step\n",
            "Training with Epochs: 200, Batch Size: 64\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 - 5s - 866ms/step - loss: 0.0549 - val_loss: 0.0186\n",
            "Epoch 2/200\n",
            "6/6 - 1s - 90ms/step - loss: 0.0088 - val_loss: 0.0292\n",
            "Epoch 3/200\n",
            "6/6 - 1s - 102ms/step - loss: 0.0081 - val_loss: 0.0082\n",
            "Epoch 4/200\n",
            "6/6 - 1s - 105ms/step - loss: 0.0038 - val_loss: 0.0030\n",
            "Epoch 5/200\n",
            "6/6 - 1s - 85ms/step - loss: 0.0032 - val_loss: 0.0096\n",
            "Epoch 6/200\n",
            "6/6 - 1s - 106ms/step - loss: 0.0031 - val_loss: 0.0059\n",
            "Epoch 7/200\n",
            "6/6 - 1s - 89ms/step - loss: 0.0025 - val_loss: 0.0031\n",
            "Epoch 8/200\n",
            "6/6 - 1s - 107ms/step - loss: 0.0023 - val_loss: 0.0053\n",
            "Epoch 9/200\n",
            "6/6 - 1s - 88ms/step - loss: 0.0022 - val_loss: 0.0034\n",
            "Epoch 10/200\n",
            "6/6 - 1s - 102ms/step - loss: 0.0021 - val_loss: 0.0034\n",
            "Epoch 11/200\n",
            "6/6 - 1s - 105ms/step - loss: 0.0020 - val_loss: 0.0038\n",
            "Epoch 12/200\n",
            "6/6 - 1s - 161ms/step - loss: 0.0020 - val_loss: 0.0031\n",
            "Epoch 13/200\n",
            "6/6 - 1s - 166ms/step - loss: 0.0020 - val_loss: 0.0031\n",
            "Epoch 14/200\n",
            "6/6 - 1s - 154ms/step - loss: 0.0019 - val_loss: 0.0032\n",
            "Epoch 15/200\n",
            "6/6 - 1s - 176ms/step - loss: 0.0019 - val_loss: 0.0031\n",
            "Epoch 16/200\n",
            "6/6 - 1s - 132ms/step - loss: 0.0019 - val_loss: 0.0031\n",
            "Epoch 17/200\n",
            "6/6 - 1s - 99ms/step - loss: 0.0018 - val_loss: 0.0032\n",
            "Epoch 18/200\n",
            "6/6 - 1s - 98ms/step - loss: 0.0018 - val_loss: 0.0029\n",
            "Epoch 19/200\n",
            "6/6 - 1s - 109ms/step - loss: 0.0018 - val_loss: 0.0032\n",
            "Epoch 20/200\n",
            "6/6 - 1s - 101ms/step - loss: 0.0018 - val_loss: 0.0029\n",
            "Epoch 21/200\n",
            "6/6 - 1s - 99ms/step - loss: 0.0017 - val_loss: 0.0031\n",
            "Epoch 22/200\n",
            "6/6 - 1s - 106ms/step - loss: 0.0017 - val_loss: 0.0027\n",
            "Epoch 23/200\n",
            "6/6 - 1s - 88ms/step - loss: 0.0017 - val_loss: 0.0031\n",
            "Epoch 24/200\n",
            "6/6 - 1s - 104ms/step - loss: 0.0016 - val_loss: 0.0022\n",
            "Epoch 25/200\n",
            "6/6 - 1s - 85ms/step - loss: 0.0017 - val_loss: 0.0034\n",
            "Epoch 26/200\n",
            "6/6 - 1s - 88ms/step - loss: 0.0016 - val_loss: 0.0022\n",
            "Epoch 27/200\n",
            "6/6 - 1s - 105ms/step - loss: 0.0015 - val_loss: 0.0026\n",
            "Epoch 28/200\n",
            "6/6 - 1s - 101ms/step - loss: 0.0014 - val_loss: 0.0026\n",
            "Epoch 29/200\n",
            "6/6 - 1s - 90ms/step - loss: 0.0016 - val_loss: 0.0023\n",
            "Epoch 30/200\n",
            "6/6 - 1s - 98ms/step - loss: 0.0015 - val_loss: 0.0027\n",
            "Epoch 31/200\n",
            "6/6 - 1s - 89ms/step - loss: 0.0014 - val_loss: 0.0018\n",
            "Epoch 32/200\n",
            "6/6 - 1s - 86ms/step - loss: 0.0015 - val_loss: 0.0023\n",
            "Epoch 33/200\n",
            "6/6 - 1s - 174ms/step - loss: 0.0015 - val_loss: 0.0032\n",
            "Epoch 34/200\n",
            "6/6 - 1s - 209ms/step - loss: 0.0016 - val_loss: 0.0017\n",
            "Epoch 35/200\n",
            "6/6 - 1s - 182ms/step - loss: 0.0015 - val_loss: 0.0032\n",
            "Epoch 36/200\n",
            "6/6 - 1s - 90ms/step - loss: 0.0015 - val_loss: 0.0022\n",
            "Epoch 37/200\n",
            "6/6 - 1s - 91ms/step - loss: 0.0014 - val_loss: 0.0024\n",
            "Epoch 38/200\n",
            "6/6 - 1s - 99ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "Epoch 39/200\n",
            "6/6 - 1s - 90ms/step - loss: 0.0013 - val_loss: 0.0024\n",
            "Epoch 40/200\n",
            "6/6 - 1s - 100ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "Epoch 41/200\n",
            "6/6 - 1s - 107ms/step - loss: 0.0012 - val_loss: 0.0017\n",
            "Epoch 42/200\n",
            "6/6 - 1s - 87ms/step - loss: 0.0013 - val_loss: 0.0023\n",
            "Epoch 43/200\n",
            "6/6 - 1s - 89ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 44/200\n",
            "6/6 - 1s - 102ms/step - loss: 0.0013 - val_loss: 0.0021\n",
            "Epoch 45/200\n",
            "6/6 - 1s - 101ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "Epoch 46/200\n",
            "6/6 - 1s - 88ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 47/200\n",
            "6/6 - 1s - 103ms/step - loss: 0.0011 - val_loss: 0.0017\n",
            "Epoch 48/200\n",
            "6/6 - 1s - 88ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 49/200\n",
            "6/6 - 1s - 86ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 50/200\n",
            "6/6 - 1s - 104ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 51/200\n",
            "6/6 - 1s - 87ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 52/200\n",
            "6/6 - 1s - 111ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 53/200\n",
            "6/6 - 1s - 163ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 54/200\n",
            "6/6 - 1s - 171ms/step - loss: 0.0011 - val_loss: 9.3857e-04\n",
            "Epoch 55/200\n",
            "6/6 - 1s - 218ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 56/200\n",
            "6/6 - 1s - 121ms/step - loss: 0.0011 - val_loss: 0.0022\n",
            "Epoch 57/200\n",
            "6/6 - 1s - 89ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 58/200\n",
            "6/6 - 1s - 94ms/step - loss: 9.9981e-04 - val_loss: 0.0010\n",
            "Epoch 59/200\n",
            "6/6 - 1s - 98ms/step - loss: 0.0010 - val_loss: 0.0016\n",
            "Epoch 60/200\n",
            "6/6 - 1s - 101ms/step - loss: 9.9631e-04 - val_loss: 0.0011\n",
            "Epoch 61/200\n",
            "6/6 - 1s - 109ms/step - loss: 9.5438e-04 - val_loss: 9.0947e-04\n",
            "Epoch 62/200\n",
            "6/6 - 1s - 105ms/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 63/200\n",
            "6/6 - 1s - 102ms/step - loss: 8.9360e-04 - val_loss: 9.4385e-04\n",
            "Epoch 64/200\n",
            "6/6 - 1s - 87ms/step - loss: 9.4086e-04 - val_loss: 0.0010\n",
            "Epoch 65/200\n",
            "6/6 - 1s - 105ms/step - loss: 9.2168e-04 - val_loss: 0.0013\n",
            "Epoch 66/200\n",
            "6/6 - 1s - 89ms/step - loss: 9.2730e-04 - val_loss: 0.0011\n",
            "Epoch 67/200\n",
            "6/6 - 1s - 101ms/step - loss: 9.0860e-04 - val_loss: 7.8416e-04\n",
            "Epoch 68/200\n",
            "6/6 - 1s - 89ms/step - loss: 9.7731e-04 - val_loss: 0.0014\n",
            "Epoch 69/200\n",
            "6/6 - 1s - 87ms/step - loss: 9.2539e-04 - val_loss: 0.0012\n",
            "Epoch 70/200\n",
            "6/6 - 1s - 87ms/step - loss: 8.6898e-04 - val_loss: 8.1609e-04\n",
            "Epoch 71/200\n",
            "6/6 - 1s - 90ms/step - loss: 9.3607e-04 - val_loss: 0.0017\n",
            "Epoch 72/200\n",
            "6/6 - 1s - 103ms/step - loss: 9.0822e-04 - val_loss: 0.0011\n",
            "Epoch 73/200\n",
            "6/6 - 1s - 97ms/step - loss: 8.7432e-04 - val_loss: 7.6414e-04\n",
            "Epoch 74/200\n",
            "6/6 - 1s - 162ms/step - loss: 9.0383e-04 - val_loss: 0.0011\n",
            "Epoch 75/200\n",
            "6/6 - 1s - 209ms/step - loss: 8.4123e-04 - val_loss: 0.0010\n",
            "Epoch 76/200\n",
            "6/6 - 1s - 166ms/step - loss: 8.5633e-04 - val_loss: 8.2099e-04\n",
            "Epoch 77/200\n",
            "6/6 - 1s - 183ms/step - loss: 8.4747e-04 - val_loss: 0.0011\n",
            "Epoch 78/200\n",
            "6/6 - 1s - 100ms/step - loss: 8.3685e-04 - val_loss: 9.4189e-04\n",
            "Epoch 79/200\n",
            "6/6 - 1s - 90ms/step - loss: 7.8562e-04 - val_loss: 8.6831e-04\n",
            "Epoch 80/200\n",
            "6/6 - 1s - 88ms/step - loss: 8.1488e-04 - val_loss: 8.1866e-04\n",
            "Epoch 81/200\n",
            "6/6 - 1s - 86ms/step - loss: 8.0111e-04 - val_loss: 8.2237e-04\n",
            "Epoch 82/200\n",
            "6/6 - 1s - 104ms/step - loss: 7.7662e-04 - val_loss: 0.0010\n",
            "Epoch 83/200\n",
            "6/6 - 1s - 104ms/step - loss: 8.2512e-04 - val_loss: 6.9369e-04\n",
            "Epoch 84/200\n",
            "6/6 - 1s - 104ms/step - loss: 8.3758e-04 - val_loss: 8.0363e-04\n",
            "Epoch 85/200\n",
            "6/6 - 1s - 87ms/step - loss: 7.6565e-04 - val_loss: 9.8216e-04\n",
            "Epoch 86/200\n",
            "6/6 - 1s - 91ms/step - loss: 7.8735e-04 - val_loss: 0.0013\n",
            "Epoch 87/200\n",
            "6/6 - 1s - 88ms/step - loss: 8.1174e-04 - val_loss: 7.7953e-04\n",
            "Epoch 88/200\n",
            "6/6 - 1s - 104ms/step - loss: 8.1453e-04 - val_loss: 7.1041e-04\n",
            "Epoch 89/200\n",
            "6/6 - 1s - 104ms/step - loss: 7.9007e-04 - val_loss: 8.6175e-04\n",
            "Epoch 90/200\n",
            "6/6 - 1s - 102ms/step - loss: 7.4039e-04 - val_loss: 7.4306e-04\n",
            "Epoch 91/200\n",
            "6/6 - 1s - 105ms/step - loss: 7.0062e-04 - val_loss: 6.6871e-04\n",
            "Epoch 92/200\n",
            "6/6 - 1s - 137ms/step - loss: 7.5543e-04 - val_loss: 7.1285e-04\n",
            "Epoch 93/200\n",
            "6/6 - 1s - 164ms/step - loss: 7.9799e-04 - val_loss: 9.2091e-04\n",
            "Epoch 94/200\n",
            "6/6 - 1s - 215ms/step - loss: 7.2706e-04 - val_loss: 0.0010\n",
            "Epoch 95/200\n",
            "6/6 - 1s - 146ms/step - loss: 7.7924e-04 - val_loss: 0.0012\n",
            "Epoch 96/200\n",
            "6/6 - 1s - 92ms/step - loss: 7.8813e-04 - val_loss: 7.6161e-04\n",
            "Epoch 97/200\n",
            "6/6 - 1s - 88ms/step - loss: 7.0514e-04 - val_loss: 6.2315e-04\n",
            "Epoch 98/200\n",
            "6/6 - 1s - 106ms/step - loss: 7.5239e-04 - val_loss: 8.1894e-04\n",
            "Epoch 99/200\n",
            "6/6 - 1s - 90ms/step - loss: 7.6616e-04 - val_loss: 0.0013\n",
            "Epoch 100/200\n",
            "6/6 - 1s - 85ms/step - loss: 7.7676e-04 - val_loss: 6.1071e-04\n",
            "Epoch 101/200\n",
            "6/6 - 1s - 105ms/step - loss: 7.5730e-04 - val_loss: 8.0347e-04\n",
            "Epoch 102/200\n",
            "6/6 - 1s - 86ms/step - loss: 6.9225e-04 - val_loss: 8.8873e-04\n",
            "Epoch 103/200\n",
            "6/6 - 1s - 88ms/step - loss: 6.7857e-04 - val_loss: 8.1931e-04\n",
            "Epoch 104/200\n",
            "6/6 - 1s - 103ms/step - loss: 7.1016e-04 - val_loss: 9.4143e-04\n",
            "Epoch 105/200\n",
            "6/6 - 1s - 108ms/step - loss: 7.4753e-04 - val_loss: 0.0011\n",
            "Epoch 106/200\n",
            "6/6 - 1s - 84ms/step - loss: 8.2412e-04 - val_loss: 5.8552e-04\n",
            "Epoch 107/200\n",
            "6/6 - 1s - 89ms/step - loss: 7.3110e-04 - val_loss: 6.1189e-04\n",
            "Epoch 108/200\n",
            "6/6 - 1s - 93ms/step - loss: 7.6100e-04 - val_loss: 0.0012\n",
            "Epoch 109/200\n",
            "6/6 - 1s - 88ms/step - loss: 7.3448e-04 - val_loss: 7.5886e-04\n",
            "Epoch 110/200\n",
            "6/6 - 1s - 106ms/step - loss: 7.0390e-04 - val_loss: 5.7811e-04\n",
            "Epoch 111/200\n",
            "6/6 - 1s - 88ms/step - loss: 8.1999e-04 - val_loss: 8.8911e-04\n",
            "Epoch 112/200\n",
            "6/6 - 1s - 99ms/step - loss: 6.9501e-04 - val_loss: 8.4302e-04\n",
            "Epoch 113/200\n",
            "6/6 - 1s - 167ms/step - loss: 6.8051e-04 - val_loss: 9.6968e-04\n",
            "Epoch 114/200\n",
            "6/6 - 1s - 161ms/step - loss: 6.8301e-04 - val_loss: 6.4663e-04\n",
            "Epoch 115/200\n",
            "6/6 - 1s - 225ms/step - loss: 6.5696e-04 - val_loss: 5.8897e-04\n",
            "Epoch 116/200\n",
            "6/6 - 1s - 131ms/step - loss: 6.5901e-04 - val_loss: 0.0015\n",
            "Epoch 117/200\n",
            "6/6 - 1s - 87ms/step - loss: 7.8109e-04 - val_loss: 0.0012\n",
            "Epoch 118/200\n",
            "6/6 - 1s - 90ms/step - loss: 8.6028e-04 - val_loss: 5.6004e-04\n",
            "Epoch 119/200\n",
            "6/6 - 1s - 100ms/step - loss: 7.0760e-04 - val_loss: 6.3159e-04\n",
            "Epoch 120/200\n",
            "6/6 - 1s - 104ms/step - loss: 6.5674e-04 - val_loss: 7.3827e-04\n",
            "Epoch 121/200\n",
            "6/6 - 1s - 88ms/step - loss: 6.4080e-04 - val_loss: 7.4280e-04\n",
            "Epoch 122/200\n",
            "6/6 - 1s - 89ms/step - loss: 6.9829e-04 - val_loss: 5.4230e-04\n",
            "Epoch 123/200\n",
            "6/6 - 1s - 88ms/step - loss: 7.4864e-04 - val_loss: 5.6695e-04\n",
            "Epoch 124/200\n",
            "6/6 - 1s - 86ms/step - loss: 6.5450e-04 - val_loss: 7.8339e-04\n",
            "Epoch 125/200\n",
            "6/6 - 1s - 107ms/step - loss: 6.5729e-04 - val_loss: 8.6546e-04\n",
            "Epoch 126/200\n",
            "6/6 - 1s - 87ms/step - loss: 6.5226e-04 - val_loss: 5.4570e-04\n",
            "Epoch 127/200\n",
            "6/6 - 1s - 104ms/step - loss: 6.4850e-04 - val_loss: 5.8336e-04\n",
            "Epoch 128/200\n",
            "6/6 - 1s - 103ms/step - loss: 6.0182e-04 - val_loss: 7.0591e-04\n",
            "Epoch 129/200\n",
            "6/6 - 1s - 104ms/step - loss: 6.7948e-04 - val_loss: 6.1405e-04\n",
            "Epoch 130/200\n",
            "6/6 - 1s - 107ms/step - loss: 6.4651e-04 - val_loss: 5.4020e-04\n",
            "Epoch 131/200\n",
            "6/6 - 1s - 102ms/step - loss: 6.5654e-04 - val_loss: 6.5351e-04\n",
            "Epoch 132/200\n",
            "6/6 - 1s - 88ms/step - loss: 6.5876e-04 - val_loss: 9.8636e-04\n",
            "Epoch 133/200\n",
            "6/6 - 1s - 158ms/step - loss: 6.4947e-04 - val_loss: 5.3192e-04\n",
            "Epoch 134/200\n",
            "6/6 - 1s - 232ms/step - loss: 6.4491e-04 - val_loss: 7.1978e-04\n",
            "Epoch 135/200\n",
            "6/6 - 1s - 177ms/step - loss: 6.3844e-04 - val_loss: 8.5625e-04\n",
            "Epoch 136/200\n",
            "6/6 - 1s - 132ms/step - loss: 6.4239e-04 - val_loss: 0.0010\n",
            "Epoch 137/200\n",
            "6/6 - 1s - 89ms/step - loss: 6.8680e-04 - val_loss: 6.3903e-04\n",
            "Epoch 138/200\n",
            "6/6 - 1s - 90ms/step - loss: 5.9235e-04 - val_loss: 5.1491e-04\n",
            "Epoch 139/200\n",
            "6/6 - 1s - 99ms/step - loss: 6.2612e-04 - val_loss: 5.1274e-04\n",
            "Epoch 140/200\n",
            "6/6 - 1s - 89ms/step - loss: 6.3781e-04 - val_loss: 0.0010\n",
            "Epoch 141/200\n",
            "6/6 - 1s - 85ms/step - loss: 6.5688e-04 - val_loss: 6.5885e-04\n",
            "Epoch 142/200\n",
            "6/6 - 1s - 108ms/step - loss: 5.9258e-04 - val_loss: 5.6099e-04\n",
            "Epoch 143/200\n",
            "6/6 - 1s - 98ms/step - loss: 5.9833e-04 - val_loss: 6.0017e-04\n",
            "Epoch 144/200\n",
            "6/6 - 1s - 107ms/step - loss: 5.7614e-04 - val_loss: 5.1070e-04\n",
            "Epoch 145/200\n",
            "6/6 - 1s - 105ms/step - loss: 6.3314e-04 - val_loss: 5.1326e-04\n",
            "Epoch 146/200\n",
            "6/6 - 1s - 99ms/step - loss: 5.9276e-04 - val_loss: 7.6015e-04\n",
            "Epoch 147/200\n",
            "6/6 - 1s - 106ms/step - loss: 6.0069e-04 - val_loss: 4.9401e-04\n",
            "Epoch 148/200\n",
            "6/6 - 1s - 104ms/step - loss: 6.3419e-04 - val_loss: 5.4856e-04\n",
            "Epoch 149/200\n",
            "6/6 - 1s - 104ms/step - loss: 5.7661e-04 - val_loss: 6.6538e-04\n",
            "Epoch 150/200\n",
            "6/6 - 1s - 89ms/step - loss: 5.9303e-04 - val_loss: 7.3582e-04\n",
            "Epoch 151/200\n",
            "6/6 - 1s - 103ms/step - loss: 5.9996e-04 - val_loss: 5.1520e-04\n",
            "Epoch 152/200\n",
            "6/6 - 1s - 92ms/step - loss: 5.9603e-04 - val_loss: 5.7365e-04\n",
            "Epoch 153/200\n",
            "6/6 - 1s - 134ms/step - loss: 5.7534e-04 - val_loss: 7.5319e-04\n",
            "Epoch 154/200\n",
            "6/6 - 1s - 157ms/step - loss: 6.2293e-04 - val_loss: 4.8059e-04\n",
            "Epoch 155/200\n",
            "6/6 - 1s - 223ms/step - loss: 5.9208e-04 - val_loss: 8.0996e-04\n",
            "Epoch 156/200\n",
            "6/6 - 1s - 150ms/step - loss: 6.4506e-04 - val_loss: 6.3545e-04\n",
            "Epoch 157/200\n",
            "6/6 - 1s - 181ms/step - loss: 6.3491e-04 - val_loss: 5.6584e-04\n",
            "Epoch 158/200\n",
            "6/6 - 1s - 104ms/step - loss: 6.0738e-04 - val_loss: 5.0888e-04\n",
            "Epoch 159/200\n",
            "6/6 - 1s - 92ms/step - loss: 6.0571e-04 - val_loss: 4.8420e-04\n",
            "Epoch 160/200\n",
            "6/6 - 1s - 88ms/step - loss: 6.0723e-04 - val_loss: 0.0011\n",
            "Epoch 161/200\n",
            "6/6 - 1s - 106ms/step - loss: 7.1866e-04 - val_loss: 5.3733e-04\n",
            "Epoch 162/200\n",
            "6/6 - 1s - 102ms/step - loss: 7.1685e-04 - val_loss: 4.5834e-04\n",
            "Epoch 163/200\n",
            "6/6 - 1s - 101ms/step - loss: 6.5456e-04 - val_loss: 4.5680e-04\n",
            "Epoch 164/200\n",
            "6/6 - 1s - 90ms/step - loss: 6.5457e-04 - val_loss: 6.2458e-04\n",
            "Epoch 165/200\n",
            "6/6 - 1s - 88ms/step - loss: 6.1210e-04 - val_loss: 6.0897e-04\n",
            "Epoch 166/200\n",
            "6/6 - 1s - 91ms/step - loss: 5.9773e-04 - val_loss: 4.5629e-04\n",
            "Epoch 167/200\n",
            "6/6 - 1s - 85ms/step - loss: 6.0175e-04 - val_loss: 7.4677e-04\n",
            "Epoch 168/200\n",
            "6/6 - 1s - 90ms/step - loss: 6.0440e-04 - val_loss: 5.8999e-04\n",
            "Epoch 169/200\n",
            "6/6 - 1s - 102ms/step - loss: 5.9714e-04 - val_loss: 4.8229e-04\n",
            "Epoch 170/200\n",
            "6/6 - 1s - 103ms/step - loss: 6.1470e-04 - val_loss: 7.7444e-04\n",
            "Epoch 171/200\n",
            "6/6 - 1s - 87ms/step - loss: 6.1498e-04 - val_loss: 9.0799e-04\n",
            "Epoch 172/200\n",
            "6/6 - 1s - 108ms/step - loss: 6.3790e-04 - val_loss: 4.4961e-04\n",
            "Epoch 173/200\n",
            "6/6 - 1s - 164ms/step - loss: 6.0639e-04 - val_loss: 4.4610e-04\n",
            "Epoch 174/200\n",
            "6/6 - 1s - 176ms/step - loss: 6.4409e-04 - val_loss: 4.8022e-04\n",
            "Epoch 175/200\n",
            "6/6 - 1s - 172ms/step - loss: 6.4731e-04 - val_loss: 8.8558e-04\n",
            "Epoch 176/200\n",
            "6/6 - 1s - 138ms/step - loss: 5.8651e-04 - val_loss: 6.5173e-04\n",
            "Epoch 177/200\n",
            "6/6 - 1s - 94ms/step - loss: 6.0395e-04 - val_loss: 4.6351e-04\n",
            "Epoch 178/200\n",
            "6/6 - 1s - 104ms/step - loss: 5.8494e-04 - val_loss: 4.4852e-04\n",
            "Epoch 179/200\n",
            "6/6 - 1s - 88ms/step - loss: 6.0589e-04 - val_loss: 8.3751e-04\n",
            "Epoch 180/200\n",
            "6/6 - 1s - 88ms/step - loss: 6.3430e-04 - val_loss: 6.8457e-04\n",
            "Epoch 181/200\n",
            "6/6 - 1s - 106ms/step - loss: 5.6171e-04 - val_loss: 4.6834e-04\n",
            "Epoch 182/200\n",
            "6/6 - 1s - 99ms/step - loss: 5.9153e-04 - val_loss: 7.2631e-04\n",
            "Epoch 183/200\n",
            "6/6 - 1s - 109ms/step - loss: 5.7623e-04 - val_loss: 5.2209e-04\n",
            "Epoch 184/200\n",
            "6/6 - 1s - 102ms/step - loss: 5.6338e-04 - val_loss: 7.0562e-04\n",
            "Epoch 185/200\n",
            "6/6 - 1s - 91ms/step - loss: 5.8809e-04 - val_loss: 6.9071e-04\n",
            "Epoch 186/200\n",
            "6/6 - 1s - 88ms/step - loss: 6.0844e-04 - val_loss: 7.6414e-04\n",
            "Epoch 187/200\n",
            "6/6 - 1s - 91ms/step - loss: 6.0155e-04 - val_loss: 4.2582e-04\n",
            "Epoch 188/200\n",
            "6/6 - 1s - 86ms/step - loss: 6.0001e-04 - val_loss: 6.1780e-04\n",
            "Epoch 189/200\n",
            "6/6 - 1s - 88ms/step - loss: 5.7880e-04 - val_loss: 5.5778e-04\n",
            "Epoch 190/200\n",
            "6/6 - 1s - 91ms/step - loss: 5.6263e-04 - val_loss: 5.6844e-04\n",
            "Epoch 191/200\n",
            "6/6 - 1s - 100ms/step - loss: 5.6169e-04 - val_loss: 6.7943e-04\n",
            "Epoch 192/200\n",
            "6/6 - 1s - 91ms/step - loss: 5.5690e-04 - val_loss: 4.2170e-04\n",
            "Epoch 193/200\n",
            "6/6 - 1s - 130ms/step - loss: 5.7730e-04 - val_loss: 4.5558e-04\n",
            "Epoch 194/200\n",
            "6/6 - 1s - 163ms/step - loss: 5.8790e-04 - val_loss: 4.1847e-04\n",
            "Epoch 195/200\n",
            "6/6 - 1s - 214ms/step - loss: 5.8501e-04 - val_loss: 4.4682e-04\n",
            "Epoch 196/200\n",
            "6/6 - 1s - 163ms/step - loss: 5.9146e-04 - val_loss: 6.2190e-04\n",
            "Epoch 197/200\n",
            "6/6 - 1s - 136ms/step - loss: 5.5229e-04 - val_loss: 5.3456e-04\n",
            "Epoch 198/200\n",
            "6/6 - 1s - 90ms/step - loss: 5.8439e-04 - val_loss: 4.3953e-04\n",
            "Epoch 199/200\n",
            "6/6 - 1s - 91ms/step - loss: 5.7579e-04 - val_loss: 4.3340e-04\n",
            "Epoch 200/200\n",
            "6/6 - 1s - 91ms/step - loss: 5.6609e-04 - val_loss: 4.0945e-04\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 174ms/step\n",
            "Training with Epochs: 200, Batch Size: 128\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 - 4s - 1s/step - loss: 0.0959 - val_loss: 0.0652\n",
            "Epoch 2/200\n",
            "3/3 - 0s - 132ms/step - loss: 0.0126 - val_loss: 0.0218\n",
            "Epoch 3/200\n",
            "3/3 - 1s - 210ms/step - loss: 0.0167 - val_loss: 0.0048\n",
            "Epoch 4/200\n",
            "3/3 - 1s - 208ms/step - loss: 0.0042 - val_loss: 0.0272\n",
            "Epoch 5/200\n",
            "3/3 - 1s - 218ms/step - loss: 0.0089 - val_loss: 0.0256\n",
            "Epoch 6/200\n",
            "3/3 - 1s - 282ms/step - loss: 0.0064 - val_loss: 0.0101\n",
            "Epoch 7/200\n",
            "3/3 - 1s - 421ms/step - loss: 0.0032 - val_loss: 0.0029\n",
            "Epoch 8/200\n",
            "3/3 - 1s - 248ms/step - loss: 0.0047 - val_loss: 0.0028\n",
            "Epoch 9/200\n",
            "3/3 - 1s - 218ms/step - loss: 0.0039 - val_loss: 0.0051\n",
            "Epoch 10/200\n",
            "3/3 - 0s - 139ms/step - loss: 0.0028 - val_loss: 0.0101\n",
            "Epoch 11/200\n",
            "3/3 - 0s - 140ms/step - loss: 0.0032 - val_loss: 0.0101\n",
            "Epoch 12/200\n",
            "3/3 - 0s - 141ms/step - loss: 0.0029 - val_loss: 0.0061\n",
            "Epoch 13/200\n",
            "3/3 - 0s - 136ms/step - loss: 0.0024 - val_loss: 0.0035\n",
            "Epoch 14/200\n",
            "3/3 - 1s - 211ms/step - loss: 0.0025 - val_loss: 0.0030\n",
            "Epoch 15/200\n",
            "3/3 - 0s - 136ms/step - loss: 0.0023 - val_loss: 0.0041\n",
            "Epoch 16/200\n",
            "3/3 - 0s - 137ms/step - loss: 0.0020 - val_loss: 0.0054\n",
            "Epoch 17/200\n",
            "3/3 - 1s - 203ms/step - loss: 0.0021 - val_loss: 0.0051\n",
            "Epoch 18/200\n",
            "3/3 - 0s - 141ms/step - loss: 0.0020 - val_loss: 0.0036\n",
            "Epoch 19/200\n",
            "3/3 - 0s - 138ms/step - loss: 0.0019 - val_loss: 0.0028\n",
            "Epoch 20/200\n",
            "3/3 - 1s - 212ms/step - loss: 0.0020 - val_loss: 0.0030\n",
            "Epoch 21/200\n",
            "3/3 - 1s - 200ms/step - loss: 0.0019 - val_loss: 0.0038\n",
            "Epoch 22/200\n",
            "3/3 - 1s - 208ms/step - loss: 0.0019 - val_loss: 0.0036\n",
            "Epoch 23/200\n",
            "3/3 - 1s - 207ms/step - loss: 0.0018 - val_loss: 0.0029\n",
            "Epoch 24/200\n",
            "3/3 - 0s - 139ms/step - loss: 0.0019 - val_loss: 0.0027\n",
            "Epoch 25/200\n",
            "3/3 - 0s - 137ms/step - loss: 0.0018 - val_loss: 0.0030\n",
            "Epoch 26/200\n",
            "3/3 - 1s - 208ms/step - loss: 0.0018 - val_loss: 0.0032\n",
            "Epoch 27/200\n",
            "3/3 - 1s - 200ms/step - loss: 0.0019 - val_loss: 0.0030\n",
            "Epoch 28/200\n",
            "3/3 - 0s - 137ms/step - loss: 0.0017 - val_loss: 0.0028\n",
            "Epoch 29/200\n",
            "3/3 - 1s - 278ms/step - loss: 0.0018 - val_loss: 0.0027\n",
            "Epoch 30/200\n",
            "3/3 - 1s - 430ms/step - loss: 0.0018 - val_loss: 0.0030\n",
            "Epoch 31/200\n",
            "3/3 - 1s - 340ms/step - loss: 0.0018 - val_loss: 0.0031\n",
            "Epoch 32/200\n",
            "3/3 - 1s - 211ms/step - loss: 0.0017 - val_loss: 0.0030\n",
            "Epoch 33/200\n",
            "3/3 - 1s - 196ms/step - loss: 0.0018 - val_loss: 0.0027\n",
            "Epoch 34/200\n",
            "3/3 - 1s - 217ms/step - loss: 0.0017 - val_loss: 0.0031\n",
            "Epoch 35/200\n",
            "3/3 - 0s - 134ms/step - loss: 0.0017 - val_loss: 0.0031\n",
            "Epoch 36/200\n",
            "3/3 - 0s - 142ms/step - loss: 0.0017 - val_loss: 0.0027\n",
            "Epoch 37/200\n",
            "3/3 - 1s - 201ms/step - loss: 0.0017 - val_loss: 0.0026\n",
            "Epoch 38/200\n",
            "3/3 - 1s - 229ms/step - loss: 0.0017 - val_loss: 0.0030\n",
            "Epoch 39/200\n",
            "3/3 - 1s - 185ms/step - loss: 0.0017 - val_loss: 0.0030\n",
            "Epoch 40/200\n",
            "3/3 - 0s - 139ms/step - loss: 0.0017 - val_loss: 0.0026\n",
            "Epoch 41/200\n",
            "3/3 - 1s - 201ms/step - loss: 0.0017 - val_loss: 0.0026\n",
            "Epoch 42/200\n",
            "3/3 - 0s - 142ms/step - loss: 0.0017 - val_loss: 0.0028\n",
            "Epoch 43/200\n",
            "3/3 - 1s - 204ms/step - loss: 0.0016 - val_loss: 0.0028\n",
            "Epoch 44/200\n",
            "3/3 - 1s - 198ms/step - loss: 0.0016 - val_loss: 0.0025\n",
            "Epoch 45/200\n",
            "3/3 - 0s - 138ms/step - loss: 0.0016 - val_loss: 0.0027\n",
            "Epoch 46/200\n",
            "3/3 - 0s - 140ms/step - loss: 0.0016 - val_loss: 0.0030\n",
            "Epoch 47/200\n",
            "3/3 - 1s - 203ms/step - loss: 0.0016 - val_loss: 0.0026\n",
            "Epoch 48/200\n",
            "3/3 - 0s - 143ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 49/200\n",
            "3/3 - 0s - 134ms/step - loss: 0.0016 - val_loss: 0.0025\n",
            "Epoch 50/200\n",
            "3/3 - 1s - 312ms/step - loss: 0.0014 - val_loss: 0.0025\n",
            "Epoch 51/200\n",
            "3/3 - 1s - 228ms/step - loss: 0.0015 - val_loss: 0.0026\n",
            "Epoch 52/200\n",
            "3/3 - 1s - 456ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 53/200\n",
            "3/3 - 1s - 295ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 54/200\n",
            "3/3 - 0s - 133ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 55/200\n",
            "3/3 - 1s - 217ms/step - loss: 0.0015 - val_loss: 0.0023\n",
            "Epoch 56/200\n",
            "3/3 - 0s - 136ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "Epoch 57/200\n",
            "3/3 - 0s - 142ms/step - loss: 0.0015 - val_loss: 0.0027\n",
            "Epoch 58/200\n",
            "3/3 - 0s - 134ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "Epoch 59/200\n",
            "3/3 - 1s - 210ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "Epoch 60/200\n",
            "3/3 - 0s - 132ms/step - loss: 0.0014 - val_loss: 0.0025\n",
            "Epoch 61/200\n",
            "3/3 - 1s - 211ms/step - loss: 0.0015 - val_loss: 0.0024\n",
            "Epoch 62/200\n",
            "3/3 - 0s - 137ms/step - loss: 0.0014 - val_loss: 0.0023\n",
            "Epoch 63/200\n",
            "3/3 - 0s - 140ms/step - loss: 0.0014 - val_loss: 0.0024\n",
            "Epoch 64/200\n",
            "3/3 - 1s - 203ms/step - loss: 0.0014 - val_loss: 0.0023\n",
            "Epoch 65/200\n",
            "3/3 - 1s - 205ms/step - loss: 0.0014 - val_loss: 0.0023\n",
            "Epoch 66/200\n",
            "3/3 - 0s - 133ms/step - loss: 0.0015 - val_loss: 0.0025\n",
            "Epoch 67/200\n",
            "3/3 - 0s - 139ms/step - loss: 0.0014 - val_loss: 0.0022\n",
            "Epoch 68/200\n",
            "3/3 - 0s - 138ms/step - loss: 0.0014 - val_loss: 0.0020\n",
            "Epoch 69/200\n",
            "3/3 - 1s - 207ms/step - loss: 0.0013 - val_loss: 0.0023\n",
            "Epoch 70/200\n",
            "3/3 - 1s - 205ms/step - loss: 0.0013 - val_loss: 0.0024\n",
            "Epoch 71/200\n",
            "3/3 - 0s - 136ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "Epoch 72/200\n",
            "3/3 - 1s - 266ms/step - loss: 0.0014 - val_loss: 0.0019\n",
            "Epoch 73/200\n",
            "3/3 - 1s - 239ms/step - loss: 0.0014 - val_loss: 0.0024\n",
            "Epoch 74/200\n",
            "3/3 - 1s - 404ms/step - loss: 0.0013 - val_loss: 0.0021\n",
            "Epoch 75/200\n",
            "3/3 - 1s - 337ms/step - loss: 0.0013 - val_loss: 0.0021\n",
            "Epoch 76/200\n",
            "3/3 - 0s - 141ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "Epoch 77/200\n",
            "3/3 - 0s - 135ms/step - loss: 0.0013 - val_loss: 0.0021\n",
            "Epoch 78/200\n",
            "3/3 - 0s - 135ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "Epoch 79/200\n",
            "3/3 - 1s - 208ms/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 80/200\n",
            "3/3 - 1s - 209ms/step - loss: 0.0013 - val_loss: 0.0023\n",
            "Epoch 81/200\n",
            "3/3 - 1s - 199ms/step - loss: 0.0013 - val_loss: 0.0019\n",
            "Epoch 82/200\n",
            "3/3 - 0s - 143ms/step - loss: 0.0013 - val_loss: 0.0018\n",
            "Epoch 83/200\n",
            "3/3 - 0s - 136ms/step - loss: 0.0013 - val_loss: 0.0020\n",
            "Epoch 84/200\n",
            "3/3 - 1s - 208ms/step - loss: 0.0012 - val_loss: 0.0021\n",
            "Epoch 85/200\n",
            "3/3 - 0s - 137ms/step - loss: 0.0012 - val_loss: 0.0018\n",
            "Epoch 86/200\n",
            "3/3 - 1s - 212ms/step - loss: 0.0012 - val_loss: 0.0018\n",
            "Epoch 87/200\n",
            "3/3 - 1s - 200ms/step - loss: 0.0012 - val_loss: 0.0020\n",
            "Epoch 88/200\n",
            "3/3 - 1s - 211ms/step - loss: 0.0013 - val_loss: 0.0018\n",
            "Epoch 89/200\n",
            "3/3 - 1s - 207ms/step - loss: 0.0012 - val_loss: 0.0017\n",
            "Epoch 90/200\n",
            "3/3 - 0s - 140ms/step - loss: 0.0012 - val_loss: 0.0019\n",
            "Epoch 91/200\n",
            "3/3 - 0s - 144ms/step - loss: 0.0012 - val_loss: 0.0019\n",
            "Epoch 92/200\n",
            "3/3 - 0s - 138ms/step - loss: 0.0011 - val_loss: 0.0016\n",
            "Epoch 93/200\n",
            "3/3 - 1s - 200ms/step - loss: 0.0012 - val_loss: 0.0018\n",
            "Epoch 94/200\n",
            "3/3 - 1s - 237ms/step - loss: 0.0011 - val_loss: 0.0017\n",
            "Epoch 95/200\n",
            "3/3 - 1s - 252ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 96/200\n",
            "3/3 - 1s - 225ms/step - loss: 0.0011 - val_loss: 0.0018\n",
            "Epoch 97/200\n",
            "3/3 - 1s - 363ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 98/200\n",
            "3/3 - 1s - 178ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 99/200\n",
            "3/3 - 0s - 133ms/step - loss: 0.0011 - val_loss: 0.0018\n",
            "Epoch 100/200\n",
            "3/3 - 0s - 140ms/step - loss: 0.0012 - val_loss: 0.0017\n",
            "Epoch 101/200\n",
            "3/3 - 0s - 133ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 102/200\n",
            "3/3 - 0s - 138ms/step - loss: 0.0011 - val_loss: 0.0020\n",
            "Epoch 103/200\n",
            "3/3 - 0s - 137ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 104/200\n",
            "3/3 - 0s - 133ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 105/200\n",
            "3/3 - 1s - 213ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 106/200\n",
            "3/3 - 0s - 137ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 107/200\n",
            "3/3 - 0s - 137ms/step - loss: 0.0010 - val_loss: 0.0016\n",
            "Epoch 108/200\n",
            "3/3 - 1s - 202ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 109/200\n",
            "3/3 - 0s - 141ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 110/200\n",
            "3/3 - 0s - 136ms/step - loss: 0.0011 - val_loss: 0.0016\n",
            "Epoch 111/200\n",
            "3/3 - 1s - 211ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 112/200\n",
            "3/3 - 0s - 138ms/step - loss: 9.9893e-04 - val_loss: 0.0013\n",
            "Epoch 113/200\n",
            "3/3 - 1s - 211ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 114/200\n",
            "3/3 - 0s - 138ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 115/200\n",
            "3/3 - 0s - 140ms/step - loss: 0.0010 - val_loss: 0.0015\n",
            "Epoch 116/200\n",
            "3/3 - 0s - 137ms/step - loss: 9.9813e-04 - val_loss: 0.0014\n",
            "Epoch 117/200\n",
            "3/3 - 1s - 209ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 118/200\n",
            "3/3 - 1s - 286ms/step - loss: 9.9707e-04 - val_loss: 0.0013\n",
            "Epoch 119/200\n",
            "3/3 - 1s - 257ms/step - loss: 9.7496e-04 - val_loss: 0.0012\n",
            "Epoch 120/200\n",
            "3/3 - 1s - 225ms/step - loss: 9.5876e-04 - val_loss: 0.0015\n",
            "Epoch 121/200\n",
            "3/3 - 1s - 352ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 122/200\n",
            "3/3 - 1s - 190ms/step - loss: 9.6057e-04 - val_loss: 0.0012\n",
            "Epoch 123/200\n",
            "3/3 - 1s - 207ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 124/200\n",
            "3/3 - 0s - 136ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 125/200\n",
            "3/3 - 1s - 210ms/step - loss: 9.6843e-04 - val_loss: 0.0011\n",
            "Epoch 126/200\n",
            "3/3 - 0s - 136ms/step - loss: 9.7888e-04 - val_loss: 0.0014\n",
            "Epoch 127/200\n",
            "3/3 - 1s - 205ms/step - loss: 9.8183e-04 - val_loss: 0.0010\n",
            "Epoch 128/200\n",
            "3/3 - 0s - 133ms/step - loss: 9.7675e-04 - val_loss: 0.0013\n",
            "Epoch 129/200\n",
            "3/3 - 1s - 206ms/step - loss: 9.4253e-04 - val_loss: 0.0013\n",
            "Epoch 130/200\n",
            "3/3 - 1s - 209ms/step - loss: 9.8796e-04 - val_loss: 9.5347e-04\n",
            "Epoch 131/200\n",
            "3/3 - 1s - 207ms/step - loss: 9.5090e-04 - val_loss: 0.0015\n",
            "Epoch 132/200\n",
            "3/3 - 0s - 142ms/step - loss: 9.3511e-04 - val_loss: 9.2557e-04\n",
            "Epoch 133/200\n",
            "3/3 - 0s - 134ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 134/200\n",
            "3/3 - 0s - 141ms/step - loss: 9.5844e-04 - val_loss: 0.0010\n",
            "Epoch 135/200\n",
            "3/3 - 1s - 199ms/step - loss: 9.4467e-04 - val_loss: 0.0012\n",
            "Epoch 136/200\n",
            "3/3 - 0s - 144ms/step - loss: 9.2567e-04 - val_loss: 0.0011\n",
            "Epoch 137/200\n",
            "3/3 - 1s - 196ms/step - loss: 9.5537e-04 - val_loss: 0.0011\n",
            "Epoch 138/200\n",
            "3/3 - 0s - 140ms/step - loss: 9.2751e-04 - val_loss: 0.0011\n",
            "Epoch 139/200\n",
            "3/3 - 1s - 201ms/step - loss: 9.5704e-04 - val_loss: 0.0011\n",
            "Epoch 140/200\n",
            "3/3 - 1s - 302ms/step - loss: 9.2733e-04 - val_loss: 0.0012\n",
            "Epoch 141/200\n",
            "3/3 - 1s - 409ms/step - loss: 8.4981e-04 - val_loss: 9.3190e-04\n",
            "Epoch 142/200\n",
            "3/3 - 1s - 338ms/step - loss: 8.9410e-04 - val_loss: 0.0012\n",
            "Epoch 143/200\n",
            "3/3 - 1s - 210ms/step - loss: 8.6982e-04 - val_loss: 0.0010\n",
            "Epoch 144/200\n",
            "3/3 - 0s - 139ms/step - loss: 9.1244e-04 - val_loss: 0.0010\n",
            "Epoch 145/200\n",
            "3/3 - 1s - 208ms/step - loss: 8.8192e-04 - val_loss: 0.0011\n",
            "Epoch 146/200\n",
            "3/3 - 1s - 207ms/step - loss: 8.5050e-04 - val_loss: 9.2798e-04\n",
            "Epoch 147/200\n",
            "3/3 - 1s - 197ms/step - loss: 8.5934e-04 - val_loss: 0.0011\n",
            "Epoch 148/200\n",
            "3/3 - 0s - 138ms/step - loss: 8.4994e-04 - val_loss: 9.7189e-04\n",
            "Epoch 149/200\n",
            "3/3 - 0s - 145ms/step - loss: 9.3065e-04 - val_loss: 0.0011\n",
            "Epoch 150/200\n",
            "3/3 - 1s - 197ms/step - loss: 8.4528e-04 - val_loss: 0.0012\n",
            "Epoch 151/200\n",
            "3/3 - 0s - 143ms/step - loss: 9.1478e-04 - val_loss: 8.6557e-04\n",
            "Epoch 152/200\n",
            "3/3 - 0s - 140ms/step - loss: 9.1638e-04 - val_loss: 0.0013\n",
            "Epoch 153/200\n",
            "3/3 - 1s - 208ms/step - loss: 8.9959e-04 - val_loss: 8.7980e-04\n",
            "Epoch 154/200\n",
            "3/3 - 0s - 138ms/step - loss: 9.1369e-04 - val_loss: 0.0010\n",
            "Epoch 155/200\n",
            "3/3 - 0s - 138ms/step - loss: 8.8751e-04 - val_loss: 0.0012\n",
            "Epoch 156/200\n",
            "3/3 - 1s - 205ms/step - loss: 8.0196e-04 - val_loss: 9.2897e-04\n",
            "Epoch 157/200\n",
            "3/3 - 0s - 140ms/step - loss: 8.6892e-04 - val_loss: 8.8600e-04\n",
            "Epoch 158/200\n",
            "3/3 - 0s - 136ms/step - loss: 8.4587e-04 - val_loss: 0.0011\n",
            "Epoch 159/200\n",
            "3/3 - 0s - 137ms/step - loss: 8.3459e-04 - val_loss: 9.6188e-04\n",
            "Epoch 160/200\n",
            "3/3 - 0s - 140ms/step - loss: 8.0591e-04 - val_loss: 8.3154e-04\n",
            "Epoch 161/200\n",
            "3/3 - 1s - 207ms/step - loss: 8.0761e-04 - val_loss: 0.0010\n",
            "Epoch 162/200\n",
            "3/3 - 1s - 229ms/step - loss: 7.6665e-04 - val_loss: 9.1081e-04\n",
            "Epoch 163/200\n",
            "3/3 - 1s - 441ms/step - loss: 8.1172e-04 - val_loss: 0.0011\n",
            "Epoch 164/200\n",
            "3/3 - 1s - 253ms/step - loss: 8.2283e-04 - val_loss: 9.3054e-04\n",
            "Epoch 165/200\n",
            "3/3 - 1s - 248ms/step - loss: 8.2885e-04 - val_loss: 0.0010\n",
            "Epoch 166/200\n",
            "3/3 - 0s - 157ms/step - loss: 8.4355e-04 - val_loss: 9.6726e-04\n",
            "Epoch 167/200\n",
            "3/3 - 1s - 196ms/step - loss: 8.7251e-04 - val_loss: 9.5787e-04\n",
            "Epoch 168/200\n",
            "3/3 - 1s - 206ms/step - loss: 8.3643e-04 - val_loss: 8.5417e-04\n",
            "Epoch 169/200\n",
            "3/3 - 0s - 141ms/step - loss: 7.6547e-04 - val_loss: 9.3557e-04\n",
            "Epoch 170/200\n",
            "3/3 - 1s - 203ms/step - loss: 8.1787e-04 - val_loss: 8.0810e-04\n",
            "Epoch 171/200\n",
            "3/3 - 0s - 142ms/step - loss: 8.0841e-04 - val_loss: 0.0012\n",
            "Epoch 172/200\n",
            "3/3 - 0s - 136ms/step - loss: 8.1714e-04 - val_loss: 7.5026e-04\n",
            "Epoch 173/200\n",
            "3/3 - 1s - 209ms/step - loss: 8.1716e-04 - val_loss: 0.0011\n",
            "Epoch 174/200\n",
            "3/3 - 0s - 136ms/step - loss: 8.3922e-04 - val_loss: 7.6084e-04\n",
            "Epoch 175/200\n",
            "3/3 - 1s - 208ms/step - loss: 8.1418e-04 - val_loss: 9.8233e-04\n",
            "Epoch 176/200\n",
            "3/3 - 0s - 136ms/step - loss: 7.6590e-04 - val_loss: 8.6035e-04\n",
            "Epoch 177/200\n",
            "3/3 - 1s - 208ms/step - loss: 7.9598e-04 - val_loss: 8.7902e-04\n",
            "Epoch 178/200\n",
            "3/3 - 0s - 134ms/step - loss: 8.2641e-04 - val_loss: 0.0010\n",
            "Epoch 179/200\n",
            "3/3 - 0s - 139ms/step - loss: 7.9329e-04 - val_loss: 7.8050e-04\n",
            "Epoch 180/200\n",
            "3/3 - 0s - 136ms/step - loss: 7.8175e-04 - val_loss: 9.5105e-04\n",
            "Epoch 181/200\n",
            "3/3 - 0s - 140ms/step - loss: 7.9190e-04 - val_loss: 8.5034e-04\n",
            "Epoch 182/200\n",
            "3/3 - 1s - 200ms/step - loss: 7.6044e-04 - val_loss: 8.5125e-04\n",
            "Epoch 183/200\n",
            "3/3 - 0s - 140ms/step - loss: 7.4115e-04 - val_loss: 8.7409e-04\n",
            "Epoch 184/200\n",
            "3/3 - 1s - 199ms/step - loss: 7.4744e-04 - val_loss: 8.5274e-04\n",
            "Epoch 185/200\n",
            "3/3 - 0s - 141ms/step - loss: 7.6816e-04 - val_loss: 8.3807e-04\n",
            "Epoch 186/200\n",
            "3/3 - 1s - 297ms/step - loss: 7.8491e-04 - val_loss: 7.9470e-04\n",
            "Epoch 187/200\n",
            "3/3 - 1s - 421ms/step - loss: 7.4250e-04 - val_loss: 8.0031e-04\n",
            "Epoch 188/200\n",
            "3/3 - 1s - 327ms/step - loss: 8.2782e-04 - val_loss: 8.4226e-04\n",
            "Epoch 189/200\n",
            "3/3 - 1s - 201ms/step - loss: 7.6384e-04 - val_loss: 7.0654e-04\n",
            "Epoch 190/200\n",
            "3/3 - 1s - 213ms/step - loss: 7.7218e-04 - val_loss: 9.9274e-04\n",
            "Epoch 191/200\n",
            "3/3 - 0s - 138ms/step - loss: 7.1649e-04 - val_loss: 8.8236e-04\n",
            "Epoch 192/200\n",
            "3/3 - 0s - 135ms/step - loss: 7.5944e-04 - val_loss: 7.3656e-04\n",
            "Epoch 193/200\n",
            "3/3 - 1s - 206ms/step - loss: 7.7613e-04 - val_loss: 7.9627e-04\n",
            "Epoch 194/200\n",
            "3/3 - 0s - 139ms/step - loss: 7.8020e-04 - val_loss: 9.9885e-04\n",
            "Epoch 195/200\n",
            "3/3 - 0s - 134ms/step - loss: 7.4046e-04 - val_loss: 6.8940e-04\n",
            "Epoch 196/200\n",
            "3/3 - 0s - 146ms/step - loss: 7.6135e-04 - val_loss: 8.5762e-04\n",
            "Epoch 197/200\n",
            "3/3 - 1s - 195ms/step - loss: 7.1792e-04 - val_loss: 6.9938e-04\n",
            "Epoch 198/200\n",
            "3/3 - 0s - 141ms/step - loss: 7.9202e-04 - val_loss: 0.0010\n",
            "Epoch 199/200\n",
            "3/3 - 0s - 141ms/step - loss: 7.5419e-04 - val_loss: 6.6614e-04\n",
            "Epoch 200/200\n",
            "3/3 - 0s - 140ms/step - loss: 7.8380e-04 - val_loss: 9.0461e-04\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 150ms/step\n",
            "Best RMSE: 26.073426294533576 with params: {'units': 100, 'epochs': 200, 'batch_size': 32}\n",
            "\n",
            "Detailed results for each parameter combination:\n",
            "Epochs: 50, Batch Size: 16 => MSE: 1261.6952, RMSE: 35.5203, MAE: 29.8566, MAPE: 1.2689%\n",
            "Epochs: 50, Batch Size: 32 => MSE: 2109.6063, RMSE: 45.9305, MAE: 40.4280, MAPE: 1.7224%\n",
            "Epochs: 50, Batch Size: 64 => MSE: 2786.5721, RMSE: 52.7880, MAE: 44.5228, MAPE: 1.8951%\n",
            "Epochs: 50, Batch Size: 128 => MSE: 3943.0998, RMSE: 62.7941, MAE: 51.8568, MAPE: 2.2020%\n",
            "Epochs: 100, Batch Size: 16 => MSE: 1287.5510, RMSE: 35.8825, MAE: 30.9463, MAPE: 1.3159%\n",
            "Epochs: 100, Batch Size: 32 => MSE: 2970.7359, RMSE: 54.5045, MAE: 48.9072, MAPE: 2.0849%\n",
            "Epochs: 100, Batch Size: 64 => MSE: 3632.2264, RMSE: 60.2680, MAE: 54.7045, MAPE: 2.3319%\n",
            "Epochs: 100, Batch Size: 128 => MSE: 3043.3403, RMSE: 55.1665, MAE: 46.5457, MAPE: 1.9798%\n",
            "Epochs: 150, Batch Size: 16 => MSE: 3535.0384, RMSE: 59.4562, MAE: 54.7641, MAPE: 2.3345%\n",
            "Epochs: 150, Batch Size: 32 => MSE: 1433.2770, RMSE: 37.8586, MAE: 32.7478, MAPE: 1.3943%\n",
            "Epochs: 150, Batch Size: 64 => MSE: 1224.3502, RMSE: 34.9907, MAE: 29.0673, MAPE: 1.2356%\n",
            "Epochs: 150, Batch Size: 128 => MSE: 4503.1899, RMSE: 67.1058, MAE: 61.3550, MAPE: 2.6143%\n",
            "Epochs: 200, Batch Size: 16 => MSE: 1004.1938, RMSE: 31.6890, MAE: 26.9610, MAPE: 1.1476%\n",
            "Epochs: 200, Batch Size: 32 => MSE: 679.8236, RMSE: 26.0734, MAE: 21.5983, MAPE: 0.9202%\n",
            "Epochs: 200, Batch Size: 64 => MSE: 1188.9851, RMSE: 34.4817, MAE: 28.9484, MAPE: 1.2312%\n",
            "Epochs: 200, Batch Size: 128 => MSE: 2929.6208, RMSE: 54.1260, MAE: 48.3627, MAPE: 2.0610%\n"
          ]
        }
      ],
      "source": [
        "# Definisikan parameter untuk epoch dan batch size\n",
        "epoch_options = [50, 100, 150, 200]\n",
        "batch_size_options = [16, 32, 64, 128]\n",
        "\n",
        "best_rmse = float('inf')\n",
        "best_params = None\n",
        "results = []\n",
        "\n",
        "# Loop untuk setiap kombinasi epoch dan batch size\n",
        "for e in epoch_options:\n",
        "    for b in batch_size_options:\n",
        "        print(f\"Training with Epochs: {e}, Batch Size: {b}\")\n",
        "        params = {'units': 100, 'epochs': e, 'batch_size': b}\n",
        "        model = build_model(params, X_train)\n",
        "\n",
        "        # Melatih model\n",
        "        history = model.fit(X_train, y_train,\n",
        "                            epochs=e,\n",
        "                            batch_size=b,\n",
        "                            verbose=2,\n",
        "                            validation_split=0.1)\n",
        "\n",
        "        # Melakukan prediksi\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred = scaler.inverse_transform(y_pred)\n",
        "        y_test_orig = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "        # Menghitung MSE, RMSE, MAPE, dan MAE\n",
        "        mse = mean_squared_error(y_test_orig, y_pred)\n",
        "        rmse = math.sqrt(mse)\n",
        "        mae = mean_absolute_error(y_test_orig, y_pred)\n",
        "        mape = np.mean(np.abs((y_test_orig - y_pred) / y_test_orig)) * 100\n",
        "\n",
        "        # Simpan hasil evaluasi untuk setiap kombinasi parameter\n",
        "        results.append((e, b, mse, rmse, mae, mape))\n",
        "\n",
        "        # Update model terbaik jika RMSE saat ini lebih baik\n",
        "        if rmse < best_rmse:\n",
        "            best_rmse = rmse\n",
        "            best_params = params\n",
        "            best_model = model\n",
        "            best_history = history\n",
        "            best_y_pred = y_pred\n",
        "            best_y_test_orig = y_test_orig\n",
        "\n",
        "# Cetak hasil evaluasi\n",
        "print(f\"Best RMSE: {best_rmse} with params: {best_params}\")\n",
        "print(\"\\nDetailed results for each parameter combination:\")\n",
        "\n",
        "# Menampilkan hasil untuk setiap kombinasi parameter yang dilatih\n",
        "for result in results:\n",
        "    e, b, mse, rmse, mae, mape = result\n",
        "    print(f\"Epochs: {e}, Batch Size: {b} => MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IArwWQW1NuJU"
      },
      "source": [
        "Step 5 Visualisasi Hasil Prediksi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqDId_ZPNtIB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "f00918ce-df66-4593-8d24-d407b657a3bf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"ba6948eb-94de-4c69-a801-0d2fdd9fc9ba\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ba6948eb-94de-4c69-a801-0d2fdd9fc9ba\")) {                    Plotly.newPlot(                        \"ba6948eb-94de-4c69-a801-0d2fdd9fc9ba\",                        [{\"line\":{\"color\":\"gray\"},\"mode\":\"lines\",\"name\":\"Training Data\",\"x\":[\"2022-08-01T00:00:00\",\"2022-08-02T00:00:00\",\"2022-08-03T00:00:00\",\"2022-08-04T00:00:00\",\"2022-08-05T00:00:00\",\"2022-08-08T00:00:00\",\"2022-08-09T00:00:00\",\"2022-08-10T00:00:00\",\"2022-08-11T00:00:00\",\"2022-08-12T00:00:00\",\"2022-08-15T00:00:00\",\"2022-08-16T00:00:00\",\"2022-08-17T00:00:00\",\"2022-08-18T00:00:00\",\"2022-08-19T00:00:00\",\"2022-08-22T00:00:00\",\"2022-08-23T00:00:00\",\"2022-08-24T00:00:00\",\"2022-08-25T00:00:00\",\"2022-08-26T00:00:00\",\"2022-08-29T00:00:00\",\"2022-08-30T00:00:00\",\"2022-08-31T00:00:00\",\"2022-09-01T00:00:00\",\"2022-09-02T00:00:00\",\"2022-09-06T00:00:00\",\"2022-09-07T00:00:00\",\"2022-09-08T00:00:00\",\"2022-09-09T00:00:00\",\"2022-09-12T00:00:00\",\"2022-09-13T00:00:00\",\"2022-09-14T00:00:00\",\"2022-09-15T00:00:00\",\"2022-09-16T00:00:00\",\"2022-09-19T00:00:00\",\"2022-09-20T00:00:00\",\"2022-09-21T00:00:00\",\"2022-09-22T00:00:00\",\"2022-09-23T00:00:00\",\"2022-09-26T00:00:00\",\"2022-09-27T00:00:00\",\"2022-09-28T00:00:00\",\"2022-09-29T00:00:00\",\"2022-09-30T00:00:00\",\"2022-10-03T00:00:00\",\"2022-10-04T00:00:00\",\"2022-10-05T00:00:00\",\"2022-10-06T00:00:00\",\"2022-10-07T00:00:00\",\"2022-10-10T00:00:00\",\"2022-10-11T00:00:00\",\"2022-10-12T00:00:00\",\"2022-10-13T00:00:00\",\"2022-10-14T00:00:00\",\"2022-10-17T00:00:00\",\"2022-10-18T00:00:00\",\"2022-10-19T00:00:00\",\"2022-10-20T00:00:00\",\"2022-10-21T00:00:00\",\"2022-10-24T00:00:00\",\"2022-10-25T00:00:00\",\"2022-10-26T00:00:00\",\"2022-10-27T00:00:00\",\"2022-10-28T00:00:00\",\"2022-10-31T00:00:00\",\"2022-11-01T00:00:00\",\"2022-11-02T00:00:00\",\"2022-11-03T00:00:00\",\"2022-11-04T00:00:00\",\"2022-11-07T00:00:00\",\"2022-11-08T00:00:00\",\"2022-11-09T00:00:00\",\"2022-11-10T00:00:00\",\"2022-11-11T00:00:00\",\"2022-11-14T00:00:00\",\"2022-11-15T00:00:00\",\"2022-11-16T00:00:00\",\"2022-11-17T00:00:00\",\"2022-11-18T00:00:00\",\"2022-11-21T00:00:00\",\"2022-11-22T00:00:00\",\"2022-11-23T00:00:00\",\"2022-11-25T00:00:00\",\"2022-11-28T00:00:00\",\"2022-11-29T00:00:00\",\"2022-11-30T00:00:00\",\"2022-12-01T00:00:00\",\"2022-12-02T00:00:00\",\"2022-12-05T00:00:00\",\"2022-12-06T00:00:00\",\"2022-12-07T00:00:00\",\"2022-12-08T00:00:00\",\"2022-12-09T00:00:00\",\"2022-12-12T00:00:00\",\"2022-12-13T00:00:00\",\"2022-12-14T00:00:00\",\"2022-12-15T00:00:00\",\"2022-12-16T00:00:00\",\"2022-12-19T00:00:00\",\"2022-12-20T00:00:00\",\"2022-12-21T00:00:00\",\"2022-12-22T00:00:00\",\"2022-12-23T00:00:00\",\"2022-12-27T00:00:00\",\"2022-12-28T00:00:00\",\"2022-12-29T00:00:00\",\"2022-12-30T00:00:00\",\"2023-01-03T00:00:00\",\"2023-01-04T00:00:00\",\"2023-01-05T00:00:00\",\"2023-01-06T00:00:00\",\"2023-01-09T00:00:00\",\"2023-01-10T00:00:00\",\"2023-01-11T00:00:00\",\"2023-01-12T00:00:00\",\"2023-01-13T00:00:00\",\"2023-01-17T00:00:00\",\"2023-01-18T00:00:00\",\"2023-01-19T00:00:00\",\"2023-01-20T00:00:00\",\"2023-01-23T00:00:00\",\"2023-01-24T00:00:00\",\"2023-01-25T00:00:00\",\"2023-01-26T00:00:00\",\"2023-01-27T00:00:00\",\"2023-01-30T00:00:00\",\"2023-01-31T00:00:00\",\"2023-02-01T00:00:00\",\"2023-02-02T00:00:00\",\"2023-02-03T00:00:00\",\"2023-02-06T00:00:00\",\"2023-02-07T00:00:00\",\"2023-02-08T00:00:00\",\"2023-02-09T00:00:00\",\"2023-02-10T00:00:00\",\"2023-02-13T00:00:00\",\"2023-02-14T00:00:00\",\"2023-02-15T00:00:00\",\"2023-02-16T00:00:00\",\"2023-02-17T00:00:00\",\"2023-02-21T00:00:00\",\"2023-02-22T00:00:00\",\"2023-02-23T00:00:00\",\"2023-02-24T00:00:00\",\"2023-02-27T00:00:00\",\"2023-02-28T00:00:00\",\"2023-03-01T00:00:00\",\"2023-03-02T00:00:00\",\"2023-03-03T00:00:00\",\"2023-03-06T00:00:00\",\"2023-03-07T00:00:00\",\"2023-03-08T00:00:00\",\"2023-03-09T00:00:00\",\"2023-03-10T00:00:00\",\"2023-03-13T00:00:00\",\"2023-03-14T00:00:00\",\"2023-03-15T00:00:00\",\"2023-03-16T00:00:00\",\"2023-03-17T00:00:00\",\"2023-03-20T00:00:00\",\"2023-03-21T00:00:00\",\"2023-03-22T00:00:00\",\"2023-03-23T00:00:00\",\"2023-03-24T00:00:00\",\"2023-03-27T00:00:00\",\"2023-03-28T00:00:00\",\"2023-03-29T00:00:00\",\"2023-03-30T00:00:00\",\"2023-03-31T00:00:00\",\"2023-04-03T00:00:00\",\"2023-04-04T00:00:00\",\"2023-04-05T00:00:00\",\"2023-04-06T00:00:00\",\"2023-04-10T00:00:00\",\"2023-04-11T00:00:00\",\"2023-04-12T00:00:00\",\"2023-04-13T00:00:00\",\"2023-04-14T00:00:00\",\"2023-04-17T00:00:00\",\"2023-04-18T00:00:00\",\"2023-04-19T00:00:00\",\"2023-04-20T00:00:00\",\"2023-04-21T00:00:00\",\"2023-04-24T00:00:00\",\"2023-04-25T00:00:00\",\"2023-04-26T00:00:00\",\"2023-04-27T00:00:00\",\"2023-04-28T00:00:00\",\"2023-05-01T00:00:00\",\"2023-05-02T00:00:00\",\"2023-05-03T00:00:00\",\"2023-05-04T00:00:00\",\"2023-05-05T00:00:00\",\"2023-05-08T00:00:00\",\"2023-05-09T00:00:00\",\"2023-05-10T00:00:00\",\"2023-05-11T00:00:00\",\"2023-05-12T00:00:00\",\"2023-05-15T00:00:00\",\"2023-05-16T00:00:00\",\"2023-05-17T00:00:00\",\"2023-05-18T00:00:00\",\"2023-05-19T00:00:00\",\"2023-05-22T00:00:00\",\"2023-05-23T00:00:00\",\"2023-05-24T00:00:00\",\"2023-05-25T00:00:00\",\"2023-05-26T00:00:00\",\"2023-05-30T00:00:00\",\"2023-05-31T00:00:00\",\"2023-06-01T00:00:00\",\"2023-06-02T00:00:00\",\"2023-06-05T00:00:00\",\"2023-06-06T00:00:00\",\"2023-06-07T00:00:00\",\"2023-06-08T00:00:00\",\"2023-06-09T00:00:00\",\"2023-06-12T00:00:00\",\"2023-06-13T00:00:00\",\"2023-06-14T00:00:00\",\"2023-06-15T00:00:00\",\"2023-06-16T00:00:00\",\"2023-06-20T00:00:00\",\"2023-06-21T00:00:00\",\"2023-06-22T00:00:00\",\"2023-06-23T00:00:00\",\"2023-06-26T00:00:00\",\"2023-06-27T00:00:00\",\"2023-06-28T00:00:00\",\"2023-06-29T00:00:00\",\"2023-06-30T00:00:00\",\"2023-07-03T00:00:00\",\"2023-07-05T00:00:00\",\"2023-07-06T00:00:00\",\"2023-07-07T00:00:00\",\"2023-07-10T00:00:00\",\"2023-07-11T00:00:00\",\"2023-07-12T00:00:00\",\"2023-07-13T00:00:00\",\"2023-07-14T00:00:00\",\"2023-07-17T00:00:00\",\"2023-07-18T00:00:00\",\"2023-07-19T00:00:00\",\"2023-07-20T00:00:00\",\"2023-07-21T00:00:00\",\"2023-07-24T00:00:00\",\"2023-07-25T00:00:00\",\"2023-07-26T00:00:00\",\"2023-07-27T00:00:00\",\"2023-07-28T00:00:00\",\"2023-07-31T00:00:00\",\"2023-08-01T00:00:00\",\"2023-08-02T00:00:00\",\"2023-08-03T00:00:00\",\"2023-08-04T00:00:00\",\"2023-08-07T00:00:00\",\"2023-08-08T00:00:00\",\"2023-08-09T00:00:00\",\"2023-08-10T00:00:00\",\"2023-08-11T00:00:00\",\"2023-08-14T00:00:00\",\"2023-08-15T00:00:00\",\"2023-08-16T00:00:00\",\"2023-08-17T00:00:00\",\"2023-08-18T00:00:00\",\"2023-08-21T00:00:00\",\"2023-08-22T00:00:00\",\"2023-08-23T00:00:00\",\"2023-08-24T00:00:00\",\"2023-08-25T00:00:00\",\"2023-08-28T00:00:00\",\"2023-08-29T00:00:00\",\"2023-08-30T00:00:00\",\"2023-08-31T00:00:00\",\"2023-09-01T00:00:00\",\"2023-09-05T00:00:00\",\"2023-09-06T00:00:00\",\"2023-09-07T00:00:00\",\"2023-09-08T00:00:00\",\"2023-09-11T00:00:00\",\"2023-09-12T00:00:00\",\"2023-09-13T00:00:00\",\"2023-09-14T00:00:00\",\"2023-09-15T00:00:00\",\"2023-09-18T00:00:00\",\"2023-09-19T00:00:00\",\"2023-09-20T00:00:00\",\"2023-09-21T00:00:00\",\"2023-09-22T00:00:00\",\"2023-09-25T00:00:00\",\"2023-09-26T00:00:00\",\"2023-09-27T00:00:00\",\"2023-09-28T00:00:00\",\"2023-09-29T00:00:00\",\"2023-10-02T00:00:00\",\"2023-10-03T00:00:00\",\"2023-10-04T00:00:00\",\"2023-10-05T00:00:00\",\"2023-10-06T00:00:00\",\"2023-10-09T00:00:00\",\"2023-10-10T00:00:00\",\"2023-10-11T00:00:00\",\"2023-10-12T00:00:00\",\"2023-10-13T00:00:00\",\"2023-10-16T00:00:00\",\"2023-10-17T00:00:00\",\"2023-10-18T00:00:00\",\"2023-10-19T00:00:00\",\"2023-10-20T00:00:00\",\"2023-10-23T00:00:00\",\"2023-10-24T00:00:00\",\"2023-10-25T00:00:00\",\"2023-10-26T00:00:00\",\"2023-10-27T00:00:00\",\"2023-10-30T00:00:00\",\"2023-10-31T00:00:00\",\"2023-11-01T00:00:00\",\"2023-11-02T00:00:00\",\"2023-11-03T00:00:00\",\"2023-11-06T00:00:00\",\"2023-11-07T00:00:00\",\"2023-11-08T00:00:00\",\"2023-11-09T00:00:00\",\"2023-11-10T00:00:00\",\"2023-11-13T00:00:00\",\"2023-11-14T00:00:00\",\"2023-11-15T00:00:00\",\"2023-11-16T00:00:00\",\"2023-11-17T00:00:00\",\"2023-11-20T00:00:00\",\"2023-11-21T00:00:00\",\"2023-11-22T00:00:00\",\"2023-11-23T00:00:00\",\"2023-11-24T00:00:00\",\"2023-11-27T00:00:00\",\"2023-11-28T00:00:00\",\"2023-11-29T00:00:00\",\"2023-11-30T00:00:00\",\"2023-12-01T00:00:00\",\"2023-12-04T00:00:00\",\"2023-12-05T00:00:00\",\"2023-12-06T00:00:00\",\"2023-12-07T00:00:00\",\"2023-12-08T00:00:00\",\"2023-12-11T00:00:00\",\"2023-12-12T00:00:00\",\"2023-12-13T00:00:00\",\"2023-12-14T00:00:00\",\"2023-12-15T00:00:00\",\"2023-12-18T00:00:00\",\"2023-12-19T00:00:00\",\"2023-12-20T00:00:00\",\"2023-12-21T00:00:00\",\"2023-12-22T00:00:00\",\"2023-12-26T00:00:00\",\"2023-12-27T00:00:00\",\"2023-12-28T00:00:00\",\"2023-12-29T00:00:00\",\"2024-01-02T00:00:00\",\"2024-01-03T00:00:00\",\"2024-01-04T00:00:00\",\"2024-01-05T00:00:00\",\"2024-01-08T00:00:00\",\"2024-01-09T00:00:00\",\"2024-01-10T00:00:00\",\"2024-01-11T00:00:00\",\"2024-01-12T00:00:00\",\"2024-01-16T00:00:00\",\"2024-01-17T00:00:00\",\"2024-01-18T00:00:00\",\"2024-01-19T00:00:00\",\"2024-01-22T00:00:00\",\"2024-01-23T00:00:00\",\"2024-01-24T00:00:00\",\"2024-01-25T00:00:00\",\"2024-01-26T00:00:00\",\"2024-01-29T00:00:00\",\"2024-01-30T00:00:00\",\"2024-01-31T00:00:00\",\"2024-02-01T00:00:00\",\"2024-02-02T00:00:00\",\"2024-02-05T00:00:00\",\"2024-02-06T00:00:00\",\"2024-02-07T00:00:00\",\"2024-02-08T00:00:00\",\"2024-02-09T00:00:00\",\"2024-02-12T00:00:00\",\"2024-02-13T00:00:00\",\"2024-02-14T00:00:00\",\"2024-02-15T00:00:00\",\"2024-02-16T00:00:00\",\"2024-02-20T00:00:00\",\"2024-02-21T00:00:00\",\"2024-02-22T00:00:00\",\"2024-02-23T00:00:00\",\"2024-02-26T00:00:00\",\"2024-02-27T00:00:00\",\"2024-02-28T00:00:00\",\"2024-02-29T00:00:00\",\"2024-03-01T00:00:00\",\"2024-03-04T00:00:00\",\"2024-03-05T00:00:00\",\"2024-03-06T00:00:00\",\"2024-03-07T00:00:00\",\"2024-03-08T00:00:00\",\"2024-03-11T00:00:00\",\"2024-03-12T00:00:00\",\"2024-03-13T00:00:00\",\"2024-03-14T00:00:00\"],\"y\":[1769.0,1771.0999755859375,1758.0,1788.5,1772.9000244140625,1786.800048828125,1794.0,1795.5999755859375,1789.699951171875,1798.5999755859375,1781.4000244140625,1773.199951171875,1760.300048828125,1755.300048828125,1747.5999755859375,1734.0,1746.800048828125,1747.800048828125,1757.699951171875,1736.0999755859375,1736.5999755859375,1723.199951171875,1712.800048828125,1696.5999755859375,1709.800048828125,1700.4000244140625,1715.300048828125,1708.0,1716.199951171875,1728.0999755859375,1705.0,1696.5,1665.4000244140625,1671.699951171875,1666.199951171875,1659.699951171875,1664.5999755859375,1670.800048828125,1645.300048828125,1623.300048828125,1626.699951171875,1660.4000244140625,1658.5,1662.4000244140625,1692.9000244140625,1721.0999755859375,1711.4000244140625,1711.699951171875,1700.5,1667.300048828125,1678.699951171875,1670.300048828125,1670.0,1641.699951171875,1657.0,1649.0,1627.5,1630.800048828125,1651.0,1648.699951171875,1652.800048828125,1664.0,1660.699951171875,1639.5999755859375,1635.9000244140625,1645.0,1645.699951171875,1627.300048828125,1672.5,1676.5,1712.0999755859375,1710.0999755859375,1750.300048828125,1766.0,1773.5999755859375,1773.800048828125,1773.0,1760.800048828125,1751.9000244140625,1737.4000244140625,1738.300048828125,1744.9000244140625,1753.300048828125,1740.0999755859375,1748.4000244140625,1746.0,1801.0999755859375,1795.9000244140625,1767.4000244140625,1769.300048828125,1785.5,1788.699951171875,1798.0999755859375,1780.5,1813.9000244140625,1807.5,1777.199951171875,1790.0,1787.699951171875,1815.9000244140625,1815.9000244140625,1787.0,1795.9000244140625,1814.800048828125,1807.9000244140625,1819.5,1819.699951171875,1839.699951171875,1852.800048828125,1834.800048828125,1864.199951171875,1872.699951171875,1871.5999755859375,1874.5999755859375,1895.5,1918.4000244140625,1907.199951171875,1904.4000244140625,1922.0999755859375,1926.4000244140625,1927.0999755859375,1933.9000244140625,1941.199951171875,1929.0999755859375,1928.5999755859375,1922.9000244140625,1929.5,1927.800048828125,1916.300048828125,1862.9000244140625,1866.199951171875,1871.699951171875,1877.4000244140625,1866.199951171875,1862.800048828125,1851.9000244140625,1854.0,1834.199951171875,1842.0,1840.4000244140625,1833.0,1832.0,1818.0,1808.800048828125,1817.0,1828.9000244140625,1837.699951171875,1833.5,1847.699951171875,1847.9000244140625,1813.9000244140625,1812.699951171875,1829.300048828125,1862.0,1911.699951171875,1906.199951171875,1926.5999755859375,1919.0,1969.800048828125,1979.199951171875,1938.0,1946.800048828125,1993.800048828125,1982.0999755859375,1952.4000244140625,1972.4000244140625,1966.0999755859375,1980.300048828125,1969.0,1983.9000244140625,2022.199951171875,2020.9000244140625,2011.9000244140625,1989.0999755859375,2004.800048828125,2010.9000244140625,2041.300048828125,2002.199951171875,1994.199951171875,2007.4000244140625,1995.199951171875,2007.5999755859375,1979.5,1989.0999755859375,1994.0,1985.699951171875,1989.9000244140625,1990.0999755859375,1983.4000244140625,2014.300048828125,2028.5999755859375,2048.0,2017.4000244140625,2026.300048828125,2036.199951171875,2030.5,2014.699951171875,2014.5,2018.0,1988.4000244140625,1980.699951171875,1956.5,1978.699951171875,1974.800048828125,1972.4000244140625,1962.800048828125,1943.0999755859375,1944.0999755859375,1958.0,1963.9000244140625,1978.0,1952.4000244140625,1958.0,1965.5,1942.699951171875,1963.5999755859375,1962.199951171875,1955.300048828125,1944.5999755859375,1955.300048828125,1957.800048828125,1958.4000244140625,1935.5,1933.300048828125,1912.699951171875,1919.0999755859375,1923.699951171875,1914.0,1912.300048828125,1909.199951171875,1921.0999755859375,1921.699951171875,1919.5999755859375,1908.699951171875,1926.199951171875,1925.0,1931.300048828125,1956.199951171875,1959.199951171875,1960.0999755859375,1952.4000244140625,1977.199951171875,1977.5,1968.300048828125,1964.300048828125,1960.300048828125,1962.0999755859375,1968.9000244140625,1945.4000244140625,1960.4000244140625,1970.5,1940.699951171875,1937.4000244140625,1932.0,1939.5999755859375,1933.5,1924.0999755859375,1915.4000244140625,1914.4000244140625,1912.9000244140625,1910.5999755859375,1902.5,1896.0999755859375,1884.0999755859375,1886.0999755859375,1893.300048828125,1896.4000244140625,1918.5,1918.199951171875,1911.0999755859375,1917.9000244140625,1936.5,1944.300048828125,1938.199951171875,1939.800048828125,1926.199951171875,1918.0999755859375,1917.5,1918.4000244140625,1923.300048828125,1911.300048828125,1909.0999755859375,1910.0,1923.699951171875,1931.5,1932.0,1945.5999755859375,1919.199951171875,1925.4000244140625,1916.5999755859375,1900.4000244140625,1871.5999755859375,1860.4000244140625,1848.0999755859375,1830.0,1824.5999755859375,1818.5,1816.5999755859375,1830.199951171875,1849.5,1861.0,1872.800048828125,1869.300048828125,1927.4000244140625,1921.0999755859375,1922.699951171875,1955.300048828125,1968.4000244140625,1982.5,1976.300048828125,1975.0,1984.0999755859375,1987.199951171875,1988.5999755859375,1996.199951171875,1985.199951171875,1978.800048828125,1985.5999755859375,1991.5,1981.5999755859375,1966.800048828125,1951.5,1964.199951171875,1932.5999755859375,1945.5,1961.800048828125,1960.0999755859375,1983.9000244140625,1981.5999755859375,1977.699951171875,1999.300048828125,1991.4000244140625,1991.5,2002.199951171875,2011.800048828125,2039.699951171875,2047.0999755859375,2038.0999755859375,2071.0,2024.0999755859375,2018.5,2030.5,2029.9000244140625,1998.300048828125,1978.0,1977.800048828125,1982.300048828125,2030.199951171875,2021.0999755859375,2026.300048828125,2038.4000244140625,2034.5,2039.0999755859375,2057.10009765625,2058.199951171875,2081.89990234375,2073.89990234375,2062.39990234375,2064.39990234375,2034.199951171875,2042.300048828125,2042.4000244140625,2026.5999755859375,2026.4000244140625,2021.699951171875,2014.300048828125,2046.699951171875,2026.0,2002.5999755859375,2018.5999755859375,2026.5,2019.800048828125,2023.699951171875,2013.9000244140625,2016.9000244140625,2016.800048828125,2025.199951171875,2031.5,2048.39990234375,2053.0,2036.0999755859375,2025.699951171875,2034.5,2035.199951171875,2032.199951171875,2023.300048828125,2018.199951171875,1992.9000244140625,1990.300048828125,2002.0999755859375,2011.5,2027.5,2022.300048828125,2019.699951171875,2038.5999755859375,2028.5,2034.0,2033.0,2045.699951171875,2086.89990234375,2117.699951171875,2133.5,2150.300048828125,2158.0,2178.60009765625,2182.5,2160.39990234375,2175.39990234375,2163.0],\"type\":\"scatter\"},{\"line\":{\"color\":\"blue\"},\"mode\":\"lines\",\"name\":\"Actual Stock Prices\",\"x\":[\"2024-03-15T00:00:00\",\"2024-03-18T00:00:00\",\"2024-03-19T00:00:00\",\"2024-03-20T00:00:00\",\"2024-03-21T00:00:00\",\"2024-03-22T00:00:00\",\"2024-03-25T00:00:00\",\"2024-03-26T00:00:00\",\"2024-03-27T00:00:00\",\"2024-03-28T00:00:00\",\"2024-04-01T00:00:00\",\"2024-04-02T00:00:00\",\"2024-04-03T00:00:00\",\"2024-04-04T00:00:00\",\"2024-04-05T00:00:00\",\"2024-04-08T00:00:00\",\"2024-04-09T00:00:00\",\"2024-04-10T00:00:00\",\"2024-04-11T00:00:00\",\"2024-04-12T00:00:00\",\"2024-04-15T00:00:00\",\"2024-04-16T00:00:00\",\"2024-04-17T00:00:00\",\"2024-04-18T00:00:00\",\"2024-04-19T00:00:00\",\"2024-04-22T00:00:00\",\"2024-04-23T00:00:00\",\"2024-04-24T00:00:00\",\"2024-04-25T00:00:00\",\"2024-04-26T00:00:00\",\"2024-04-29T00:00:00\",\"2024-04-30T00:00:00\",\"2024-05-01T00:00:00\",\"2024-05-02T00:00:00\",\"2024-05-03T00:00:00\",\"2024-05-06T00:00:00\",\"2024-05-07T00:00:00\",\"2024-05-08T00:00:00\",\"2024-05-09T00:00:00\",\"2024-05-10T00:00:00\",\"2024-05-13T00:00:00\",\"2024-05-14T00:00:00\",\"2024-05-15T00:00:00\",\"2024-05-16T00:00:00\",\"2024-05-17T00:00:00\",\"2024-05-20T00:00:00\",\"2024-05-21T00:00:00\",\"2024-05-22T00:00:00\",\"2024-05-23T00:00:00\",\"2024-05-24T00:00:00\",\"2024-05-28T00:00:00\",\"2024-05-29T00:00:00\",\"2024-05-30T00:00:00\",\"2024-05-31T00:00:00\",\"2024-06-03T00:00:00\",\"2024-06-04T00:00:00\",\"2024-06-05T00:00:00\",\"2024-06-06T00:00:00\",\"2024-06-07T00:00:00\",\"2024-06-10T00:00:00\",\"2024-06-11T00:00:00\",\"2024-06-12T00:00:00\",\"2024-06-13T00:00:00\",\"2024-06-14T00:00:00\",\"2024-06-17T00:00:00\",\"2024-06-18T00:00:00\",\"2024-06-20T00:00:00\",\"2024-06-21T00:00:00\",\"2024-06-24T00:00:00\",\"2024-06-25T00:00:00\",\"2024-06-26T00:00:00\",\"2024-06-27T00:00:00\",\"2024-06-28T00:00:00\",\"2024-07-01T00:00:00\",\"2024-07-02T00:00:00\",\"2024-07-03T00:00:00\",\"2024-07-05T00:00:00\",\"2024-07-08T00:00:00\",\"2024-07-09T00:00:00\",\"2024-07-10T00:00:00\",\"2024-07-11T00:00:00\",\"2024-07-12T00:00:00\",\"2024-07-15T00:00:00\",\"2024-07-16T00:00:00\",\"2024-07-17T00:00:00\",\"2024-07-18T00:00:00\",\"2024-07-19T00:00:00\",\"2024-07-22T00:00:00\",\"2024-07-23T00:00:00\",\"2024-07-24T00:00:00\",\"2024-07-25T00:00:00\",\"2024-07-26T00:00:00\",\"2024-07-29T00:00:00\",\"2024-07-30T00:00:00\",\"2024-07-31T00:00:00\"],\"y\":[2157.300048828125,2160.699951171875,2156.300048828125,2157.89990234375,2182.39990234375,2158.10009765625,2174.800048828125,2175.60009765625,2190.60009765625,2217.39990234375,2236.5,2261.0,2294.39990234375,2288.800048828125,2325.699951171875,2331.699951171875,2343.5,2329.60009765625,2354.800048828125,2356.199951171875,2365.800048828125,2390.800048828125,2371.699951171875,2382.300048828125,2398.39990234375,2332.199951171875,2327.699951171875,2324.5,2329.800048828125,2334.800048828125,2345.39990234375,2291.39990234375,2299.89990234375,2299.199951171875,2299.0,2321.60009765625,2315.199951171875,2313.60009765625,2332.10009765625,2367.300048828125,2336.10009765625,2353.39990234375,2388.699951171875,2380.0,2412.199951171875,2433.89990234375,2421.699951171875,2389.199951171875,2335.0,2332.5,2355.199951171875,2340.300048828125,2342.89990234375,2322.89990234375,2346.60009765625,2325.5,2354.10009765625,2370.300048828125,2305.199951171875,2307.699951171875,2307.5,2336.0,2300.199951171875,2331.39990234375,2312.39990234375,2330.39990234375,2353.800048828125,2316.39990234375,2330.0,2316.60009765625,2299.199951171875,2324.5,2327.699951171875,2327.60009765625,2323.0,2359.800048828125,2388.5,2355.199951171875,2360.10009765625,2372.199951171875,2415.0,2414.0,2422.89990234375,2462.39990234375,2454.800048828125,2451.800048828125,2395.5,2392.0,2404.60009765625,2413.300048828125,2351.89990234375,2380.0,2377.300048828125,2405.0,2426.5],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\"},\"mode\":\"lines\",\"name\":\"Predicted Stock Prices\",\"x\":[\"2024-03-15T00:00:00\",\"2024-03-18T00:00:00\",\"2024-03-19T00:00:00\",\"2024-03-20T00:00:00\",\"2024-03-21T00:00:00\",\"2024-03-22T00:00:00\",\"2024-03-25T00:00:00\",\"2024-03-26T00:00:00\",\"2024-03-27T00:00:00\",\"2024-03-28T00:00:00\",\"2024-04-01T00:00:00\",\"2024-04-02T00:00:00\",\"2024-04-03T00:00:00\",\"2024-04-04T00:00:00\",\"2024-04-05T00:00:00\",\"2024-04-08T00:00:00\",\"2024-04-09T00:00:00\",\"2024-04-10T00:00:00\",\"2024-04-11T00:00:00\",\"2024-04-12T00:00:00\",\"2024-04-15T00:00:00\",\"2024-04-16T00:00:00\",\"2024-04-17T00:00:00\",\"2024-04-18T00:00:00\",\"2024-04-19T00:00:00\",\"2024-04-22T00:00:00\",\"2024-04-23T00:00:00\",\"2024-04-24T00:00:00\",\"2024-04-25T00:00:00\",\"2024-04-26T00:00:00\",\"2024-04-29T00:00:00\",\"2024-04-30T00:00:00\",\"2024-05-01T00:00:00\",\"2024-05-02T00:00:00\",\"2024-05-03T00:00:00\",\"2024-05-06T00:00:00\",\"2024-05-07T00:00:00\",\"2024-05-08T00:00:00\",\"2024-05-09T00:00:00\",\"2024-05-10T00:00:00\",\"2024-05-13T00:00:00\",\"2024-05-14T00:00:00\",\"2024-05-15T00:00:00\",\"2024-05-16T00:00:00\",\"2024-05-17T00:00:00\",\"2024-05-20T00:00:00\",\"2024-05-21T00:00:00\",\"2024-05-22T00:00:00\",\"2024-05-23T00:00:00\",\"2024-05-24T00:00:00\",\"2024-05-28T00:00:00\",\"2024-05-29T00:00:00\",\"2024-05-30T00:00:00\",\"2024-05-31T00:00:00\",\"2024-06-03T00:00:00\",\"2024-06-04T00:00:00\",\"2024-06-05T00:00:00\",\"2024-06-06T00:00:00\",\"2024-06-07T00:00:00\",\"2024-06-10T00:00:00\",\"2024-06-11T00:00:00\",\"2024-06-12T00:00:00\",\"2024-06-13T00:00:00\",\"2024-06-14T00:00:00\",\"2024-06-17T00:00:00\",\"2024-06-18T00:00:00\",\"2024-06-20T00:00:00\",\"2024-06-21T00:00:00\",\"2024-06-24T00:00:00\",\"2024-06-25T00:00:00\",\"2024-06-26T00:00:00\",\"2024-06-27T00:00:00\",\"2024-06-28T00:00:00\",\"2024-07-01T00:00:00\",\"2024-07-02T00:00:00\",\"2024-07-03T00:00:00\",\"2024-07-05T00:00:00\",\"2024-07-08T00:00:00\",\"2024-07-09T00:00:00\",\"2024-07-10T00:00:00\",\"2024-07-11T00:00:00\",\"2024-07-12T00:00:00\",\"2024-07-15T00:00:00\",\"2024-07-16T00:00:00\",\"2024-07-17T00:00:00\",\"2024-07-18T00:00:00\",\"2024-07-19T00:00:00\",\"2024-07-22T00:00:00\",\"2024-07-23T00:00:00\",\"2024-07-24T00:00:00\",\"2024-07-25T00:00:00\",\"2024-07-26T00:00:00\",\"2024-07-29T00:00:00\",\"2024-07-30T00:00:00\",\"2024-07-31T00:00:00\"],\"y\":[2164.248291015625,2157.697021484375,2158.192626953125,2154.657470703125,2156.218017578125,2176.950439453125,2161.708740234375,2174.914794921875,2176.206298828125,2189.889404296875,2214.19384765625,2234.796142578125,2259.927978515625,2291.953857421875,2291.421142578125,2323.8154296875,2328.2548828125,2340.124755859375,2326.5126953125,2346.114990234375,2344.406982421875,2354.482666015625,2375.61572265625,2360.565673828125,2371.51318359375,2382.186279296875,2325.89453125,2320.201904296875,2307.6630859375,2312.244140625,2317.931640625,2330.75146484375,2287.64697265625,2294.234375,2288.068115234375,2289.697265625,2310.14990234375,2307.71337890625,2309.732177734375,2325.1220703125,2356.46044921875,2332.996337890625,2350.114501953125,2376.173828125,2371.0166015625,2402.202880859375,2419.512939453125,2412.27880859375,2383.88232421875,2331.36376953125,2320.0966796875,2331.33251953125,2320.615966796875,2327.859619140625,2311.65185546875,2333.82421875,2315.598876953125,2343.6826171875,2357.14013671875,2305.93212890625,2306.0986328125,2296.994873046875,2321.71533203125,2293.16845703125,2322.824462890625,2304.76123046875,2323.681640625,2342.6484375,2314.093505859375,2326.109375,2309.740234375,2294.96875,2313.2939453125,2315.834228515625,2319.73779296875,2316.763671875,2348.701171875,2374.948486328125,2351.670654296875,2357.251708984375,2361.695556640625,2398.93212890625,2399.5947265625,2412.87890625,2446.3447265625,2439.7021484375,2440.7314453125,2386.402587890625,2379.97802734375,2380.367431640625,2388.68017578125,2337.93408203125,2363.982666015625,2355.45654296875,2386.349609375],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#f2f5fa\"},\"error_y\":{\"color\":\"#f2f5fa\"},\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"baxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#506784\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"header\":{\"fill\":{\"color\":\"#2a3f5f\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#f2f5fa\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#f2f5fa\"},\"geo\":{\"bgcolor\":\"rgb(17,17,17)\",\"lakecolor\":\"rgb(17,17,17)\",\"landcolor\":\"rgb(17,17,17)\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#506784\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"dark\"},\"paper_bgcolor\":\"rgb(17,17,17)\",\"plot_bgcolor\":\"rgb(17,17,17)\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"bgcolor\":\"rgb(17,17,17)\",\"radialaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"},\"yaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"},\"zaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"}},\"shapedefaults\":{\"line\":{\"color\":\"#f2f5fa\"}},\"sliderdefaults\":{\"bgcolor\":\"#C8D4E3\",\"bordercolor\":\"rgb(17,17,17)\",\"borderwidth\":1,\"tickwidth\":0},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"bgcolor\":\"rgb(17,17,17)\",\"caxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"updatemenudefaults\":{\"bgcolor\":\"#506784\",\"borderwidth\":0},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Forex Currency Price Prediction\"},\"xaxis\":{\"title\":{\"text\":\"Date\"}},\"yaxis\":{\"title\":{\"text\":\"Price\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ba6948eb-94de-4c69-a801-0d2fdd9fc9ba');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def visualize_predictions(data, train_size, n_steps, y_test_orig, y_pred):\n",
        "    fig = go.Figure()\n",
        "\n",
        "    fig.add_trace(go.Scatter(x=data.index[:train_size + n_steps],\n",
        "                             y=data['Close'].values[:train_size + n_steps],\n",
        "                             mode='lines',\n",
        "                             name=\"Training Data\",\n",
        "                             line=dict(color='gray')))\n",
        "\n",
        "    fig.add_trace(go.Scatter(x=data.index[train_size + n_steps:],\n",
        "                             y=y_test_orig.flatten(),\n",
        "                             mode='lines',\n",
        "                             name=\"Actual Stock Prices\",\n",
        "                             line=dict(color='blue')))\n",
        "\n",
        "    fig.add_trace(go.Scatter(x=data.index[train_size + n_steps:],\n",
        "                             y=y_pred.flatten(),\n",
        "                             mode='lines',\n",
        "                             name=\"Predicted Stock Prices\",\n",
        "                             line=dict(color='red')))\n",
        "\n",
        "    fig.update_layout(title=\"Gold Currency Price Prediction\",\n",
        "                      xaxis_title=\"Date\",\n",
        "                      yaxis_title=\"Price\",\n",
        "                      template='plotly_dark')\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "visualize_predictions(data, train_size, n_steps, best_y_test_orig, best_y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsqWPn0SN69x"
      },
      "source": [
        "Step 6 Prediksi Masa Depan dengan Model Terbaik"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMS0Pdl8N5ms",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5d3b0b01-4b1c-410e-84c1-a9834de02381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"533817f3-6a11-4d02-ae14-8ae4ae4a6bd9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"533817f3-6a11-4d02-ae14-8ae4ae4a6bd9\")) {                    Plotly.newPlot(                        \"533817f3-6a11-4d02-ae14-8ae4ae4a6bd9\",                        [{\"line\":{\"color\":\"blue\"},\"mode\":\"lines\",\"name\":\"Actual Stock Prices\",\"x\":[\"2022-08-01T00:00:00\",\"2022-08-02T00:00:00\",\"2022-08-03T00:00:00\",\"2022-08-04T00:00:00\",\"2022-08-05T00:00:00\",\"2022-08-08T00:00:00\",\"2022-08-09T00:00:00\",\"2022-08-10T00:00:00\",\"2022-08-11T00:00:00\",\"2022-08-12T00:00:00\",\"2022-08-15T00:00:00\",\"2022-08-16T00:00:00\",\"2022-08-17T00:00:00\",\"2022-08-18T00:00:00\",\"2022-08-19T00:00:00\",\"2022-08-22T00:00:00\",\"2022-08-23T00:00:00\",\"2022-08-24T00:00:00\",\"2022-08-25T00:00:00\",\"2022-08-26T00:00:00\",\"2022-08-29T00:00:00\",\"2022-08-30T00:00:00\",\"2022-08-31T00:00:00\",\"2022-09-01T00:00:00\",\"2022-09-02T00:00:00\",\"2022-09-06T00:00:00\",\"2022-09-07T00:00:00\",\"2022-09-08T00:00:00\",\"2022-09-09T00:00:00\",\"2022-09-12T00:00:00\",\"2022-09-13T00:00:00\",\"2022-09-14T00:00:00\",\"2022-09-15T00:00:00\",\"2022-09-16T00:00:00\",\"2022-09-19T00:00:00\",\"2022-09-20T00:00:00\",\"2022-09-21T00:00:00\",\"2022-09-22T00:00:00\",\"2022-09-23T00:00:00\",\"2022-09-26T00:00:00\",\"2022-09-27T00:00:00\",\"2022-09-28T00:00:00\",\"2022-09-29T00:00:00\",\"2022-09-30T00:00:00\",\"2022-10-03T00:00:00\",\"2022-10-04T00:00:00\",\"2022-10-05T00:00:00\",\"2022-10-06T00:00:00\",\"2022-10-07T00:00:00\",\"2022-10-10T00:00:00\",\"2022-10-11T00:00:00\",\"2022-10-12T00:00:00\",\"2022-10-13T00:00:00\",\"2022-10-14T00:00:00\",\"2022-10-17T00:00:00\",\"2022-10-18T00:00:00\",\"2022-10-19T00:00:00\",\"2022-10-20T00:00:00\",\"2022-10-21T00:00:00\",\"2022-10-24T00:00:00\",\"2022-10-25T00:00:00\",\"2022-10-26T00:00:00\",\"2022-10-27T00:00:00\",\"2022-10-28T00:00:00\",\"2022-10-31T00:00:00\",\"2022-11-01T00:00:00\",\"2022-11-02T00:00:00\",\"2022-11-03T00:00:00\",\"2022-11-04T00:00:00\",\"2022-11-07T00:00:00\",\"2022-11-08T00:00:00\",\"2022-11-09T00:00:00\",\"2022-11-10T00:00:00\",\"2022-11-11T00:00:00\",\"2022-11-14T00:00:00\",\"2022-11-15T00:00:00\",\"2022-11-16T00:00:00\",\"2022-11-17T00:00:00\",\"2022-11-18T00:00:00\",\"2022-11-21T00:00:00\",\"2022-11-22T00:00:00\",\"2022-11-23T00:00:00\",\"2022-11-25T00:00:00\",\"2022-11-28T00:00:00\",\"2022-11-29T00:00:00\",\"2022-11-30T00:00:00\",\"2022-12-01T00:00:00\",\"2022-12-02T00:00:00\",\"2022-12-05T00:00:00\",\"2022-12-06T00:00:00\",\"2022-12-07T00:00:00\",\"2022-12-08T00:00:00\",\"2022-12-09T00:00:00\",\"2022-12-12T00:00:00\",\"2022-12-13T00:00:00\",\"2022-12-14T00:00:00\",\"2022-12-15T00:00:00\",\"2022-12-16T00:00:00\",\"2022-12-19T00:00:00\",\"2022-12-20T00:00:00\",\"2022-12-21T00:00:00\",\"2022-12-22T00:00:00\",\"2022-12-23T00:00:00\",\"2022-12-27T00:00:00\",\"2022-12-28T00:00:00\",\"2022-12-29T00:00:00\",\"2022-12-30T00:00:00\",\"2023-01-03T00:00:00\",\"2023-01-04T00:00:00\",\"2023-01-05T00:00:00\",\"2023-01-06T00:00:00\",\"2023-01-09T00:00:00\",\"2023-01-10T00:00:00\",\"2023-01-11T00:00:00\",\"2023-01-12T00:00:00\",\"2023-01-13T00:00:00\",\"2023-01-17T00:00:00\",\"2023-01-18T00:00:00\",\"2023-01-19T00:00:00\",\"2023-01-20T00:00:00\",\"2023-01-23T00:00:00\",\"2023-01-24T00:00:00\",\"2023-01-25T00:00:00\",\"2023-01-26T00:00:00\",\"2023-01-27T00:00:00\",\"2023-01-30T00:00:00\",\"2023-01-31T00:00:00\",\"2023-02-01T00:00:00\",\"2023-02-02T00:00:00\",\"2023-02-03T00:00:00\",\"2023-02-06T00:00:00\",\"2023-02-07T00:00:00\",\"2023-02-08T00:00:00\",\"2023-02-09T00:00:00\",\"2023-02-10T00:00:00\",\"2023-02-13T00:00:00\",\"2023-02-14T00:00:00\",\"2023-02-15T00:00:00\",\"2023-02-16T00:00:00\",\"2023-02-17T00:00:00\",\"2023-02-21T00:00:00\",\"2023-02-22T00:00:00\",\"2023-02-23T00:00:00\",\"2023-02-24T00:00:00\",\"2023-02-27T00:00:00\",\"2023-02-28T00:00:00\",\"2023-03-01T00:00:00\",\"2023-03-02T00:00:00\",\"2023-03-03T00:00:00\",\"2023-03-06T00:00:00\",\"2023-03-07T00:00:00\",\"2023-03-08T00:00:00\",\"2023-03-09T00:00:00\",\"2023-03-10T00:00:00\",\"2023-03-13T00:00:00\",\"2023-03-14T00:00:00\",\"2023-03-15T00:00:00\",\"2023-03-16T00:00:00\",\"2023-03-17T00:00:00\",\"2023-03-20T00:00:00\",\"2023-03-21T00:00:00\",\"2023-03-22T00:00:00\",\"2023-03-23T00:00:00\",\"2023-03-24T00:00:00\",\"2023-03-27T00:00:00\",\"2023-03-28T00:00:00\",\"2023-03-29T00:00:00\",\"2023-03-30T00:00:00\",\"2023-03-31T00:00:00\",\"2023-04-03T00:00:00\",\"2023-04-04T00:00:00\",\"2023-04-05T00:00:00\",\"2023-04-06T00:00:00\",\"2023-04-10T00:00:00\",\"2023-04-11T00:00:00\",\"2023-04-12T00:00:00\",\"2023-04-13T00:00:00\",\"2023-04-14T00:00:00\",\"2023-04-17T00:00:00\",\"2023-04-18T00:00:00\",\"2023-04-19T00:00:00\",\"2023-04-20T00:00:00\",\"2023-04-21T00:00:00\",\"2023-04-24T00:00:00\",\"2023-04-25T00:00:00\",\"2023-04-26T00:00:00\",\"2023-04-27T00:00:00\",\"2023-04-28T00:00:00\",\"2023-05-01T00:00:00\",\"2023-05-02T00:00:00\",\"2023-05-03T00:00:00\",\"2023-05-04T00:00:00\",\"2023-05-05T00:00:00\",\"2023-05-08T00:00:00\",\"2023-05-09T00:00:00\",\"2023-05-10T00:00:00\",\"2023-05-11T00:00:00\",\"2023-05-12T00:00:00\",\"2023-05-15T00:00:00\",\"2023-05-16T00:00:00\",\"2023-05-17T00:00:00\",\"2023-05-18T00:00:00\",\"2023-05-19T00:00:00\",\"2023-05-22T00:00:00\",\"2023-05-23T00:00:00\",\"2023-05-24T00:00:00\",\"2023-05-25T00:00:00\",\"2023-05-26T00:00:00\",\"2023-05-30T00:00:00\",\"2023-05-31T00:00:00\",\"2023-06-01T00:00:00\",\"2023-06-02T00:00:00\",\"2023-06-05T00:00:00\",\"2023-06-06T00:00:00\",\"2023-06-07T00:00:00\",\"2023-06-08T00:00:00\",\"2023-06-09T00:00:00\",\"2023-06-12T00:00:00\",\"2023-06-13T00:00:00\",\"2023-06-14T00:00:00\",\"2023-06-15T00:00:00\",\"2023-06-16T00:00:00\",\"2023-06-20T00:00:00\",\"2023-06-21T00:00:00\",\"2023-06-22T00:00:00\",\"2023-06-23T00:00:00\",\"2023-06-26T00:00:00\",\"2023-06-27T00:00:00\",\"2023-06-28T00:00:00\",\"2023-06-29T00:00:00\",\"2023-06-30T00:00:00\",\"2023-07-03T00:00:00\",\"2023-07-05T00:00:00\",\"2023-07-06T00:00:00\",\"2023-07-07T00:00:00\",\"2023-07-10T00:00:00\",\"2023-07-11T00:00:00\",\"2023-07-12T00:00:00\",\"2023-07-13T00:00:00\",\"2023-07-14T00:00:00\",\"2023-07-17T00:00:00\",\"2023-07-18T00:00:00\",\"2023-07-19T00:00:00\",\"2023-07-20T00:00:00\",\"2023-07-21T00:00:00\",\"2023-07-24T00:00:00\",\"2023-07-25T00:00:00\",\"2023-07-26T00:00:00\",\"2023-07-27T00:00:00\",\"2023-07-28T00:00:00\",\"2023-07-31T00:00:00\",\"2023-08-01T00:00:00\",\"2023-08-02T00:00:00\",\"2023-08-03T00:00:00\",\"2023-08-04T00:00:00\",\"2023-08-07T00:00:00\",\"2023-08-08T00:00:00\",\"2023-08-09T00:00:00\",\"2023-08-10T00:00:00\",\"2023-08-11T00:00:00\",\"2023-08-14T00:00:00\",\"2023-08-15T00:00:00\",\"2023-08-16T00:00:00\",\"2023-08-17T00:00:00\",\"2023-08-18T00:00:00\",\"2023-08-21T00:00:00\",\"2023-08-22T00:00:00\",\"2023-08-23T00:00:00\",\"2023-08-24T00:00:00\",\"2023-08-25T00:00:00\",\"2023-08-28T00:00:00\",\"2023-08-29T00:00:00\",\"2023-08-30T00:00:00\",\"2023-08-31T00:00:00\",\"2023-09-01T00:00:00\",\"2023-09-05T00:00:00\",\"2023-09-06T00:00:00\",\"2023-09-07T00:00:00\",\"2023-09-08T00:00:00\",\"2023-09-11T00:00:00\",\"2023-09-12T00:00:00\",\"2023-09-13T00:00:00\",\"2023-09-14T00:00:00\",\"2023-09-15T00:00:00\",\"2023-09-18T00:00:00\",\"2023-09-19T00:00:00\",\"2023-09-20T00:00:00\",\"2023-09-21T00:00:00\",\"2023-09-22T00:00:00\",\"2023-09-25T00:00:00\",\"2023-09-26T00:00:00\",\"2023-09-27T00:00:00\",\"2023-09-28T00:00:00\",\"2023-09-29T00:00:00\",\"2023-10-02T00:00:00\",\"2023-10-03T00:00:00\",\"2023-10-04T00:00:00\",\"2023-10-05T00:00:00\",\"2023-10-06T00:00:00\",\"2023-10-09T00:00:00\",\"2023-10-10T00:00:00\",\"2023-10-11T00:00:00\",\"2023-10-12T00:00:00\",\"2023-10-13T00:00:00\",\"2023-10-16T00:00:00\",\"2023-10-17T00:00:00\",\"2023-10-18T00:00:00\",\"2023-10-19T00:00:00\",\"2023-10-20T00:00:00\",\"2023-10-23T00:00:00\",\"2023-10-24T00:00:00\",\"2023-10-25T00:00:00\",\"2023-10-26T00:00:00\",\"2023-10-27T00:00:00\",\"2023-10-30T00:00:00\",\"2023-10-31T00:00:00\",\"2023-11-01T00:00:00\",\"2023-11-02T00:00:00\",\"2023-11-03T00:00:00\",\"2023-11-06T00:00:00\",\"2023-11-07T00:00:00\",\"2023-11-08T00:00:00\",\"2023-11-09T00:00:00\",\"2023-11-10T00:00:00\",\"2023-11-13T00:00:00\",\"2023-11-14T00:00:00\",\"2023-11-15T00:00:00\",\"2023-11-16T00:00:00\",\"2023-11-17T00:00:00\",\"2023-11-20T00:00:00\",\"2023-11-21T00:00:00\",\"2023-11-22T00:00:00\",\"2023-11-23T00:00:00\",\"2023-11-24T00:00:00\",\"2023-11-27T00:00:00\",\"2023-11-28T00:00:00\",\"2023-11-29T00:00:00\",\"2023-11-30T00:00:00\",\"2023-12-01T00:00:00\",\"2023-12-04T00:00:00\",\"2023-12-05T00:00:00\",\"2023-12-06T00:00:00\",\"2023-12-07T00:00:00\",\"2023-12-08T00:00:00\",\"2023-12-11T00:00:00\",\"2023-12-12T00:00:00\",\"2023-12-13T00:00:00\",\"2023-12-14T00:00:00\",\"2023-12-15T00:00:00\",\"2023-12-18T00:00:00\",\"2023-12-19T00:00:00\",\"2023-12-20T00:00:00\",\"2023-12-21T00:00:00\",\"2023-12-22T00:00:00\",\"2023-12-26T00:00:00\",\"2023-12-27T00:00:00\",\"2023-12-28T00:00:00\",\"2023-12-29T00:00:00\",\"2024-01-02T00:00:00\",\"2024-01-03T00:00:00\",\"2024-01-04T00:00:00\",\"2024-01-05T00:00:00\",\"2024-01-08T00:00:00\",\"2024-01-09T00:00:00\",\"2024-01-10T00:00:00\",\"2024-01-11T00:00:00\",\"2024-01-12T00:00:00\",\"2024-01-16T00:00:00\",\"2024-01-17T00:00:00\",\"2024-01-18T00:00:00\",\"2024-01-19T00:00:00\",\"2024-01-22T00:00:00\",\"2024-01-23T00:00:00\",\"2024-01-24T00:00:00\",\"2024-01-25T00:00:00\",\"2024-01-26T00:00:00\",\"2024-01-29T00:00:00\",\"2024-01-30T00:00:00\",\"2024-01-31T00:00:00\",\"2024-02-01T00:00:00\",\"2024-02-02T00:00:00\",\"2024-02-05T00:00:00\",\"2024-02-06T00:00:00\",\"2024-02-07T00:00:00\",\"2024-02-08T00:00:00\",\"2024-02-09T00:00:00\",\"2024-02-12T00:00:00\",\"2024-02-13T00:00:00\",\"2024-02-14T00:00:00\",\"2024-02-15T00:00:00\",\"2024-02-16T00:00:00\",\"2024-02-20T00:00:00\",\"2024-02-21T00:00:00\",\"2024-02-22T00:00:00\",\"2024-02-23T00:00:00\",\"2024-02-26T00:00:00\",\"2024-02-27T00:00:00\",\"2024-02-28T00:00:00\",\"2024-02-29T00:00:00\",\"2024-03-01T00:00:00\",\"2024-03-04T00:00:00\",\"2024-03-05T00:00:00\",\"2024-03-06T00:00:00\",\"2024-03-07T00:00:00\",\"2024-03-08T00:00:00\",\"2024-03-11T00:00:00\",\"2024-03-12T00:00:00\",\"2024-03-13T00:00:00\",\"2024-03-14T00:00:00\",\"2024-03-15T00:00:00\",\"2024-03-18T00:00:00\",\"2024-03-19T00:00:00\",\"2024-03-20T00:00:00\",\"2024-03-21T00:00:00\",\"2024-03-22T00:00:00\",\"2024-03-25T00:00:00\",\"2024-03-26T00:00:00\",\"2024-03-27T00:00:00\",\"2024-03-28T00:00:00\",\"2024-04-01T00:00:00\",\"2024-04-02T00:00:00\",\"2024-04-03T00:00:00\",\"2024-04-04T00:00:00\",\"2024-04-05T00:00:00\",\"2024-04-08T00:00:00\",\"2024-04-09T00:00:00\",\"2024-04-10T00:00:00\",\"2024-04-11T00:00:00\",\"2024-04-12T00:00:00\",\"2024-04-15T00:00:00\",\"2024-04-16T00:00:00\",\"2024-04-17T00:00:00\",\"2024-04-18T00:00:00\",\"2024-04-19T00:00:00\",\"2024-04-22T00:00:00\",\"2024-04-23T00:00:00\",\"2024-04-24T00:00:00\",\"2024-04-25T00:00:00\",\"2024-04-26T00:00:00\",\"2024-04-29T00:00:00\",\"2024-04-30T00:00:00\",\"2024-05-01T00:00:00\",\"2024-05-02T00:00:00\",\"2024-05-03T00:00:00\",\"2024-05-06T00:00:00\",\"2024-05-07T00:00:00\",\"2024-05-08T00:00:00\",\"2024-05-09T00:00:00\",\"2024-05-10T00:00:00\",\"2024-05-13T00:00:00\",\"2024-05-14T00:00:00\",\"2024-05-15T00:00:00\",\"2024-05-16T00:00:00\",\"2024-05-17T00:00:00\",\"2024-05-20T00:00:00\",\"2024-05-21T00:00:00\",\"2024-05-22T00:00:00\",\"2024-05-23T00:00:00\",\"2024-05-24T00:00:00\",\"2024-05-28T00:00:00\",\"2024-05-29T00:00:00\",\"2024-05-30T00:00:00\",\"2024-05-31T00:00:00\",\"2024-06-03T00:00:00\",\"2024-06-04T00:00:00\",\"2024-06-05T00:00:00\",\"2024-06-06T00:00:00\",\"2024-06-07T00:00:00\",\"2024-06-10T00:00:00\",\"2024-06-11T00:00:00\",\"2024-06-12T00:00:00\",\"2024-06-13T00:00:00\",\"2024-06-14T00:00:00\",\"2024-06-17T00:00:00\",\"2024-06-18T00:00:00\",\"2024-06-20T00:00:00\",\"2024-06-21T00:00:00\",\"2024-06-24T00:00:00\",\"2024-06-25T00:00:00\",\"2024-06-26T00:00:00\",\"2024-06-27T00:00:00\",\"2024-06-28T00:00:00\",\"2024-07-01T00:00:00\",\"2024-07-02T00:00:00\",\"2024-07-03T00:00:00\",\"2024-07-05T00:00:00\",\"2024-07-08T00:00:00\",\"2024-07-09T00:00:00\",\"2024-07-10T00:00:00\",\"2024-07-11T00:00:00\",\"2024-07-12T00:00:00\",\"2024-07-15T00:00:00\",\"2024-07-16T00:00:00\",\"2024-07-17T00:00:00\",\"2024-07-18T00:00:00\",\"2024-07-19T00:00:00\",\"2024-07-22T00:00:00\",\"2024-07-23T00:00:00\",\"2024-07-24T00:00:00\",\"2024-07-25T00:00:00\",\"2024-07-26T00:00:00\",\"2024-07-29T00:00:00\",\"2024-07-30T00:00:00\",\"2024-07-31T00:00:00\"],\"y\":[1769.0,1771.0999755859375,1758.0,1788.5,1772.9000244140625,1786.800048828125,1794.0,1795.5999755859375,1789.699951171875,1798.5999755859375,1781.4000244140625,1773.199951171875,1760.300048828125,1755.300048828125,1747.5999755859375,1734.0,1746.800048828125,1747.800048828125,1757.699951171875,1736.0999755859375,1736.5999755859375,1723.199951171875,1712.800048828125,1696.5999755859375,1709.800048828125,1700.4000244140625,1715.300048828125,1708.0,1716.199951171875,1728.0999755859375,1705.0,1696.5,1665.4000244140625,1671.699951171875,1666.199951171875,1659.699951171875,1664.5999755859375,1670.800048828125,1645.300048828125,1623.300048828125,1626.699951171875,1660.4000244140625,1658.5,1662.4000244140625,1692.9000244140625,1721.0999755859375,1711.4000244140625,1711.699951171875,1700.5,1667.300048828125,1678.699951171875,1670.300048828125,1670.0,1641.699951171875,1657.0,1649.0,1627.5,1630.800048828125,1651.0,1648.699951171875,1652.800048828125,1664.0,1660.699951171875,1639.5999755859375,1635.9000244140625,1645.0,1645.699951171875,1627.300048828125,1672.5,1676.5,1712.0999755859375,1710.0999755859375,1750.300048828125,1766.0,1773.5999755859375,1773.800048828125,1773.0,1760.800048828125,1751.9000244140625,1737.4000244140625,1738.300048828125,1744.9000244140625,1753.300048828125,1740.0999755859375,1748.4000244140625,1746.0,1801.0999755859375,1795.9000244140625,1767.4000244140625,1769.300048828125,1785.5,1788.699951171875,1798.0999755859375,1780.5,1813.9000244140625,1807.5,1777.199951171875,1790.0,1787.699951171875,1815.9000244140625,1815.9000244140625,1787.0,1795.9000244140625,1814.800048828125,1807.9000244140625,1819.5,1819.699951171875,1839.699951171875,1852.800048828125,1834.800048828125,1864.199951171875,1872.699951171875,1871.5999755859375,1874.5999755859375,1895.5,1918.4000244140625,1907.199951171875,1904.4000244140625,1922.0999755859375,1926.4000244140625,1927.0999755859375,1933.9000244140625,1941.199951171875,1929.0999755859375,1928.5999755859375,1922.9000244140625,1929.5,1927.800048828125,1916.300048828125,1862.9000244140625,1866.199951171875,1871.699951171875,1877.4000244140625,1866.199951171875,1862.800048828125,1851.9000244140625,1854.0,1834.199951171875,1842.0,1840.4000244140625,1833.0,1832.0,1818.0,1808.800048828125,1817.0,1828.9000244140625,1837.699951171875,1833.5,1847.699951171875,1847.9000244140625,1813.9000244140625,1812.699951171875,1829.300048828125,1862.0,1911.699951171875,1906.199951171875,1926.5999755859375,1919.0,1969.800048828125,1979.199951171875,1938.0,1946.800048828125,1993.800048828125,1982.0999755859375,1952.4000244140625,1972.4000244140625,1966.0999755859375,1980.300048828125,1969.0,1983.9000244140625,2022.199951171875,2020.9000244140625,2011.9000244140625,1989.0999755859375,2004.800048828125,2010.9000244140625,2041.300048828125,2002.199951171875,1994.199951171875,2007.4000244140625,1995.199951171875,2007.5999755859375,1979.5,1989.0999755859375,1994.0,1985.699951171875,1989.9000244140625,1990.0999755859375,1983.4000244140625,2014.300048828125,2028.5999755859375,2048.0,2017.4000244140625,2026.300048828125,2036.199951171875,2030.5,2014.699951171875,2014.5,2018.0,1988.4000244140625,1980.699951171875,1956.5,1978.699951171875,1974.800048828125,1972.4000244140625,1962.800048828125,1943.0999755859375,1944.0999755859375,1958.0,1963.9000244140625,1978.0,1952.4000244140625,1958.0,1965.5,1942.699951171875,1963.5999755859375,1962.199951171875,1955.300048828125,1944.5999755859375,1955.300048828125,1957.800048828125,1958.4000244140625,1935.5,1933.300048828125,1912.699951171875,1919.0999755859375,1923.699951171875,1914.0,1912.300048828125,1909.199951171875,1921.0999755859375,1921.699951171875,1919.5999755859375,1908.699951171875,1926.199951171875,1925.0,1931.300048828125,1956.199951171875,1959.199951171875,1960.0999755859375,1952.4000244140625,1977.199951171875,1977.5,1968.300048828125,1964.300048828125,1960.300048828125,1962.0999755859375,1968.9000244140625,1945.4000244140625,1960.4000244140625,1970.5,1940.699951171875,1937.4000244140625,1932.0,1939.5999755859375,1933.5,1924.0999755859375,1915.4000244140625,1914.4000244140625,1912.9000244140625,1910.5999755859375,1902.5,1896.0999755859375,1884.0999755859375,1886.0999755859375,1893.300048828125,1896.4000244140625,1918.5,1918.199951171875,1911.0999755859375,1917.9000244140625,1936.5,1944.300048828125,1938.199951171875,1939.800048828125,1926.199951171875,1918.0999755859375,1917.5,1918.4000244140625,1923.300048828125,1911.300048828125,1909.0999755859375,1910.0,1923.699951171875,1931.5,1932.0,1945.5999755859375,1919.199951171875,1925.4000244140625,1916.5999755859375,1900.4000244140625,1871.5999755859375,1860.4000244140625,1848.0999755859375,1830.0,1824.5999755859375,1818.5,1816.5999755859375,1830.199951171875,1849.5,1861.0,1872.800048828125,1869.300048828125,1927.4000244140625,1921.0999755859375,1922.699951171875,1955.300048828125,1968.4000244140625,1982.5,1976.300048828125,1975.0,1984.0999755859375,1987.199951171875,1988.5999755859375,1996.199951171875,1985.199951171875,1978.800048828125,1985.5999755859375,1991.5,1981.5999755859375,1966.800048828125,1951.5,1964.199951171875,1932.5999755859375,1945.5,1961.800048828125,1960.0999755859375,1983.9000244140625,1981.5999755859375,1977.699951171875,1999.300048828125,1991.4000244140625,1991.5,2002.199951171875,2011.800048828125,2039.699951171875,2047.0999755859375,2038.0999755859375,2071.0,2024.0999755859375,2018.5,2030.5,2029.9000244140625,1998.300048828125,1978.0,1977.800048828125,1982.300048828125,2030.199951171875,2021.0999755859375,2026.300048828125,2038.4000244140625,2034.5,2039.0999755859375,2057.10009765625,2058.199951171875,2081.89990234375,2073.89990234375,2062.39990234375,2064.39990234375,2034.199951171875,2042.300048828125,2042.4000244140625,2026.5999755859375,2026.4000244140625,2021.699951171875,2014.300048828125,2046.699951171875,2026.0,2002.5999755859375,2018.5999755859375,2026.5,2019.800048828125,2023.699951171875,2013.9000244140625,2016.9000244140625,2016.800048828125,2025.199951171875,2031.5,2048.39990234375,2053.0,2036.0999755859375,2025.699951171875,2034.5,2035.199951171875,2032.199951171875,2023.300048828125,2018.199951171875,1992.9000244140625,1990.300048828125,2002.0999755859375,2011.5,2027.5,2022.300048828125,2019.699951171875,2038.5999755859375,2028.5,2034.0,2033.0,2045.699951171875,2086.89990234375,2117.699951171875,2133.5,2150.300048828125,2158.0,2178.60009765625,2182.5,2160.39990234375,2175.39990234375,2163.0,2157.300048828125,2160.699951171875,2156.300048828125,2157.89990234375,2182.39990234375,2158.10009765625,2174.800048828125,2175.60009765625,2190.60009765625,2217.39990234375,2236.5,2261.0,2294.39990234375,2288.800048828125,2325.699951171875,2331.699951171875,2343.5,2329.60009765625,2354.800048828125,2356.199951171875,2365.800048828125,2390.800048828125,2371.699951171875,2382.300048828125,2398.39990234375,2332.199951171875,2327.699951171875,2324.5,2329.800048828125,2334.800048828125,2345.39990234375,2291.39990234375,2299.89990234375,2299.199951171875,2299.0,2321.60009765625,2315.199951171875,2313.60009765625,2332.10009765625,2367.300048828125,2336.10009765625,2353.39990234375,2388.699951171875,2380.0,2412.199951171875,2433.89990234375,2421.699951171875,2389.199951171875,2335.0,2332.5,2355.199951171875,2340.300048828125,2342.89990234375,2322.89990234375,2346.60009765625,2325.5,2354.10009765625,2370.300048828125,2305.199951171875,2307.699951171875,2307.5,2336.0,2300.199951171875,2331.39990234375,2312.39990234375,2330.39990234375,2353.800048828125,2316.39990234375,2330.0,2316.60009765625,2299.199951171875,2324.5,2327.699951171875,2327.60009765625,2323.0,2359.800048828125,2388.5,2355.199951171875,2360.10009765625,2372.199951171875,2415.0,2414.0,2422.89990234375,2462.39990234375,2454.800048828125,2451.800048828125,2395.5,2392.0,2404.60009765625,2413.300048828125,2351.89990234375,2380.0,2377.300048828125,2405.0,2426.5],\"type\":\"scatter\"},{\"line\":{\"color\":\"green\"},\"mode\":\"lines\",\"name\":\"Future Predictions\",\"x\":[\"2024-07-31T00:00:00\",\"2024-08-01T00:00:00\",\"2024-08-02T00:00:00\",\"2024-08-03T00:00:00\",\"2024-08-04T00:00:00\",\"2024-08-05T00:00:00\",\"2024-08-06T00:00:00\",\"2024-08-07T00:00:00\",\"2024-08-08T00:00:00\",\"2024-08-09T00:00:00\",\"2024-08-10T00:00:00\",\"2024-08-11T00:00:00\",\"2024-08-12T00:00:00\",\"2024-08-13T00:00:00\",\"2024-08-14T00:00:00\",\"2024-08-15T00:00:00\",\"2024-08-16T00:00:00\",\"2024-08-17T00:00:00\",\"2024-08-18T00:00:00\",\"2024-08-19T00:00:00\",\"2024-08-20T00:00:00\",\"2024-08-21T00:00:00\",\"2024-08-22T00:00:00\",\"2024-08-23T00:00:00\",\"2024-08-24T00:00:00\",\"2024-08-25T00:00:00\",\"2024-08-26T00:00:00\",\"2024-08-27T00:00:00\",\"2024-08-28T00:00:00\",\"2024-08-29T00:00:00\",\"2024-08-30T00:00:00\"],\"y\":[2426.5,2407.076171875,2396.302001953125,2389.375244140625,2379.168212890625,2367.3056640625,2354.583984375,2341.53759765625,2328.62890625,2316.23193359375,2304.632080078125,2294.032470703125,2284.554443359375,2276.256591796875,2269.147705078125,2263.1806640625,2258.2734375,2254.313720703125,2251.181884765625,2248.74755859375,2246.8740234375,2245.43994140625,2244.33203125,2243.455322265625,2242.718994140625,2242.046142578125,2241.380859375,2240.6884765625,2239.93212890625,2239.093505859375,2238.160888671875],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#f2f5fa\"},\"error_y\":{\"color\":\"#f2f5fa\"},\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"baxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#506784\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"header\":{\"fill\":{\"color\":\"#2a3f5f\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#f2f5fa\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#f2f5fa\"},\"geo\":{\"bgcolor\":\"rgb(17,17,17)\",\"lakecolor\":\"rgb(17,17,17)\",\"landcolor\":\"rgb(17,17,17)\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#506784\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"dark\"},\"paper_bgcolor\":\"rgb(17,17,17)\",\"plot_bgcolor\":\"rgb(17,17,17)\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"bgcolor\":\"rgb(17,17,17)\",\"radialaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"},\"yaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"},\"zaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"}},\"shapedefaults\":{\"line\":{\"color\":\"#f2f5fa\"}},\"sliderdefaults\":{\"bgcolor\":\"#C8D4E3\",\"bordercolor\":\"rgb(17,17,17)\",\"borderwidth\":1,\"tickwidth\":0},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"bgcolor\":\"rgb(17,17,17)\",\"caxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"updatemenudefaults\":{\"bgcolor\":\"#506784\",\"borderwidth\":0},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Prediksi Masa Depan\"},\"xaxis\":{\"title\":{\"text\":\"Date\"}},\"yaxis\":{\"title\":{\"text\":\"Price\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('533817f3-6a11-4d02-ae14-8ae4ae4a6bd9');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def forecast_future_prices(model, scaler, scaled_data, n_steps, forecast_days):\n",
        "    last_sequence = scaled_data[-n_steps:]\n",
        "    future_predictions = []\n",
        "\n",
        "    for _ in range(forecast_days):\n",
        "        last_sequence = last_sequence.reshape((1, n_steps, 1))\n",
        "        predicted_price = model.predict(last_sequence)\n",
        "        future_predictions.append(predicted_price[0, 0])\n",
        "        last_sequence = np.append(last_sequence[:, 1:, :], predicted_price.reshape((1, 1, 1)), axis=1)\n",
        "\n",
        "    future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))\n",
        "    return future_predictions.flatten()\n",
        "\n",
        "forecast_days = 30 #Ganti Sesuai Kebutuhan\n",
        "future_predictions = forecast_future_prices(best_model, scaler, scaled_data, n_steps, forecast_days)\n",
        "\n",
        "def visualize_future_predictions(data, future_predictions, forecast_days):\n",
        "    last_date = data.index[-1]\n",
        "    last_close_price = data['Close'].values[-1]\n",
        "    future_predictions = np.insert(future_predictions, 0, last_close_price)\n",
        "    future_dates = pd.date_range(last_date, periods=forecast_days + 1)\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    fig.add_trace(go.Scatter(x=data.index,\n",
        "                             y=data['Close'].values,\n",
        "                             mode='lines',\n",
        "                             name=\"Actual Stock Prices\",\n",
        "                             line=dict(color='blue')))\n",
        "\n",
        "    fig.add_trace(go.Scatter(x=future_dates,\n",
        "                             y=future_predictions,\n",
        "                             mode='lines',\n",
        "                             name=\"Future Predictions\",\n",
        "                             line=dict(color='green')))\n",
        "\n",
        "    fig.update_layout(title=\"Prediksi Masa Depan\",\n",
        "                      xaxis_title=\"Date\",\n",
        "                      yaxis_title=\"Price\",\n",
        "                      template='plotly_dark')\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "visualize_future_predictions(data, future_predictions, forecast_days)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}